[
  {
    "objectID": "slides/04-group-by.html#the-importance-of-axis-labels",
    "href": "slides/04-group-by.html#the-importance-of-axis-labels",
    "title": "Multivariate Summaries",
    "section": "The Importance of Axis Labels",
    "text": "The Importance of Axis Labels\n\n\n\n\n\n\n\n\n\nDoes your plot communicate the context of the data you are plotting?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#last-week",
    "href": "slides/04-group-by.html#last-week",
    "title": "Multivariate Summaries",
    "section": "Last Week",
    "text": "Last Week\n\nReading in data and cleaning / prepping it.\nSummarizing one categorical variable with a distribution.\nSummarizing two categorical variables with joint and conditional distributions.\nUsing plotnine and the grammar of graphics to make bar plots and column plots.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#quantitative-variables-so-far",
    "href": "slides/04-group-by.html#quantitative-variables-so-far",
    "title": "Multivariate Summaries",
    "section": "Quantitative Variables So Far",
    "text": "Quantitative Variables So Far\n\nVisualizing by converting to categorical.\nVisualizing with histograms or densities.\nEstimating probabilities from histograms and densities.\nDescribing the skew.\nCalculating and explaining the mean and the median.\nCalculating and explaining the standard deviation and variance.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#new-dataset-airplane-flights",
    "href": "slides/04-group-by.html#new-dataset-airplane-flights",
    "title": "Multivariate Summaries",
    "section": "New dataset: Airplane Flights",
    "text": "New dataset: Airplane Flights\n\nWhich airline carriers are most likely to be delayed?\n\n\nLet’s look at a data set of all domestic flights that departed from one of New York City’s airports (JFK, LaGuardia, and Newark) on November 16, 2013.\n\n\ndata_dir = \"https://datasci112.stanford.edu/data/\"\ndf = pd.read_csv(data_dir + \"flights_nyc_20131116.csv\")\ndf\n\n    carrier  flight origin dest  dep_delay\n0        US    1895    EWR  CLT       -5.0\n1        UA    1014    LGA  IAH       -3.0\n2        AA    2243    JFK  MIA        2.0\n3        UA     303    JFK  SFO       -8.0\n4        US     795    LGA  PHL       -8.0\n..      ...     ...    ...  ...        ...\n573      B6     745    JFK  PSE       -3.0\n574      B6     839    JFK  BQN        0.0\n575      UA     360    EWR  PBI        NaN\n576      US    1946    EWR  CLT        NaN\n577      US    2142    LGA  BOS        NaN\n\n[578 rows x 5 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays",
    "href": "slides/04-group-by.html#delays",
    "title": "Multivariate Summaries",
    "section": "Delays",
    "text": "Delays\nWe already know how to summarize the flight delays:\n\n\n\n\n\n\nCheck-in 2.2: Interpret these numbers!\n\n\n\n\n\n\n\ndf['dep_delay'].median()\n\nnp.float64(-4.0)\n\ndf['dep_delay'].mean()\n\nnp.float64(2.0469565217391303)\n\ndf['dep_delay'].std()\n\nnp.float64(23.52882923523891)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-1",
    "href": "slides/04-group-by.html#delays-1",
    "title": "Multivariate Summaries",
    "section": "Delays",
    "text": "Delays\nWe already know how to visualize the flight delays:\n\n\n\n\n\n\nCheck-in 2.2: How would you describe this distribution?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin",
    "href": "slides/04-group-by.html#delays-by-origin",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\n\nDo the three origin airports (JFK, LGA, EWR) have different delay patterns?\n\n\n\n\n\n\n\n\nCheck-in 2.2: What could you change in this code to include the origin variable?\n\n\n\n\n\n\n\n(\n  ggplot(df, aes(x = 'dep_delay')) + \n  geom_histogram() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin-1",
    "href": "slides/04-group-by.html#delays-by-origin-1",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\nOverlapping histograms can be really hard to read…\n\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay', fill = 'origin')) + \n  geom_histogram() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin-2",
    "href": "slides/04-group-by.html#delays-by-origin-2",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\n… but overlapping densities often look nicer…\n\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay', fill = 'origin')) + \n  geom_density() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin-3",
    "href": "slides/04-group-by.html#delays-by-origin-3",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\n… especially if we make them a little see-through!\n\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay', fill = 'origin')) + \n  geom_density(alpha = 0.5) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#variable-transformations",
    "href": "slides/04-group-by.html#variable-transformations",
    "title": "Multivariate Summaries",
    "section": "Variable Transformations",
    "text": "Variable Transformations\n\nThat last plot was okay, but it was hard to see the details, because the distribution is so skewed right.\nSometimes, for easier visualization, it is worth transforming a variable.\nFor skewed data, we often use a log transformation.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#log-transformation",
    "href": "slides/04-group-by.html#log-transformation",
    "title": "Multivariate Summaries",
    "section": "Log Transformation",
    "text": "Log Transformation\nExample: Salaries of $10,000, and $100,000, and $10,000,000:\n\n\ndat = pd.DataFrame({\"salary\": [10000, 100000, 10000000]})\ndat[\"log_salary\"] = np.log(dat[\"salary\"])\n\n\n\n\n\n\n\nCode\n(\n  ggplot(data = dat, mapping = aes(x = \"salary\")) + \n  geom_histogram(bins = 100) + \n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(data = dat, mapping = aes(x = \"log_salary\")) + \n  geom_histogram(bins = 100) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#log-transformations",
    "href": "slides/04-group-by.html#log-transformations",
    "title": "Multivariate Summaries",
    "section": "Log transformations",
    "text": "Log transformations\n\nUsually, we use the natural log, just for convenience.\n\n\n\nPros:\nSkewed data looks less skewed, so it is easier to see patterns.\n\n\n\nCons:\nThe variable is now on a different scale so it is not as interpretable.\n\n\n\n\n\n\n\n\nRemember, log transformations need positive numbers!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin---transformed",
    "href": "slides/04-group-by.html#delays-by-origin---transformed",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin - Transformed",
    "text": "Delays by Origin - Transformed\n\n\n\nCode\n# Shift delays to be above zero\ndf['delay_shifted'] = df['dep_delay'] - df['dep_delay'].min() + 1\n\n# Log transform\ndf['log_delay'] = np.log(df['delay_shifted'])\n\n(\n  ggplot(df, aes(x = 'log_delay', fill = 'origin')) + \n  geom_density(alpha = 0.5) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#another-option-boxplots",
    "href": "slides/04-group-by.html#another-option-boxplots",
    "title": "Multivariate Summaries",
    "section": "Another option: Boxplots",
    "text": "Another option: Boxplots\n\n\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = 'log_delay', x = 'origin')) + \n  geom_boxplot() + \n  labs(x = \"\", \n       y = \"Log Delay (minutes)\", \n       title = \"Comparing Departure Delays for NYC Airports\") +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = 'log_delay', x = 'origin')) + \n  geom_boxplot() +\n  labs(x = \"\", \n       y = \"Log Delay (minutes)\", \n       title = \"Comparing Departure Delays for NYC Airports\") +\n  coord_flip() +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#facetting-1",
    "href": "slides/04-group-by.html#facetting-1",
    "title": "Multivariate Summaries",
    "section": "Facetting",
    "text": "Facetting\n\nThis plot still was a little hard to read.\nWhat if we just made separate plots for each origin?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#separate-plots-for-each-origin",
    "href": "slides/04-group-by.html#separate-plots-for-each-origin",
    "title": "Multivariate Summaries",
    "section": "Separate Plots for Each Origin",
    "text": "Separate Plots for Each Origin\nOne option would be to create separate data frames for each origin.\n\n\nis_jfk = (df['origin'] == \"JFK\")\ndf_jfk = df[is_jfk]\ndf_jfk\n\n    carrier  flight origin dest  dep_delay  delay_shifted  log_delay\n2        AA    2243    JFK  MIA        2.0           22.0   3.091042\n3        UA     303    JFK  SFO       -8.0           12.0   2.484907\n11       EV    5716    JFK  IAD       -4.0           16.0   2.772589\n12       B6     583    JFK  MCO       -3.0           17.0   2.833213\n14       B6    1403    JFK  SJU       -2.0           18.0   2.890372\n..      ...     ...    ...  ...        ...            ...        ...\n570      B6     718    JFK  BOS        1.0           21.0   3.044522\n571      B6    1816    JFK  SYR       20.0           40.0   3.688879\n572      B6    1503    JFK  SJU      -10.0           10.0   2.302585\n573      B6     745    JFK  PSE       -3.0           17.0   2.833213\n574      B6     839    JFK  BQN        0.0           20.0   2.995732\n\n[208 rows x 7 columns]\n\n\n\n\nThis seems kind of annoying…",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#fyi-boolean-masking",
    "href": "slides/04-group-by.html#fyi-boolean-masking",
    "title": "Multivariate Summaries",
    "section": "FYI: Boolean Masking",
    "text": "FYI: Boolean Masking\nHow did we filter the previous df to only include \"JFK\" origins?\n\n\n\nStep 1\n\n\nis_jfk = (df['origin'] == \"JFK\")\nis_jfk\n\n0      False\n1      False\n2       True\n3       True\n4      False\n       ...  \n573     True\n574     True\n575    False\n576    False\n577    False\nName: origin, Length: 578, dtype: bool\n\n\n\n\n\n\n\n\nStep 2\n\n\ndf_jfk = df[is_jfk]\ndf_jfk[\"origin\"]\n\n2      JFK\n3      JFK\n11     JFK\n12     JFK\n14     JFK\n      ... \n570    JFK\n571    JFK\n572    JFK\n573    JFK\n574    JFK\nName: origin, Length: 208, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#facetting-2",
    "href": "slides/04-group-by.html#facetting-2",
    "title": "Multivariate Summaries",
    "section": "Facetting",
    "text": "Facetting\nFortunately, plotnine (and other plotting packages) has a trick for you!\n\n\n(\n  ggplot(df, aes(x = 'dep_delay')) + \n  geom_density() + \n  facet_wrap('origin')\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#freeing-the-scales",
    "href": "slides/04-group-by.html#freeing-the-scales",
    "title": "Multivariate Summaries",
    "section": "Freeing the Scales",
    "text": "Freeing the Scales\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay')) + \n  geom_density() + \n  facet_wrap('origin', scales = \"free_y\") +\n  labs(x = \"Departure Delay (minutes)\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#split-apply-combine",
    "href": "slides/04-group-by.html#split-apply-combine",
    "title": "Multivariate Summaries",
    "section": "Split-apply-combine",
    "text": "Split-apply-combine\n\n\n\nOur visualizations told us some of the story, but can we use numeric summaries as well?\nTo do this, we want to calculate the mean or median delay time for each origin airport.\nWe call this split-apply-combine:\n\nsplit the dataset up by a categorical variable origin\napply a calculation like mean\ncombine the results back into one dataset\n\nIn pandas, we use the groupby() function to take care of the split and combine steps!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#group-by",
    "href": "slides/04-group-by.html#group-by",
    "title": "Multivariate Summaries",
    "section": "Group-by",
    "text": "Group-by\n\n\n\n\n(\n  df\n  .groupby(by = \"origin\")[\"dep_delay\"]\n  .mean()\n)\n\norigin\nEWR    4.064935\nJFK    1.461538\nLGA   -0.485294\nName: dep_delay, dtype: float64\n\n\n\n\n\n\n\n\n(\n  df\n  .groupby(by = \"origin\")[\"dep_delay\"]\n  .median()\n)\n\norigin\nEWR   -3.0\nJFK   -4.0\nLGA   -6.0\nName: dep_delay, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#group-by-check-in",
    "href": "slides/04-group-by.html#group-by-check-in",
    "title": "Multivariate Summaries",
    "section": "Group-by Check-in",
    "text": "Group-by Check-in\n\n\n\n\n\n\nCheck-in 2.2\n\n\n\nWhich code is causing “split by origin”?\nWhich code is causing “calculate the mean of delays”?\nWhich code is causing the re-combining of the data?\n\n\n\n\n\n(\n  df\n  .groupby(by = \"origin\")[\"dep_delay\"]\n  .mean()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#simple-example-exam-scores",
    "href": "slides/04-group-by.html#simple-example-exam-scores",
    "title": "Multivariate Summaries",
    "section": "Simple Example: Exam Scores",
    "text": "Simple Example: Exam Scores\nHermione’s exam scores are is:\n\nPotions class: 77/100\nCharms class: 95/100\nHerbology class: 90/100\n\nIn which class did she do best?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#but-wait",
    "href": "slides/04-group-by.html#but-wait",
    "title": "Multivariate Summaries",
    "section": "But wait!",
    "text": "But wait!\nThe class means are:\n\nPotions class: 75/100\nCharms class: 85/100\nHerbology class: 85/100\n\nIn which class did she do best?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#but-wait-1",
    "href": "slides/04-group-by.html#but-wait-1",
    "title": "Multivariate Summaries",
    "section": "But wait!",
    "text": "But wait!\nThe class standard deviations are:\n\nPotions class: 2 points\nCharms class: 5 points\nHerbology class: 1 point\n\nIn which class did she do best?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#different-variabilities-by-origin",
    "href": "slides/04-group-by.html#different-variabilities-by-origin",
    "title": "Multivariate Summaries",
    "section": "Different variabilities by origin",
    "text": "Different variabilities by origin\n\nIn addition to having different centers, the three origins also have different spreads.\n\n(\n  df\n  .groupby(\"origin\")[\"dep_delay\"]\n  .std()\n)\n\norigin\nEWR    25.646258\nJFK    18.713927\nLGA    26.121365\nName: dep_delay, dtype: float64\n\n\n\n\n\n\nIn general flights from \"LGA\" have departure delays that are the furthest from the mean.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#standardized-values-1",
    "href": "slides/04-group-by.html#standardized-values-1",
    "title": "Multivariate Summaries",
    "section": "Standardized values",
    "text": "Standardized values\n\nWe standardize values by subtracting the mean and dividing by the standard deviation.\nThis tells us how much better/worse than typical values our target value is.\nThis is also called the z-score. \\[z_i = \\frac{x_i - \\bar{x}}{s_x}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#standardized-values-2",
    "href": "slides/04-group-by.html#standardized-values-2",
    "title": "Multivariate Summaries",
    "section": "Standardized values",
    "text": "Standardized values\n\nSuppose you fly from LGA and your flight is 40 minutes late. Your friend flies from JFK and their flight is 30 minutes late.\nWho got “unluckier”?\n\n\n\n\nYou?\n\n(40 - -0.48) / 26.12\n\n1.5497702909647777\n\n\n\n\n\nYour friend?\n\n(30 - 1.46) / 18.71\n\n1.5253874933190805",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#activity-2.2",
    "href": "slides/04-group-by.html#activity-2.2",
    "title": "Multivariate Summaries",
    "section": "Activity 2.2",
    "text": "Activity 2.2\n\nDo the different airlines have different patterns of flight delays?\n\n\nMake a plot to answer the question.\nCalculate values to answer the question.\nThe first row is a flight from EWR to CLT on US Airways. The second row is a flight from LGA to IAH on United Airlines. Which one was a “more extreme” delay?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#did-older-passengers-pay-a-higher-fare-on-the-titanic",
    "href": "slides/04-group-by.html#did-older-passengers-pay-a-higher-fare-on-the-titanic",
    "title": "Multivariate Summaries",
    "section": "Did older passengers pay a higher fare on the Titanic?",
    "text": "Did older passengers pay a higher fare on the Titanic?\n\nTo visualize two quantitative variables, we make a scatterplot (or point geometry).\n\n\n\n\n\nCode\n(\n  ggplot(data = df_titanic, mapping = aes(x = 'age', y = 'fare')) + \n  geom_point() +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#scatterplots",
    "href": "slides/04-group-by.html#scatterplots",
    "title": "Multivariate Summaries",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\n\nNotice\n\n\n\nThe explanatory variable was on the x-axis.\nThe response variable was on the y-axis.\n“If you are older, you pay more” not “If you pay more, you get older”.\n\n\n\n\n\n(\n  ggplot(data = df_titanic, \n         mapping = aes(x = 'age', y = 'fare')\n         ) + \n  geom_point() +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#making-a-clearer-plot",
    "href": "slides/04-group-by.html#making-a-clearer-plot",
    "title": "Multivariate Summaries",
    "section": "Making a Clearer Plot",
    "text": "Making a Clearer Plot\nDid you notice how difficult it was to pick out each point?\n\n\n\nPoint Size\n\n\n\nCode\n(\n  ggplot(data = df_titanic, \n         mapping = aes(x = 'age', y = 'fare')\n         ) + \n  geom_jitter(size = 0.5) +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparency\n\n\n\nCode\n(\n  ggplot(data = df_titanic, \n         mapping = aes(x = 'age', y = 'fare')\n         ) + \n  geom_point(alpha = 0.5) +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#spicing-things-up",
    "href": "slides/04-group-by.html#spicing-things-up",
    "title": "Multivariate Summaries",
    "section": "Spicing Things Up",
    "text": "Spicing Things Up\nHow could we make this more interesting?\n\nUse a log-transformation for fare because it is very skewed.\nAdd in a third variable, pclass. How might you do this?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#challenge",
    "href": "slides/04-group-by.html#challenge",
    "title": "Multivariate Summaries",
    "section": "Challenge",
    "text": "Challenge\nCan you re-create this plot?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#describing-a-scatterplot",
    "href": "slides/04-group-by.html#describing-a-scatterplot",
    "title": "Multivariate Summaries",
    "section": "Describing a Scatterplot",
    "text": "Describing a Scatterplot\nLet’s look at just third class:\n\n\n\nCode\nis_third= df_titanic['pclass'] == 3\ndf_third = df_titanic[is_third]\n\n(\n  ggplot(df_third, aes(x = 'age', y = 'fare')) + \n  geom_jitter(alpha = 0.8) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#describing-the-relationship",
    "href": "slides/04-group-by.html#describing-the-relationship",
    "title": "Multivariate Summaries",
    "section": "Describing the Relationship",
    "text": "Describing the Relationship\n\n\nStrength\nNot very strong: the points don’t follow a clear pattern.\n\n\n\n\nDirection\nSlightly negative: When age was higher, fare was a little lower.\n\n\n\n\n\nShape\nNot very linear: the points don’t form a straight line.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#correlation",
    "href": "slides/04-group-by.html#correlation",
    "title": "Multivariate Summaries",
    "section": "Correlation",
    "text": "Correlation\nWhat if we want a numerical summary of the relationship between variables?\n\nDo “older than average” people pay “higher than average” fares?\n\nIn other words, when the z-score of age was high, was the z-score of fare also high?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#age-fare-correlation",
    "href": "slides/04-group-by.html#age-fare-correlation",
    "title": "Multivariate Summaries",
    "section": "Age & Fare Correlation",
    "text": "Age & Fare Correlation\n\n\n\nCode\nmean_age = df_third['age'].mean()\nmean_fare = df_third['fare'].mean()\n\n(\n  ggplot(data = df_third, mapping = aes(x = 'age', y = 'fare')) + \n  geom_jitter(alpha = 0.8) + \n  geom_vline(xintercept = mean_age, color = \"red\", linetype = \"dashed\") + \n  geom_hline(yintercept = mean_fare, color = \"red\", linetype = \"dashed\") + \n  labs(x = \"Age of Passenger\", \n       y = \"Titanic Fare Paid\")\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#correlation-1",
    "href": "slides/04-group-by.html#correlation-1",
    "title": "Multivariate Summaries",
    "section": "Correlation",
    "text": "Correlation\nInterpret this result:\n\ndf_third[['age', 'fare']].corr()\n\n           age      fare\nage   1.000000 -0.238137\nfare -0.238137  1.000000\n\n\n\n\nAge and fare are slightly negatively correlated.\nCan you think of an explanation for this?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#correlation-is-not-the-relationship",
    "href": "slides/04-group-by.html#correlation-is-not-the-relationship",
    "title": "Multivariate Summaries",
    "section": "Correlation is not the Relationship",
    "text": "Correlation is not the Relationship\n\nJust for fun: Guess the Correlation Game",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#takeaways-1",
    "href": "slides/04-group-by.html#takeaways-1",
    "title": "Multivariate Summaries",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nPlot quantitative variables across groups with overlapping density plots, boxplots, or by facetting.\nSummarize quantitative variables across groups by using groupby() and then calculating summary statistics.\nKnow what split-apply-combine means.\nPlot relationships between quantitative variables with a scatterplot.\nDescribe the strength, direction, and shape of the relationship displayed in a scatterplot.\nSummarize relationships between quantitative variables with the correlation",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#getting-and-prepping-data",
    "href": "slides/02-conditional-distributions.html#getting-and-prepping-data",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Getting and Prepping Data",
    "text": "Getting and Prepping Data\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\n\ndf[\"pclass\"] = df[\"pclass\"].astype(\"category\")\ndf[\"survived\"] = df[\"survived\"].astype(\"category\")",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#thinking-about-variable-types",
    "href": "slides/02-conditional-distributions.html#thinking-about-variable-types",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Thinking About Variable Types",
    "text": "Thinking About Variable Types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\npclass\nsurvived\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\nAllen, Miss. Elisabeth Walton\n1\n1\nfemale\n29.0000\n0\n0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\nAllison, Master. Hudson Trevor\n1\n1\nmale\n0.9167\n1\n2\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\nAllison, Miss. Helen Loraine\n1\n0\nfemale\n2.0000\n1\n2\n113781\n151.5500\nC22 C26\nS\nNA\nNaN\nMontreal, PQ / Chesterville, ON\n\n\nAllison, Mr. Hudson Joshua Creighton\n1\n0\nmale\n30.0000\n1\n2\n113781\n151.5500\nC22 C26\nS\nNA\n135\nMontreal, PQ / Chesterville, ON\n\n\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\n1\n0\nfemale\n25.0000\n1\n2\n113781\n151.5500\nC22 C26\nS\nNA\nNaN\nMontreal, PQ / Chesterville, ON\n\n\nAnderson, Mr. Harry\n1\n1\nmale\n48.0000\n0\n0\n19952\n26.5500\nE12\nS\n3\nNaN\nNew York, NY",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#accessing-rows-and-columns",
    "href": "slides/02-conditional-distributions.html#accessing-rows-and-columns",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Accessing Rows and Columns",
    "text": "Accessing Rows and Columns\n\n\n\ndf.iloc[5,]\n\nname         Anderson, Mr. Harry\npclass                         1\nsurvived                       1\nsex                         male\nage                         48.0\nsibsp                          0\nparch                          0\nticket                     19952\nfare                       26.55\ncabin                        E12\nembarked                       S\nboat                           3\nbody                         NaN\nhome.dest           New York, NY\nName: 5, dtype: object\n\n\n\n\ndf[\"name\"].head()\n\n0                      Allen, Miss. Elisabeth Walton\n1                     Allison, Master. Hudson Trevor\n2                       Allison, Miss. Helen Loraine\n3               Allison, Mr. Hudson Joshua Creighton\n4    Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\nName: name, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#quick-summary-of-quantitative-variables",
    "href": "slides/02-conditional-distributions.html#quick-summary-of-quantitative-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Quick Summary of Quantitative Variables",
    "text": "Quick Summary of Quantitative Variables\n\n\ndf.describe()\n\n               age        sibsp        parch         fare        body\ncount  1046.000000  1309.000000  1309.000000  1308.000000  121.000000\nmean     29.881135     0.498854     0.385027    33.295479  160.809917\nstd      14.413500     1.041658     0.865560    51.758668   97.696922\nmin       0.166700     0.000000     0.000000     0.000000    1.000000\n25%      21.000000     0.000000     0.000000     7.895800   72.000000\n50%      28.000000     0.000000     0.000000    14.454200  155.000000\n75%      39.000000     1.000000     0.000000    31.275000  256.000000\nmax      80.000000     8.000000     9.000000   512.329200  328.000000",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#summarizing-categorical-variables",
    "href": "slides/02-conditional-distributions.html#summarizing-categorical-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Summarizing Categorical Variables",
    "text": "Summarizing Categorical Variables\nThe list of percents for each category is called the distribution of the variable.\n\ndf[\"pclass\"].value_counts()\n\npclass\n3    709\n1    323\n2    277\nName: count, dtype: int64\n\ndf[\"pclass\"].value_counts(normalize = True)\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#the-grammar-of-graphics",
    "href": "slides/02-conditional-distributions.html#the-grammar-of-graphics",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe grammar of graphics is a framework for creating data visualizations.\n\n\n\nA visualization consists of:\n\nThe aesthetic: Which variables are dictating which plot elements.\nThe geometry: What shape of plot you are making.\nThe theme: Other choices about the appearance.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#example",
    "href": "slides/02-conditional-distributions.html#example",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Example",
    "text": "Example\n\n\n\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\nfrom plotnine import ggplot, geom_point, aes, geom_boxplot\n\npenguins = load_penguins()\n\n(\n  ggplot(data = penguins, mapping = aes(x = \"species\", \n                                        y = \"bill_length_mm\", \n                                        fill = \"sex\")\n        ) +\n  geom_boxplot()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAesthetics\nWhere are variables mapped to aspects of the plot?\n\n\nGeometry\nWhat shape(s) are used to represent the data / observations?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#plotnine",
    "href": "slides/02-conditional-distributions.html#plotnine",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "plotnine",
    "text": "plotnine\nThe plotnine library implements the grammar of graphics in Python.\n\nThe aes() function is the place to map variables to plot aesthetics.\n\nx, y, and fill are three possible aesthetics that can be specified\n\nA variety of geom_XXX() functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)\n\nAesthetics can differ based on the geom you choose!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#themes",
    "href": "slides/02-conditional-distributions.html#themes",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Themes",
    "text": "Themes\n\n\n\n\n\nCode\n(\n  ggplot(data = penguins, mapping = aes(x = \"species\", \n                                        y = \"bill_length_mm\", \n                                        fill = \"sex\")\n         ) + \n  geom_boxplot()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom plotnine import theme_bw\n\n(\n  ggplot(penguins, aes(x = \"species\", \n                       y = \"bill_length_mm\", \n                       fill = \"sex\")\n                       ) + \n  geom_boxplot() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#check-in",
    "href": "slides/02-conditional-distributions.html#check-in",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Check-In",
    "text": "Check-In\nWhat are the aesthetics and geometry in the cartoon plot below?\n\nAn XKCD comic",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#bar-plots",
    "href": "slides/02-conditional-distributions.html#bar-plots",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo visualize the distribution of a categorical variable, we should use a bar plot.\n\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\")) + \n  geom_bar() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#calculating-percents",
    "href": "slides/02-conditional-distributions.html#calculating-percents",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Calculating Percents",
    "text": "Calculating Percents\n\npclass_dist = (\n  df['pclass']\n  .value_counts(normalize = True)\n  .reset_index()\n  )\n  \npclass_dist\n\n  pclass  proportion\n0      3    0.541635\n1      1    0.246753\n2      2    0.211612",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#percents-on-plots",
    "href": "slides/02-conditional-distributions.html#percents-on-plots",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Percents on Plots",
    "text": "Percents on Plots\n\n\n\nCode\n(\n  ggplot(data = pclass_dist, \n         mapping = aes(x = \"pclass\", y = \"proportion\")) + \n  geom_col() + ### notice this change to a column plot!\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nTechnically, you could still use geom_bar(), but you would need to specify that you didn’t want it to use stat = \"count\" (the default). You’ve already calculated the proportions, so you would use geom_bar(stat = \"identity\").",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot",
    "href": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 1: Stacked Bar Plot",
    "text": "Option 1: Stacked Bar Plot\n\n\n\nCode\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"stack\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot-1",
    "href": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 1: Stacked Bar Plot",
    "text": "Option 1: Stacked Bar Plot\n\nWhat are some pros and cons of the stacked bar plot?\n\n\n\n\nPros\n\n\nWe can still see the total counts in each class\nWe can easily compare the male counts in each class, since those bars are on the bottom.\n\n\n\n\n\n\n\nCons\n\n\nIt is hard to compare the female counts, since those bars are stacked on top.\nIt is hard to estimate the distributions.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot",
    "href": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 2: Side-by-Side Bar Plot",
    "text": "Option 2: Side-by-Side Bar Plot\n\n\n\nCode\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"dodge\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot-1",
    "href": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 2: Side-by-side Bar Plot",
    "text": "Option 2: Side-by-side Bar Plot\n\nWhat are some pros and cons of the side-by-side bar plot?\n\n\n\n\nPros\n\n\nWe can easily compare the female counts in each class.\nWe can easily compare the male counts in each class.\nWe can easily see counts of each within each class.\n\n\n\n\n\n\n\nCons\n\n\nIt is hard to see total counts in each class.\nIt is hard to estimate the distributions.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot",
    "href": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 3: Stacked Percentage Bar Plot",
    "text": "Option 3: Stacked Percentage Bar Plot\n\n\n\nCode\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot-1",
    "href": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 3: Stacked Percentage Bar Plot",
    "text": "Option 3: Stacked Percentage Bar Plot\n\nWhat are some pros and cons of the stacked percentage bar plot?\n\n\n\n\nPros\n\n\nThis is the best way to compare sex balance across classes!\nThis is the option I use the most, because it can answer “Are you more likely to find ______ in ______ ?” type questions.\n\n\n\n\n\n\n\nCons\n\n\nWe can no longer see any counts!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#activity-1.2",
    "href": "slides/02-conditional-distributions.html#activity-1.2",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Activity 1.2",
    "text": "Activity 1.2\n\nChoose one of the plots from lecture so far and “upgrade” it.\n\n\nYou can do this by:\n\nFinding and using a different theme\nUsing labs() to change the axis labels\nTrying different variables\nTrying a different geometries\nUsing + scale_fill_manual() to change the colors being used\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou will need to use documentation of plotnine and online resources!\nCheck out https://www.data-to-viz.com/ for ideas and example code.\nAsk GenAI questions like, “What do I add to a plotnine bar plot to change the colors?” (But of course, make sure you understand the code you use!)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#two-categorical-variables",
    "href": "slides/02-conditional-distributions.html#two-categorical-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Two Categorical Variables",
    "text": "Two Categorical Variables\n\ndf[[\"pclass\", \"sex\"]].value_counts()\n\npclass  sex   \n3       male      493\n        female    216\n1       male      179\n2       male      171\n1       female    144\n2       female    106\nName: count, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#two-way-table",
    "href": "slides/02-conditional-distributions.html#two-way-table",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Two-way Table",
    "text": "Two-way Table\n\n\n(\n  df[[\"pclass\", \"sex\"]]\n  .value_counts()\n  .unstack()\n  )\n\n\nsex     female  male\npclass              \n1          144   179\n2          106   171\n3          216   493\n\n\n\nThis is sometimes called a cross-tab or cross-tabulation.\n\n\n\n\n\n\nPivot Table\n\n\nEssentially unstack() has pivoted the sex column from long format (where the values are included in one column) to wide format where each value has its own column.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#two-way-table---percents",
    "href": "slides/02-conditional-distributions.html#two-way-table---percents",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Two-way Table - Percents",
    "text": "Two-way Table - Percents\n\n(\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623\n\n\n\nAll of these values should sum to 1, aka, 100%!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#switching-variable-order",
    "href": "slides/02-conditional-distributions.html#switching-variable-order",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Switching Variable Order",
    "text": "Switching Variable Order\nWhat cross-tabulation would you expect if we changed the order of the variables? In other words, what would happen if \"sex\" came first and \"pclass\" came second?\n\n\n\n(\n  df[[\"sex\", \"pclass\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n\npclass         1         2         3\nsex                                 \nfemale  0.110008  0.080978  0.165011\nmale    0.136746  0.130634  0.376623",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#interpretation",
    "href": "slides/02-conditional-distributions.html#interpretation",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\nWe call this the joint distribution of the two variables.\n\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623\n\n\n\n\nOf all the passengers on the Titanic, 11% were female passengers riding in first class.\n\n\n\nNOT “11% of all females on Titanic…”\nNOT “11% of all first class passengers…”",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#conditional-distribution-from-counts",
    "href": "slides/02-conditional-distributions.html#conditional-distribution-from-counts",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Conditional Distribution from Counts",
    "text": "Conditional Distribution from Counts\nWe know that:\n\n466 passengers identified as female\nOf those 466 passengers, 144 rode in first class\n\n\nSo:\n\n144 / 466 = 31% of female identifying passengers rode in first class\n\nHere we conditioned on the passenger being female, and then looked at the conditional distribution of pclass.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#conditional-distribution-from-percentages",
    "href": "slides/02-conditional-distributions.html#conditional-distribution-from-percentages",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Conditional Distribution from Percentages",
    "text": "Conditional Distribution from Percentages\nWe know that:\n\n35.5% of all passengers identified as female\nOf those 35.5% of passengers, 11% rode in first class\n\nSo:\n\n0.11 / 0.355 = 31% of female identifying passengers rode in first class\n\nHere we conditioned on the passenger being female, and then looked at the conditional distribution of pclass.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#swapping-variables",
    "href": "slides/02-conditional-distributions.html#swapping-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Swapping Variables",
    "text": "Swapping Variables\nWe know that:\n\n323 passengers rode in first class\nOf those 323 passengers, 144 identified as female\n\nSo:\n\n144 / 323 = 44.6% of first class passengers identified as female\n\nHere we conditioned on the passenger being in first class, and then looked at the conditional distribution of sex.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#which-one-to-condition-on",
    "href": "slides/02-conditional-distributions.html#which-one-to-condition-on",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Which one to condition on?",
    "text": "Which one to condition on?\nThis depends on the research question you are trying to answer.\n\n\n“What class did most female identifying passengers ride in?”\n\n-&gt; Of all female passengers, what is the conditional distribution of class?\n\n\n\n“What was the gender breakdown of first class?”\n\n-&gt; Of all first class passengers, what is the conditional distribution of sex?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#calculating-in-python",
    "href": "slides/02-conditional-distributions.html#calculating-in-python",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Calculating in Python",
    "text": "Calculating in Python\n\nWhen we study two variables, we call the individual one-variable distributions the marginal distribution of that variable.\n\n\n\n\nmarginal_class = (\n  df['pclass']\n  .value_counts(normalize = True)\n  )\n\n\n\nmarginal_class\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64\n\n\n\n\n\n\nmarginal_sex = (\n  df['sex']\n  .value_counts(normalize = True)\n  )\n\n\n\nmarginal_sex\n\nsex\nmale      0.644003\nfemale    0.355997\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#calculating-in-python-1",
    "href": "slides/02-conditional-distributions.html#calculating-in-python-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Calculating in Python",
    "text": "Calculating in Python\n\nWe need to divide the joint distribution (e.g. “11% of passengers were first class female”) by the marginal distribution of the variable we want to condition on (e.g. 35.5% of passengers were female).\n\n\njoint_class_sex = (\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n  \njoint_class_sex.divide(marginal_sex)\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#check-in-1",
    "href": "slides/02-conditional-distributions.html#check-in-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Check-In",
    "text": "Check-In\n\n\n\nmarginal_sex\n\nsex\nmale      0.644003\nfemale    0.355997\nName: proportion, dtype: float64\n\n\n\n\n\n\njoint_class_sex\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623\n\n\n\n\n\n\n\njoint_class_sex.divide(marginal_sex)\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816\n\n\n\n\n\nHow do you think divide() works?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#check-in-2",
    "href": "slides/02-conditional-distributions.html#check-in-2",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Check-In",
    "text": "Check-In\nShould the rows or columns add up to 100%? Why?\n\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#conditional-on-class",
    "href": "slides/02-conditional-distributions.html#conditional-on-class",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Conditional on Class",
    "text": "Conditional on Class\n\njoint_class_sex = (\n  df[[\"sex\", \"pclass\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n  \njoint_class_sex.divide(marginal_class)\n\npclass        1         2         3\nsex                                \nfemale  0.44582  0.382671  0.304654\nmale    0.55418  0.617329  0.695346",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#what-if-you-get-it-backwards",
    "href": "slides/02-conditional-distributions.html#what-if-you-get-it-backwards",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "What if you get it backwards?",
    "text": "What if you get it backwards?\n\njoint_class_sex = (\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n  \njoint_class_sex.divide(marginal_class)\n\n         1   2   3  female  male\npclass                          \n1      NaN NaN NaN     NaN   NaN\n2      NaN NaN NaN     NaN   NaN\n3      NaN NaN NaN     NaN   NaN",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#which-plot-better-answers-the-question",
    "href": "slides/02-conditional-distributions.html#which-plot-better-answers-the-question",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Which plot better answers the question:",
    "text": "Which plot better answers the question:\n\n“Did women tend to ride in first class more than men?”\n\n\n\n\n\nCode\n(\n  ggplot(df, aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(df, aes(x = \"sex\", fill = \"pclass)) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#takeaways-1",
    "href": "slides/02-conditional-distributions.html#takeaways-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Takeaways",
    "text": "Takeaways\n\n\n\nWe use plotnine and the grammar of graphics to make visuals.\nFor two categorical variables, we might use a stacked bar plot, a side-by-side bar plot, or a stacked percentage bar plot - depending on what we are trying to show.\nThe joint distribution of two variables gives the percents in each subcategory.\nThe marginal distribution of a variable is its individual distribution.\nThe conditional distribution of a variable is its distribution among only one category of a different variable.\nWe calculate the conditional distribution by dividing the joint by the marginal.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/05-distances.html#what-is-my-job",
    "href": "slides/05-distances.html#what-is-my-job",
    "title": "Distances Between Observations",
    "section": "What is my job?",
    "text": "What is my job?\n\n\n\n\nTeaching you stuff\n\n(Thoughtfully) choosing what to teach and how to teach it.\n\n\n\n\n\nAssessing what you’ve learned\n\nWhat do you understand about the tools I’ve taught you?\n\n\nThis is not the same as assessing if you figured out a way to accomplish a given task.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#using-the-tools-i-teach",
    "href": "slides/05-distances.html#using-the-tools-i-teach",
    "title": "Distances Between Observations",
    "section": "Using the tools I teach",
    "text": "Using the tools I teach\n\n\nis_100m = df_bolt[\"Event\"] == \"2008 Olympics 100m\"\ndf_100m = df_bolt[is_100m]\none_mean = df_100m[\"Time\"].mean()\none_std = df_100m[\"Time\"].std()\n\ndf_bolt[\"Standardized_Time\"] = 0.0\ndf_bolt.loc[is_100m, \"Standardized_Time\"] = (df_bolt.loc[is_100m, \"Time\"] - \n\n\n\n\n\n\nevent_stats = df_phelps.groupby('Event')['Time_in_seconds'].agg(['mean', 'std'])\ndf_phelps = df_phelps.merge(event_stats, on='Event')\ndf_phelps['Standardized_Time'] = (\n(df_phelps['Time_in_seconds'] - df_phelps['mean']) / df_phelps['std']\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#a-nice-clean-efficient-approach",
    "href": "slides/05-distances.html#a-nice-clean-efficient-approach",
    "title": "Distances Between Observations",
    "section": "A nice clean, efficient approach",
    "text": "A nice clean, efficient approach\n\nbolt_stats = (\n  df_bolt\n  .groupby(\"Event\")[\"Time\"]\n  .aggregate([\"mean\", \"std\"])\n  )\n\nstandardized_bolt = (\n  df_bolt\n  .set_index(['Event', 'Athlete'])['Time']\n  .subtract(bolt_stats[\"mean\"])\n  .divide(bolt_stats[\"std\"])\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lambda-functions",
    "href": "slides/05-distances.html#lambda-functions",
    "title": "Distances Between Observations",
    "section": "lambda Functions",
    "text": "lambda Functions\n\nphelps_sec = (\n  df_phelps[\"Time\"]\n  .str.split(\":\")\n  .apply(lambda x: float(x[0])*60 + float(x[1]))\n)\n\n\n\n\ndf_phelps[[\"Minutes\", \"Seconds\"]] = df_phelps[\"Time\"].str.split(\":\")\ndf_phelps[\"Time_New\"] = (\n  df_phelps[\"Minutes\"].astype(float) * 60 +\n  df_phelps[\"Seconds\"].astype(float)\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#a-loop-is-often-not-necessary",
    "href": "slides/05-distances.html#a-loop-is-often-not-necessary",
    "title": "Distances Between Observations",
    "section": "A loop is often not necessary",
    "text": "A loop is often not necessary\n\nsplit_times = df_phelps[\"Time\"].str.split(\":\")\nseconds = []\n\nfor time in split_times:\n  minute = int(time[0])\n  second = float(time[1])\n  seconds.append((minute * 60) + second)\n  df_phelps[\"Seconds\"] = seconds",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#when-to-make-a-function",
    "href": "slides/05-distances.html#when-to-make-a-function",
    "title": "Distances Between Observations",
    "section": "When to make a function?",
    "text": "When to make a function?\n\n\ndef time_to_secs(time_str):\n  mins, secs = time_str.split(':')\n  return float(mins) * 60 + float(secs)\n\ndf_phelps['time_secs'] = df_phelps['Time'].apply(time_to_secs)\n\n\n\n\n\n\ndef calculate_simpson_index(values, position):\n    \n    # Convert values to a Pandas Series, ensuring they are strings\n    values_series = pd.Series(values).astype(str)\n    \n    # Extract the specified character based on the position\n    extracted_character = values_series.str[position]\n    \n    # Calculate the frequency of each character\n    character_counts = extracted_character.value_counts(normalize=True)\n    \n    # Compute the Simpson's Index\n    simpson_index = 1 - sum(character_counts ** 2)\n    \n    return simpson_index",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#summarizing",
    "href": "slides/05-distances.html#summarizing",
    "title": "Distances Between Observations",
    "section": "Summarizing",
    "text": "Summarizing\n\nOne categorical variable: marginal distribution\nTwo categorical variables: joint and conditional distributions\nOne quantitative variable: mean, median, variance, standard deviation.\nOne quantitative, one categorical: mean, median, and std dev across groups (groupby(), split-apply-combine)\nTwo quantitative variables: z-scores, correlation",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#visualizing",
    "href": "slides/05-distances.html#visualizing",
    "title": "Distances Between Observations",
    "section": "Visualizing",
    "text": "Visualizing\n\nOne categorical variable: bar plot or column plot\nTwo categorical variables: stacked bar plot, side-by-side bar plot, or stacked percentage bar plot\nOne quantitative variable: histogram, density plot, or boxplot\nOne quantitative, one categorical: overlapping densities, side-by-side boxplots, or facetting\nTwo quantitative variables: scatterplot",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#ames-house-prices",
    "href": "slides/05-distances.html#ames-house-prices",
    "title": "Distances Between Observations",
    "section": "Ames house prices",
    "text": "Ames house prices\n\n\ndf = pd.read_table(\"https://datasci112.stanford.edu/data/housing.tsv\",\n                    sep = \"\\\\t\")\ndf.head()\n\n         PID  Gr Liv Area  Bedroom AbvGr  ...  Sale Type  Sale Condition  SalePrice\n0  526301100         1656              3  ...        WD           Normal     215000\n1  526350040          896              2  ...        WD           Normal     105000\n2  526351010         1329              3  ...        WD           Normal     172000\n3  526353030         2110              3  ...        WD           Normal     244000\n4  527105010         1629              3  ...        WD           Normal     189900\n\n[5 rows x 81 columns]\n\n\n\n\n\n\n\n\nread_table not read_csv\n\n\nThis is a tsv file (tab separated values), so we need to use a different function to read in our data! The sep argument allows you to specify the delimiter the file uses, but you can also allow the system to autodetect the delimiter.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms",
    "href": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms",
    "title": "Distances Between Observations",
    "section": "How does house size relate to number of bedrooms?",
    "text": "How does house size relate to number of bedrooms?\n\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(x = \"Gr Liv Area\", y = \"Bedroom AbvGr\"))  + \n  geom_point() +\n  labs(x = \"Total Living Area\", \n       y = \"Number of Bedrooms\")\n )",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms-1",
    "href": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms-1",
    "title": "Distances Between Observations",
    "section": "How does house size relate to number of bedrooms?",
    "text": "How does house size relate to number of bedrooms?\nWhat statistic would you calculate?\n\n\ndf[[\"Gr Liv Area\", \"Bedroom AbvGr\"]].corr()\n\n               Gr Liv Area  Bedroom AbvGr\nGr Liv Area       1.000000       0.516808\nBedroom AbvGr     0.516808       1.000000",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#similarity",
    "href": "slides/05-distances.html#similarity",
    "title": "Distances Between Observations",
    "section": "Similarity",
    "text": "Similarity\nHow might we answer the question, “Are these two houses similar?”\n\n\n\n\ndf.loc[1707, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      2956\nBedroom AbvGr       5\nName: 1707, dtype: object\n\n\n\n\n\n\n\n\ndf.loc[290, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      2650\nBedroom AbvGr       6\nName: 290, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#distance",
    "href": "slides/05-distances.html#distance",
    "title": "Distances Between Observations",
    "section": "Distance",
    "text": "Distance\nThe distance between the two observations is:\n\\[ \\sqrt{ (2956 - 2650)^2 + (5 - 6)^2} = 306 \\]\n\n… what does this number mean? Not much!\nBut we can use it to compare sets of houses and find houses that appear to be the most similar.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#another-house-to-consider",
    "href": "slides/05-distances.html#another-house-to-consider",
    "title": "Distances Between Observations",
    "section": "Another House to Consider",
    "text": "Another House to Consider\n\n\n\ndf.loc[1707, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      2956\nBedroom AbvGr       5\nName: 1707, dtype: object\n\n\n\n\n\n\ndf.loc[291, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      1666\nBedroom AbvGr       3\nName: 291, dtype: object\n\n\n\n\n\\[ \\sqrt{ (2956 - 1666)^2 + (5 - 3)^2} = 1290 \\]\nThus, house 1707 is more similar to house 290 than to house 291.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-1",
    "href": "slides/05-distances.html#lecture-activity-part-1",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 1",
    "text": "Lecture Activity Part 1\n\nComplete Part One of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#house-160-seems-more-similar",
    "href": "slides/05-distances.html#house-160-seems-more-similar",
    "title": "Distances Between Observations",
    "section": "House 160 seems more similar…",
    "text": "House 160 seems more similar…\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = \"Gr Liv Area\", x = \"Bedroom AbvGr\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 2, shape = 17) + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Square Feet)\", \n       x = \"Number of Bedrooms\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#even-if-we-zoom-in",
    "href": "slides/05-distances.html#even-if-we-zoom-in",
    "title": "Distances Between Observations",
    "section": "…even if we zoom in…",
    "text": "…even if we zoom in…\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = \"Gr Liv Area\", x = \"Bedroom AbvGr\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 5, shape = \"x\") + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Square Feet)\", \n       x = \"Number of Bedrooms\") +\n  scale_y_continuous(limits = (2500, 3500))\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#but-not-if-we-put-the-axes-on-the-same-scale",
    "href": "slides/05-distances.html#but-not-if-we-put-the-axes-on-the-same-scale",
    "title": "Distances Between Observations",
    "section": "…but not if we put the axes on the same scale!",
    "text": "…but not if we put the axes on the same scale!\n\n\n\nCode\n(\n  ggplot(df, aes(y = \"Gr Liv Area\", x = \"Bedroom AbvGr\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 5, shape = \"x\") + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Square Feet)\", \n       x = \"Number of Bedrooms\") +\n  scale_y_continuous(limits = (2900, 3000)) +\n  scale_x_continuous(limits = (0, 100))\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#scaling",
    "href": "slides/05-distances.html#scaling",
    "title": "Distances Between Observations",
    "section": "Scaling",
    "text": "Scaling\nWe need to make sure our features are on the same scale before we can use distances to measure similarity.\n\n\n\n\n\n\nStandardizing\n\n\nsubtract the mean, divide by the standard deviation",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#scaling-1",
    "href": "slides/05-distances.html#scaling-1",
    "title": "Distances Between Observations",
    "section": "Scaling",
    "text": "Scaling\n\n\ndf['size_scaled'] = (df['Gr Liv Area'] - df['Gr Liv Area'].mean()) / df['Gr Liv Area'].std()\ndf['bdrm_scaled'] = (df['Bedroom AbvGr'] - df['Bedroom AbvGr'].mean()) / df['Bedroom AbvGr'].std()\n\n\n\n\n\nCode\n(\n  ggplot(df, aes(y = \"size_scaled\", x = \"bdrm_scaled\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 5, shape = \"x\") + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Standardized)\", \n       x = \"Number of Bedrooms (Standardized)\") \n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-2",
    "href": "slides/05-distances.html#lecture-activity-part-2",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 2",
    "text": "Lecture Activity Part 2\n\nComplete Part Two of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#scikit-learn-1",
    "href": "slides/05-distances.html#scikit-learn-1",
    "title": "Distances Between Observations",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\nscikit-learn is a library for machine learning and modeling\nWe will use it a lot in this class!\nFor now, we will use it as a shortcut for scaling and for computing distances\n\n\n\nThe philosophy of sklearn is:\n\nspecify your analysis\nfit on the data to prepare the analysis\ntransform the data",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#specify",
    "href": "slides/05-distances.html#specify",
    "title": "Distances Between Observations",
    "section": "Specify",
    "text": "Specify\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriNot fittedStandardScaler() \n\n\nNo calculations have happened yet!",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#fit",
    "href": "slides/05-distances.html#fit",
    "title": "Distances Between Observations",
    "section": "Fit",
    "text": "Fit\nThe scaler object “learns” the means and standard deviations.\n\n\ndf_orig = df[['Gr Liv Area', 'Bedroom AbvGr']]\nscaler.fit(df_orig)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\nscaler.mean_\n\narray([1499.69044369,    2.85426621])\n\nscaler.scale_\n\narray([505.4226158 ,   0.82758988])\n\n\n\n\nWe still have not altered the data at all!",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#transform",
    "href": "slides/05-distances.html#transform",
    "title": "Distances Between Observations",
    "section": "Transform",
    "text": "Transform\n\ndf_scaled = scaler.transform(df_orig)\ndf_scaled\n\narray([[ 0.30926506,  0.17609421],\n       [-1.19442705, -1.03223376],\n       [-0.33771825,  0.17609421],\n       ...,\n       [-1.04801492,  0.17609421],\n       [-0.21900572, -1.03223376],\n       [ 0.9898836 ,  0.17609421]], shape=(2930, 2))",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#sklearn-numpy-and-pandas",
    "href": "slides/05-distances.html#sklearn-numpy-and-pandas",
    "title": "Distances Between Observations",
    "section": "sklearn, numpy, and pandas",
    "text": "sklearn, numpy, and pandas\n\n\nBy default, sklearn functions return numpy objects.\nThis is sometimes annoying, maybe we want to plot things after scaling.\nSolution: remake it, with the original column names.\n\n\n\n\n\npd.DataFrame(df_scaled, columns = df_orig.columns)\n\n      Gr Liv Area  Bedroom AbvGr\n0        0.309265       0.176094\n1       -1.194427      -1.032234\n2       -0.337718       0.176094\n3        1.207523       0.176094\n4        0.255844       0.176094\n...           ...            ...\n2925    -0.982723       0.176094\n2926    -1.182556      -1.032234\n2927    -1.048015       0.176094\n2928    -0.219006      -1.032234\n2929     0.989884       0.176094\n\n[2930 rows x 2 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#distances-with-sklearn",
    "href": "slides/05-distances.html#distances-with-sklearn",
    "title": "Distances Between Observations",
    "section": "Distances with sklearn",
    "text": "Distances with sklearn\n\nfrom sklearn.metrics import pairwise_distances\n\npairwise_distances(df_scaled[[1707]], df_scaled)\n\narray([[3.52929876, 5.45459713, 4.0252646 , ..., 4.61305666, 4.76999349,\n        3.06886734]], shape=(1, 2930))",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#finding-the-most-similar",
    "href": "slides/05-distances.html#finding-the-most-similar",
    "title": "Distances Between Observations",
    "section": "Finding the Most Similar",
    "text": "Finding the Most Similar\n\n\n\n\ndists = pairwise_distances(df_scaled[[1707]], \n                           df_scaled)\ndists.argsort()\n\narray([[1707,  160,  909, ...,  158, 2723, 2279]], shape=(1, 2930))\n\n\n\n\n\n\n\n\nbest = (\n  dists\n  .argsort()\n  .flatten()\n  [1:10]\n  )\n  \ndf_orig.iloc[best]\n\n      Gr Liv Area  Bedroom AbvGr\n160          2978              5\n909          3082              5\n1288         2792              5\n2350         2784              5\n253          3222              5\n2592         2640              5\n585          2640              5\n2027         2526              5\n2330         3390              5",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-3",
    "href": "slides/05-distances.html#lecture-activity-part-3",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 3",
    "text": "Lecture Activity Part 3\n\nComplete Part Three of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#other-scaling",
    "href": "slides/05-distances.html#other-scaling",
    "title": "Distances Between Observations",
    "section": "Other scaling",
    "text": "Other scaling\n\nStandardization \\[x_i \\leftarrow \\frac{x_i - \\bar{X}}{\\text{sd}(X)}\\]\nMin-Max Scaling \\[x_i \\leftarrow \\frac{x_i - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#other-distances",
    "href": "slides/05-distances.html#other-distances",
    "title": "Distances Between Observations",
    "section": "Other distances",
    "text": "Other distances\n\nEuclidean (\\(\\ell_2\\))\n\\[\\sqrt{\\sum_{j=1}^m (x_j - x'_j)^2}\\]\nManhattan (\\(\\ell_1\\))\n\\[\\sum_{j=1}^m |x_j - x'_j|\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-4",
    "href": "slides/05-distances.html#lecture-activity-part-4",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 4",
    "text": "Lecture Activity Part 4\n\nComplete Part Four of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#takeaways-1",
    "href": "slides/05-distances.html#takeaways-1",
    "title": "Distances Between Observations",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe measure similarity between observations by calculating distances.\nIt is important that all our features be on the same scale for distances to be meaningful.\nWe can use scikit-learn functions to fit and transform data, and to compute pairwise distances.\nThere are many options of ways to scale data; most common is standardizing\nThere are many options of ways to measure distances; most common is Euclidean distance.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/08-knn.html#what-to-expect",
    "href": "slides/08-knn.html#what-to-expect",
    "title": "K-Nearest-Neighbors",
    "section": "What to expect",
    "text": "What to expect\n\n\n\nContent\n\n\n\nEverything covered through the end of this week\n\nVisualizing numerical & categorical variables\nSummarizing numerical & categorical variables\nDistances between observations\nPreprocessing (scaling / one-hot-encoding) variables\n\n\n\n\n\n\n\nStructure\n\n\n\n80-minutes\n\nFirst part of class (8:10 - 9:30am; 12:10 - 1:30pm)\n\nGoogle Collab Notebook\nResources you can use:\n\nYour own Collab notebooks\nAny course materials\nOfficial Python documentation\n\nResources you can not use:\n\nOther humans\nGenerative AI for anything beyond text completion\nGoogle for anything except to reach Python documentation pages",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#a-reminder-about-code-complexity",
    "href": "slides/08-knn.html#a-reminder-about-code-complexity",
    "title": "K-Nearest-Neighbors",
    "section": "A Reminder about Code Complexity",
    "text": "A Reminder about Code Complexity\n\nThere are always multiple ways to accomplish a given task.\n\n\nIn addition to accomplishing the given task, your grade on each problem will also depend on:\n\nwhether your solution uses tools taught in this class, and\nthe efficiency of your solution (e.g., did you use a for loop when it was not necessary?)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#efficiency-in-preprocessing",
    "href": "slides/08-knn.html#efficiency-in-preprocessing",
    "title": "K-Nearest-Neighbors",
    "section": "Efficiency in Preprocessing",
    "text": "Efficiency in Preprocessing\n\n\n\n\nall_transformer = make_column_transformer(\n    (StandardScaler(),\n     [\"abv\", \"srm\", \"originalGravity\", \"ibu\"]\n     ),\n    (OneHotEncoder(sparse_output = False),\n     [\"isOrganic\", \"glass\", \"available\"]\n     ),\n    remainder = \"drop\")\n\n\n\nall_transformer = make_column_transformer(\n    (StandardScaler(),\n     make_column_selector(dtype_include = np.number)\n     ),\n    (OneHotEncoder(sparse_output = False),\n     make_column_selector(dtype_include = object)\n     ),\n    remainder = \"drop\")\n\n\n\n\n\n\nWhat are the efficiency benefits of making a general preprocessor?",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#data-for-exam-1",
    "href": "slides/08-knn.html#data-for-exam-1",
    "title": "K-Nearest-Neighbors",
    "section": "Data for Exam 1",
    "text": "Data for Exam 1\n\n\nYour analysis will focus on the qualitites of different types of coffee and how they are related to where / when / how the coffee was grown and processed.\n\n\n\n\n\n\nLink to the raw data)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#steps-for-data-analysis",
    "href": "slides/08-knn.html#steps-for-data-analysis",
    "title": "K-Nearest-Neighbors",
    "section": "Steps for data analysis",
    "text": "Steps for data analysis\n\nRead and then clean the data\n\nAre there missing values? Will we drop those rows, or replace the missing values with something?\nAre there quantitative variables that Python thinks are categorical?\nAre there categorical variables that Python thinks are quantitative?\nAre there any anomalies in the data that concern you?",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#steps-for-data-analysis-contd",
    "href": "slides/08-knn.html#steps-for-data-analysis-contd",
    "title": "K-Nearest-Neighbors",
    "section": "Steps for data analysis (cont’d)",
    "text": "Steps for data analysis (cont’d)\n\nExplore the data by visualizing and summarizing.\n\nDifferent approaches for different combos of quantitative and categorical variables.\nThink about conditional calculations (split-apply-combine)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#steps-for-data-analysis-contd-1",
    "href": "slides/08-knn.html#steps-for-data-analysis-contd-1",
    "title": "K-Nearest-Neighbors",
    "section": "Steps for data analysis (cont’d)",
    "text": "Steps for data analysis (cont’d)\n\nIdentify a research question of interest.\nPerform preprocessing steps\n\nShould we scale the quantitative variables?\nShould we one-hot-encode the categorical variables?\nShould we log-transform any variables?\n\nMeasure similarity between observations by calculating distances.\n\nWhich features should be included?\nWhich distance metric should we use?",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#data-wine-qualities",
    "href": "slides/08-knn.html#data-wine-qualities",
    "title": "K-Nearest-Neighbors",
    "section": "Data: Wine Qualities",
    "text": "Data: Wine Qualities\n\ndf = pd.read_csv(\"https://dlsun.github.io/pods/data/bordeaux.csv\")\ndf\n\n    year  price  summer  har   sep  win  age\n0   1952   37.0    17.1  160  14.3  600   40\n1   1953   63.0    16.7   80  17.3  690   39\n2   1955   45.0    17.1  130  16.8  502   37\n3   1957   22.0    16.1  110  16.2  420   35\n4   1958   18.0    16.4  187  19.1  582   34\n5   1959   66.0    17.5  187  18.7  485   33\n6   1960   14.0    16.4  290  15.8  763   32\n7   1961  100.0    17.3   38  20.4  830   31\n8   1962   33.0    16.3   52  17.2  697   30\n9   1963   17.0    15.7  155  16.2  608   29\n10  1964   31.0    17.3   96  18.8  402   28\n11  1965   11.0    15.4  267  14.8  602   27\n12  1966   47.0    16.5   86  18.4  819   26\n13  1967   19.0    16.2  118  16.5  714   25\n14  1968   11.0    16.2  292  16.4  610   24\n15  1969   12.0    16.5  244  16.6  575   23\n16  1970   40.0    16.7   89  18.0  622   22\n17  1971   27.0    16.8  112  16.9  551   21\n18  1972   10.0    15.0  158  14.6  536   20\n19  1973   16.0    17.1  123  17.9  376   19\n20  1974   11.0    16.3  184  16.2  574   18\n21  1975   30.0    16.9  171  17.2  572   17\n22  1976   25.0    17.6  247  16.1  418   16\n23  1977   11.0    15.6   87  16.8  821   15\n24  1978   27.0    15.8   51  17.4  763   14\n25  1979   21.0    16.2  122  17.3  717   13\n26  1980   14.0    16.0   74  18.4  578   12\n27  1981    NaN    17.0  111  18.0  535   11\n28  1982    NaN    17.4  162  18.5  712   10\n29  1983    NaN    17.4  119  17.9  845    9\n30  1984    NaN    16.5  119  16.0  591    8\n31  1985    NaN    16.8   38  18.9  744    7\n32  1986    NaN    16.3  171  17.5  563    6\n33  1987    NaN    17.0  115  18.9  452    5\n34  1988    NaN    17.1   59  16.8  808    4\n35  1989    NaN    18.6   82  18.4  443    3\n36  1990    NaN    18.7   80  19.3  468    2\n37  1991    NaN    17.7  183  20.4  570    1",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#data-wine-quality-variables",
    "href": "slides/08-knn.html#data-wine-quality-variables",
    "title": "K-Nearest-Neighbors",
    "section": "Data: Wine Quality Variables",
    "text": "Data: Wine Quality Variables\n\n\nyear: What year the wine was produced\nprice: Average market price for Bordeaux vintages according to a series of auctions.\n\nThe price is relative to the price of the 1961 vintage, regarded as the best one ever recorded.\n\nwin: Winter rainfall (in mm)\nsummer: Average temperature during the summer months (June - August)\nsep: Average temperature in the month of September (in Celsius)\nhar: Rainfall during harvest month(s) (in mm)\nage: How old the wine was in 1992 (years since 1992)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#analysis-goal",
    "href": "slides/08-knn.html#analysis-goal",
    "title": "K-Nearest-Neighbors",
    "section": "Analysis Goal",
    "text": "Analysis Goal\n\nGoal: Predict what will be the quality (price) of wines in a future year.\n\n\nIdea: Wines with similar features probably have similar quality.\n\n\n\nInputs: Summer temperature, harvest rainfall, September temperature, winter rainfall, age of wine\n\n“Inputs” = “Features” = “Predictors” = “Independent variables”\n\nOutput: Price in 1992\n\n“Output” = “Target” = “Dependent variable”",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#similar-wines",
    "href": "slides/08-knn.html#similar-wines",
    "title": "K-Nearest-Neighbors",
    "section": "Similar Wines",
    "text": "Similar Wines\nWhich wines have similar summer temps and winter rainfall to the 1989 vintage?\n\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(df, mapping = aes(x = \"summer\", y = \"win\")) + \n  geom_point(color = \"white\") + \n  geom_text(mapping = aes(label = \"year\")) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#predicting-1989",
    "href": "slides/08-knn.html#predicting-1989",
    "title": "K-Nearest-Neighbors",
    "section": "Predicting 1989",
    "text": "Predicting 1989\n1989\n\n\ndf[df['year'] == 1989]\n\n    year  price  summer  har   sep  win  age\n35  1989    NaN    18.6   82  18.4  443    3\n\n\n\n1990\n\n\ndf[df['year'] == 1990]\n\n    year  price  summer  har   sep  win  age\n36  1990    NaN    18.7   80  19.3  468    2\n\n\n\n1976\n\n\ndf[df['year'] == 1976]\n\n    year  price  summer  har   sep  win  age\n22  1976   25.0    17.6  247  16.1  418   16",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#training-and-test-data",
    "href": "slides/08-knn.html#training-and-test-data",
    "title": "K-Nearest-Neighbors",
    "section": "Training and test data",
    "text": "Training and test data\n\n\nThe data for which we know the target is called the training data.\n\n\nknown_prices = df['year'] &lt; 1981\n\ndf_train = df[known_prices].copy()\n\n\n\n\n\n\nThe data for which we don’t know the target (and want to predict it) is called the test data.\n\n\nto_predict = df['year'] == 1989\n\ndf_test = df[to_predict].copy()",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#specify-steps",
    "href": "slides/08-knn.html#specify-steps",
    "title": "K-Nearest-Neighbors",
    "section": "Specify steps",
    "text": "Specify steps\nFirst we make a column transformer…\n\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\npreproc = make_column_transformer(\n  (StandardScaler(), \n  make_column_selector(dtype_include = np.number)\n  ),\n  remainder = \"drop\"\n)\n\nfeatures = ['summer', 'har', 'sep', 'win', 'age']\n\n\n\n\n\n\n\n\n\n\nWhy make a general processor?",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#fit-the-preprocesser",
    "href": "slides/08-knn.html#fit-the-preprocesser",
    "title": "K-Nearest-Neighbors",
    "section": "Fit the Preprocesser",
    "text": "Fit the Preprocesser\nThen we fit it on the training data.\n\n\n\npreproc.fit(df_train[features])\n\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x139393e00&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x139393e00&gt;)]) standardscaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x139393e00&gt; StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\n\n\n\npreproc.named_transformers_['standardscaler'].mean_\n\narray([ 16.47037037, 144.81481481,  17.04814815, 608.40740741,\n        25.18518519])\n\npreproc.named_transformers_['standardscaler'].var_\n\narray([4.09492455e-01, 5.14089163e+03, 1.89286694e+00, 1.60333525e+04,\n       6.54842250e+01])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#prep-the-data",
    "href": "slides/08-knn.html#prep-the-data",
    "title": "K-Nearest-Neighbors",
    "section": "Prep the data",
    "text": "Prep the data\nThen we transform the training data AND the test data:\n\ntrain_new = preproc.transform(df_train[features])\ntest_new = preproc.transform(df_test[features])\n\ntest_new\n\narray([[ 3.32798322, -0.87607817,  0.98258257, -1.30629957, -2.74154079]])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data",
    "href": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data",
    "title": "K-Nearest-Neighbors",
    "section": "What if we had fit on the test data?",
    "text": "What if we had fit on the test data?\n\n\n\npreproc.fit(df_test[features])\n\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x139393e00&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x139393e00&gt;)]) standardscaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x139393e00&gt; StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\n\n\n\npreproc.named_transformers_['standardscaler'].mean_\n\narray([ 18.6,  82. ,  18.4, 443. ,   3. ])\n\npreproc.named_transformers_['standardscaler'].var_\n\narray([0., 0., 0., 0., 0.])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data-1",
    "href": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data-1",
    "title": "K-Nearest-Neighbors",
    "section": "What if we had fit on the test data?",
    "text": "What if we had fit on the test data?\n\npreproc.transform(df_test[features])\n\narray([[0., 0., 0., 0., 0.]])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#all-together",
    "href": "slides/08-knn.html#all-together",
    "title": "K-Nearest-Neighbors",
    "section": "All together:",
    "text": "All together:\n\n\npreproc = make_column_transformer(\n  (StandardScaler(), \n  make_column_selector(dtype_include = np.number)\n  ),\n  remainder = \"drop\"\n)\n\nfeatures = ['summer', 'har', 'sep', 'win', 'age']\n\n\n\npreproc.fit(df_train[features])\n\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt;)]) standardscaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt; StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\ntrain_new = preproc.transform(df_train[features])\ntest_new = preproc.transform(df_test[features])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#find-the-closest-k",
    "href": "slides/08-knn.html#find-the-closest-k",
    "title": "K-Nearest-Neighbors",
    "section": "Find the Closest k",
    "text": "Find the Closest k\n\nfrom sklearn.metrics import pairwise_distances\n\ndists = pairwise_distances(test_new, train_new)\ndists\n\narray([[6.16457005, 5.74908409, 5.01651675, 5.8002254 , 5.48664619,\n        4.35898872, 6.56017889, 5.28491106, 5.38614546, 6.01267804,\n        3.72811494, 6.99167472, 5.26008007, 5.3100901 , 5.76468136,\n        4.97806644, 4.05228644, 3.86667994, 6.7345252 , 3.18480532,\n        4.69099623, 3.65924   , 3.62660714, 5.86910657, 5.30050409,\n        4.60719435, 4.34675994]])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#find-the-closest-k-1",
    "href": "slides/08-knn.html#find-the-closest-k-1",
    "title": "K-Nearest-Neighbors",
    "section": "Find the Closest k",
    "text": "Find the Closest k\n\n\nbest = (\n  dists[0]\n  .argsort()\n  )\nbest[0:5]\n\narray([19, 22, 21, 10, 17])\n\n\n\n\n\n\ndf_train.loc[best]\n\n    year  price  summer  har   sep  win  age\n19  1973   16.0    17.1  123  17.9  376   19\n22  1976   25.0    17.6  247  16.1  418   16\n21  1975   30.0    16.9  171  17.2  572   17\n10  1964   31.0    17.3   96  18.8  402   28\n17  1971   27.0    16.8  112  16.9  551   21\n16  1970   40.0    16.7   89  18.0  622   22\n26  1980   14.0    16.0   74  18.4  578   12\n5   1959   66.0    17.5  187  18.7  485   33\n25  1979   21.0    16.2  122  17.3  717   13\n20  1974   11.0    16.3  184  16.2  574   18\n15  1969   12.0    16.5  244  16.6  575   23\n2   1955   45.0    17.1  130  16.8  502   37\n12  1966   47.0    16.5   86  18.4  819   26\n7   1961  100.0    17.3   38  20.4  830   31\n24  1978   27.0    15.8   51  17.4  763   14\n13  1967   19.0    16.2  118  16.5  714   25\n8   1962   33.0    16.3   52  17.2  697   30\n4   1958   18.0    16.4  187  19.1  582   34\n1   1953   63.0    16.7   80  17.3  690   39\n14  1968   11.0    16.2  292  16.4  610   24\n3   1957   22.0    16.1  110  16.2  420   35\n23  1977   11.0    15.6   87  16.8  821   15\n9   1963   17.0    15.7  155  16.2  608   29\n0   1952   37.0    17.1  160  14.3  600   40\n6   1960   14.0    16.4  290  15.8  763   32\n18  1972   10.0    15.0  158  14.6  536   20\n11  1965   11.0    15.4  267  14.8  602   27",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#predict-from-the-closest-k",
    "href": "slides/08-knn.html#predict-from-the-closest-k",
    "title": "K-Nearest-Neighbors",
    "section": "Predict from the closest k",
    "text": "Predict from the closest k\nIf \\(k = 1\\) …\n\ndf_train.loc[best[0]]\n\nyear      1973.0\nprice       16.0\nsummer      17.1\nhar        123.0\nsep         17.9\nwin        376.0\nage         19.0\nName: 19, dtype: float64\n\n\n\n…we would predict a price of…\n\ndf_train.loc[best[0], \"price\"]\n\nnp.float64(16.0)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#predict-from-the-closest-k-1",
    "href": "slides/08-knn.html#predict-from-the-closest-k-1",
    "title": "K-Nearest-Neighbors",
    "section": "Predict from the closest k",
    "text": "Predict from the closest k\nIf \\(k = 5\\) …\n\ndf_train.loc[best[0:5]]\n\n    year  price  summer  har   sep  win  age\n19  1973   16.0    17.1  123  17.9  376   19\n22  1976   25.0    17.6  247  16.1  418   16\n21  1975   30.0    16.9  171  17.2  572   17\n10  1964   31.0    17.3   96  18.8  402   28\n17  1971   27.0    16.8  112  16.9  551   21\n\n\n\n\n…we would predict a price of…\n\ndf_train.loc[best[0:5], 'price'].mean()\n\nnp.float64(25.8)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#predict-from-the-closest-k-2",
    "href": "slides/08-knn.html#predict-from-the-closest-k-2",
    "title": "K-Nearest-Neighbors",
    "section": "Predict from the closest k",
    "text": "Predict from the closest k\nIf \\(k = 100\\) …\n\ndf_train.loc[best[0:100]]\n\n\n\nwe would predict a price of…\n\ndf_train.loc[best[0:100], 'price'].mean()\n\nnp.float64(28.814814814814813)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#predicting-unknown-prices",
    "href": "slides/08-knn.html#predicting-unknown-prices",
    "title": "K-Nearest-Neighbors",
    "section": "Predicting Unknown Prices",
    "text": "Predicting Unknown Prices\nFind the predicted 1992 price for all the unknown wines (year 1981 through 1991), with:\n\n\\(k = 1\\)\n\\(k = 5\\)\n\\(k = 10\\)\n\n\n\n\n\n\n\nA good place for a function!\n\n\nYou are performing the same process with different inputs of \\(k\\), so this seems like a reasonable place to write a function to save you time / reduce errors from copying and pasting.",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#specifying",
    "href": "slides/08-knn.html#specifying",
    "title": "K-Nearest-Neighbors",
    "section": "Specifying",
    "text": "Specifying\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n  preproc,\n  KNeighborsRegressor(n_neighbors = 5)\n  )\n\n\n\n\n\n\n\n\n\nRemember the preproc we made earlier!\n\n\n\npreproc = make_column_transformer(\n  (StandardScaler(), \n  make_column_selector(dtype_include = np.number)\n  ),\n  remainder = \"drop\"\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#fitting",
    "href": "slides/08-knn.html#fitting",
    "title": "K-Nearest-Neighbors",
    "section": "Fitting",
    "text": "Fitting\n\n\n\npipeline.fit(\n  y = df_train['price'], \n  X = df_train[features]\n  )\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt;)])),\n                ('kneighborsregressor', KNeighborsRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt;)])),\n                ('kneighborsregressor', KNeighborsRegressor())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt;)]) standardscaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x1395fc550&gt; StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor()",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#predicting",
    "href": "slides/08-knn.html#predicting",
    "title": "K-Nearest-Neighbors",
    "section": "Predicting",
    "text": "Predicting\n\npipeline.predict(X = df_test[features])\n\narray([25.8])",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#activity-2",
    "href": "slides/08-knn.html#activity-2",
    "title": "K-Nearest-Neighbors",
    "section": "Activity 2",
    "text": "Activity 2\nFind the predicted 1992 price for the wines with known prices, with:\n\n\\(k = 1\\)\n\\(k = 5\\)\n\\(k = 10\\)\n\nHow close was each prediction to the right answer?",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#knn",
    "href": "slides/08-knn.html#knn",
    "title": "K-Nearest-Neighbors",
    "section": "KNN",
    "text": "KNN\n\nWe have existing observations\n\\[(X_1, y_1), ... (X_n, y_n)\\] Where \\(X_i\\) is a set of features, and \\(y_i\\) is a target value.\nGiven a new observation \\(X_{new}\\), how do we predict \\(y_{new}\\)?\n\nFind the \\(k\\) values in \\((X_1, ..., X_n)\\) that are closest to \\(X_{new}\\)\nTake the average of the corresponding \\(y_i\\)’s to our five closest \\(X_i\\)’s.\nPredict \\(\\widehat{y}_{new}\\) = average of these \\(y_i\\)’s",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/08-knn.html#knn-big-questions",
    "href": "slides/08-knn.html#knn-big-questions",
    "title": "K-Nearest-Neighbors",
    "section": "KNN Big Questions",
    "text": "KNN Big Questions\n\nWhat is our definition of closest?\nWhat number should we use for \\(k\\)?\nHow do we evaluate the success of this approach?",
    "crumbs": [
      "Lecture Slides",
      "Week 5 Part 1 - KNN"
    ]
  },
  {
    "objectID": "slides/11-classification.html#choosing-a-best-model",
    "href": "slides/11-classification.html#choosing-a-best-model",
    "title": "Classification",
    "section": "Choosing a Best Model",
    "text": "Choosing a Best Model\n\nWe select a best model - aka best prediction procedure - by cross-validation.\nFeature selection: Which predictors should we include, and how should we preprocess them?\nModel selection: Should we use Linear Regression or KNN or Decision Trees or something else?\nHyperparameter tuning: Choosing model-specific settings, like \\(k\\) for KNN.\nEach candidate is a pipeline; use GridSearchCV() or cross_val_score() to score the options",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#breast-tissue-classification",
    "href": "slides/11-classification.html#breast-tissue-classification",
    "title": "Classification",
    "section": "Breast Tissue Classification",
    "text": "Breast Tissue Classification\nElectrical signals can be used to detect whether tissue is cancerous.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#analysis-goal",
    "href": "slides/11-classification.html#analysis-goal",
    "title": "Classification",
    "section": "Analysis Goal",
    "text": "Analysis Goal\nThe goal is to determine whether a sample of breast tissue is:\n\n\nNot Cancerous\n\nconnective tissue\nadipose tissue\nglandular tissue\n\n\n\n\nCancerous\n\ncarcinoma\nfibro-adenoma\nmastopathy",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#reading-in-the-data",
    "href": "slides/11-classification.html#reading-in-the-data",
    "title": "Classification",
    "section": "Reading in the Data",
    "text": "Reading in the Data\n\nimport pandas as pd\nbreast_df = pd.read_csv(\"https://datasci112.stanford.edu/data/BreastTissue.csv\")\n\n\n\n\n     Case # Class           I0  ...      Max IP          DR            P\n0         1   car   524.794072  ...   60.204880  220.737212   556.828334\n1         2   car   330.000000  ...   69.717361   99.084964   400.225776\n2         3   car   551.879287  ...   77.793297  253.785300   656.769449\n3         4   car   380.000000  ...   88.758446  105.198568   493.701814\n4         5   car   362.831266  ...   69.389389  103.866552   424.796503\n..      ...   ...          ...  ...         ...         ...          ...\n101     102   adi  2000.000000  ...  204.090347  478.517223  2088.648870\n102     103   adi  2600.000000  ...  418.687286  977.552367  2664.583623\n103     104   adi  1600.000000  ...  103.732704  432.129749  1475.371534\n104     105   adi  2300.000000  ...  178.691742   49.593290  2480.592151\n105     106   adi  2600.000000  ...  154.122604  729.368395  2545.419744\n\n[106 rows x 11 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#variables-of-interest",
    "href": "slides/11-classification.html#variables-of-interest",
    "title": "Classification",
    "section": "Variables of Interest",
    "text": "Variables of Interest\nWe will focus on two features:\n\n\\(I_0\\): impedivity at 0 kHz,\n\\(PA_{500}\\): phase angle at 500 kHz.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#visualizing-the-data",
    "href": "slides/11-classification.html#visualizing-the-data",
    "title": "Classification",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(data = breast_df, \n         mapping = aes(x = \"I0\", y = \"PA500\", color = \"Class\")) +\n  geom_point(size = 2) +\n  theme_bw() +\n  theme(legend_position = \"top\")\n    )",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#k-nearest-neighbors",
    "href": "slides/11-classification.html#k-nearest-neighbors",
    "title": "Classification",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\nWhat would we predict for someone with an \\(I_0\\) of 400 and a \\(PA_{500}\\) of 0.18?\n\nX_train = breast_df[[\"I0\", \"PA500\"]]\ny_train = breast_df[\"Class\"]\n\nX_unknown = pd.DataFrame({\"I0\": [400], \"PA500\": [.18]})\nX_unknown\n\n    I0  PA500\n0  400   0.18",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#k-nearest-neighbors-1",
    "href": "slides/11-classification.html#k-nearest-neighbors-1",
    "title": "Classification",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot() + \n  geom_point(data = breast_df, \n             mapping = aes(x = \"I0\", y = \"PA500\", color = \"Class\")) +\n  geom_point(data = X_unknown, \n             mapping = aes(x = \"I0\", y = \"PA500\"), size = 3) +\n  theme_bw() +\n  theme(legend_position = \"top\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#k-nearest-neighbors-2",
    "href": "slides/11-classification.html#k-nearest-neighbors-2",
    "title": "Classification",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\nThis process is almost identical to KNN Regression:\nSpecify\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors = 5, \n                         metric = \"euclidean\")\n    )\n\n\nFit\n\n\npipeline = pipeline.fit(X_train, y_train)\n\n\nPredict\n\n\n\npipeline.predict(X_unknown)\n\n\narray(['car'], dtype=object)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#probabilities",
    "href": "slides/11-classification.html#probabilities",
    "title": "Classification",
    "section": "Probabilities",
    "text": "Probabilities\nWhich of these two unknown points would we be more sure about in our guess?\n\n\n\nCode\nX_unknown = pd.DataFrame({\"I0\": [400, 2200], \"PA500\": [.18, 0.05]})\n\n(\n  ggplot() + \n  geom_point(data = breast_df, \n             mapping = aes(x = \"I0\", y = \"PA500\", color = \"Class\"), \n             size = 2) +\n  geom_point(data = X_unknown, \n             mapping = aes(x = \"I0\", y = \"PA500\"), size = 3) +\n  theme_bw() +\n  theme(legend_position = \"top\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#probabilities-1",
    "href": "slides/11-classification.html#probabilities-1",
    "title": "Classification",
    "section": "Probabilities",
    "text": "Probabilities\nInstead of returning a single predicted class, we can ask sklearn to return the predicted probabilities for each class.\n\npipeline.predict_proba(X_unknown)\n\narray([[0. , 0.6, 0. , 0.2, 0. , 0.2],\n       [1. , 0. , 0. , 0. , 0. , 0. ]])\n\n\n\npipeline.classes_\n\narray(['adi', 'car', 'con', 'fad', 'gla', 'mas'], dtype=object)\n\n\n\n\n\n\n\n\n\nTip\n\n\nHow did Scikit-Learn calculate these predicted probabilities?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#cross-validation-for-classification",
    "href": "slides/11-classification.html#cross-validation-for-classification",
    "title": "Classification",
    "section": "Cross-Validation for Classification",
    "text": "Cross-Validation for Classification\nWe need a different scoring method for classification.\nA simple scoring method is accuracy:\n\\[\\text{accuracy} = \\frac{\\text{# correct predictions}}{\\text{# predictions}}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#cross-validation-for-classification-1",
    "href": "slides/11-classification.html#cross-validation-for-classification-1",
    "title": "Classification",
    "section": "Cross-Validation for Classification",
    "text": "Cross-Validation for Classification\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(\n    pipeline, X_train, y_train,\n    scoring = \"accuracy\",\n    cv = 10)\n    \nscores\n\narray([0.63636364, 0.81818182, 0.45454545, 0.54545455, 0.63636364,\n       0.54545455, 0.5       , 0.6       , 0.4       , 0.7       ])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#cross-validation-for-classification-2",
    "href": "slides/11-classification.html#cross-validation-for-classification-2",
    "title": "Classification",
    "section": "Cross-Validation for Classification",
    "text": "Cross-Validation for Classification\nAs before, we can get an overall estimate of test accuracy by averaging the cross-validation accuracies:\n\nscores.mean()\n\nnp.float64(0.5836363636363637)\n\n\n\n\nBut! Accuracy is not always the best measure of a classification model!",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#confusion-matrix",
    "href": "slides/11-classification.html#confusion-matrix",
    "title": "Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_train_predicted = pipeline.predict(X_train)\n\nconfusion_matrix(y_train, y_train_predicted)\n\narray([[20,  0,  2,  0,  0,  0],\n       [ 0, 20,  0,  0,  0,  1],\n       [ 2,  0, 11,  1,  0,  0],\n       [ 0,  0,  0, 13,  1,  1],\n       [ 0,  0,  0,  3, 12,  1],\n       [ 0,  2,  0,  8,  3,  5]])\n\n\n\n\n\n\n\n\nRemember we made the pipeline previously!\n\n\n\npipeline = pipeline.fit(X_train, y_train)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#confusion-matrix-with-classes",
    "href": "slides/11-classification.html#confusion-matrix-with-classes",
    "title": "Classification",
    "section": "Confusion Matrix with Classes",
    "text": "Confusion Matrix with Classes\n\npd.DataFrame(confusion_matrix(y_train, y_train_predicted), \n             columns = pipeline.classes_, \n             index = pipeline.classes_)\n\n     adi  car  con  fad  gla  mas\nadi   20    0    2    0    0    0\ncar    0   20    0    0    0    1\ncon    2    0   11    1    0    0\nfad    0    0    0   13    1    1\ngla    0    0    0    3   12    1\nmas    0    2    0    8    3    5\n\n\n\n\n\n\n\n\nWhat group(s) were the hardest to predict?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#activity-1",
    "href": "slides/11-classification.html#activity-1",
    "title": "Classification",
    "section": "Activity",
    "text": "Activity\nUse a grid search and the accuracy score to find the best k-value for this modeling problem.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#case-study-credit-card-fraud",
    "href": "slides/11-classification.html#case-study-credit-card-fraud",
    "title": "Classification",
    "section": "Case Study: Credit Card Fraud",
    "text": "Case Study: Credit Card Fraud\n\nWe have a data set of credit card transactions from Vesta.\n\ndf_fraud = pd.read_csv(\"https://datasci112.stanford.edu/data/fraud.csv\")\n\n\n\n\n\n            card4   card6 P_emaildomain  ...    C13    C14  isFraud\n0            visa   debit     gmail.com  ...  637.0  114.0        0\n1            visa   debit                ...    3.0    1.0        0\n2            visa   debit     yahoo.com  ...    4.0    1.0        1\n3            visa   debit   hotmail.com  ...    0.0    0.0        0\n4            visa   debit     gmail.com  ...   20.0    1.0        0\n...           ...     ...           ...  ...    ...    ...      ...\n59049  mastercard   debit     gmail.com  ...    1.0    1.0        0\n59050  mastercard  credit     yahoo.com  ...    1.0    1.0        0\n59051  mastercard   debit    icloud.com  ...   15.0    2.0        0\n59052        visa   debit     gmail.com  ...    1.0    1.0        1\n59053  mastercard   debit                ...   84.0   17.0        0\n\n[59054 rows x 19 columns]\n\n\n\n\n\n\nGoal: Predict isFraud, where 1 indicates a fraudulent transaction.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#classification-model",
    "href": "slides/11-classification.html#classification-model",
    "title": "Classification",
    "section": "Classification Model",
    "text": "Classification Model\n\nWe can use \\(k\\)-nearest neighbors for classification:\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\nct = make_column_transformer(\n        (OneHotEncoder(handle_unknown = \"ignore\", sparse_output = False), \n        [\"card4\", \"card6\", \"P_emaildomain\"]),\n        remainder = \"passthrough\")\n        \npipeline = make_pipeline(\n  ct,\n  StandardScaler(),\n  KNeighborsClassifier(n_neighbors = 5)\n  )\n\n\n\n\n\n\n\nWhat is this transformer doing? What about the pipeline?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#training-a-classifier",
    "href": "slides/11-classification.html#training-a-classifier",
    "title": "Classification",
    "section": "Training a Classifier",
    "text": "Training a Classifier\nIsolating X and y for training data\n\nX_train = df_fraud.drop(\"isFraud\", axis = \"columns\")\ny_train = df_fraud[\"isFraud\"]\n\n\n\ncross_val_score(\n    pipeline,\n    X = X_train, \n    y = y_train,\n    scoring = \"accuracy\",\n    cv = 10\n    ).mean()\n\nnp.float64(0.9681816479631644)\n\n\n\n\n\nHow is the accuracy so high????",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#a-closer-look",
    "href": "slides/11-classification.html#a-closer-look",
    "title": "Classification",
    "section": "A Closer Look",
    "text": "A Closer Look\nLet’s take a closer look at the labels.\n\ny_train.value_counts()\n\nisFraud\n0    56935\n1     2119\nName: count, dtype: int64\n\n\n\nThe vast majority of transactions aren’t fraudulent!",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#imbalanced-data",
    "href": "slides/11-classification.html#imbalanced-data",
    "title": "Classification",
    "section": "Imbalanced Data",
    "text": "Imbalanced Data\nIf we just predicted that every transaction is normal, the accuracy would be \\(1 - \\frac{2119}{59054} = 0.964\\) or 96.4%.\n\n\nEven though such predictions would be accurate overall, it is inaccurate for fraudulent transactions.\n\nA good model is “accurate for every class.”",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#precision-and-recall-by-hand",
    "href": "slides/11-classification.html#precision-and-recall-by-hand",
    "title": "Classification",
    "section": "Precision and Recall by Hand",
    "text": "Precision and Recall by Hand\n\n\n\n\n\n\n\n\nPrecision is calculated as \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\).\n \nRecall is calculated as \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\).",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#precision-and-recall-by-hand-1",
    "href": "slides/11-classification.html#precision-and-recall-by-hand-1",
    "title": "Classification",
    "section": "Precision and Recall by Hand",
    "text": "Precision and Recall by Hand\n\nTo check our understanding of these definitions, let’s calculate a few precisions and recalls by hand.\nBut first we need to get the confusion matrix!\n\n\n\n\n\nCode\npipeline.fit(X_train, y_train);\ny_train_ = pipeline.predict(X_train)\nconfusion_matrix(y_train, y_train_)\n\n\narray([[56814,   121],\n       [ 1519,   600]])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#now-lets-calculate",
    "href": "slides/11-classification.html#now-lets-calculate",
    "title": "Classification",
    "section": "Now Let’s Calculate!",
    "text": "Now Let’s Calculate!\n\n\n\nCode\nconf_mat = pd.DataFrame(confusion_matrix(y_train, y_train_), \n             columns = pipeline.classes_, \n             index = pipeline.classes_)\n             \nconf_mat[\"Total\"] = conf_mat.sum(axis=1)\nconf_mat.loc[\"Total\"] = conf_mat.sum()\n\nconf_mat\n\n\n           0    1  Total\n0      56814  121  56935\n1       1519  600   2119\nTotal  58333  721  59054\n\n\n\n\n\nWhat is the (training) accuracy?\nWhat’s the precision for fraudulent transactions?\nWhat’s the recall for fraudulent transactions?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#trade-off-between-precision-and-recall",
    "href": "slides/11-classification.html#trade-off-between-precision-and-recall",
    "title": "Classification",
    "section": "Trade Off Between Precision and Recall",
    "text": "Trade Off Between Precision and Recall\nCan you imagine a classifier that always has 100% recall for class \\(c\\), no matter the data?\n\nIn general, if the model classifies more observations as \\(c\\),\n\nrecall (for class \\(c\\)) \\(\\uparrow\\)\nprecision (for class \\(c\\)) \\(\\downarrow\\)\n\n\n\nHow do we compare two classifiers, if one has higher precision and the other has higher recall?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#f1-score",
    "href": "slides/11-classification.html#f1-score",
    "title": "Classification",
    "section": "F1 Score",
    "text": "F1 Score\nThe F1 score combines precision and recall into a single score:\n\\[\\text{F1 score} = \\text{harmonic mean of precision and recall}\\] \\[= \\frac{2} {\\left( \\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}\\right)}\\]\n\nTo achieve a high F1 score, both precision and recall have to be high.\nIf either is low, then the harmonic mean will be low.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#estimating-test-precision-recall-and-f1",
    "href": "slides/11-classification.html#estimating-test-precision-recall-and-f1",
    "title": "Classification",
    "section": "Estimating Test Precision, Recall, and F1",
    "text": "Estimating Test Precision, Recall, and F1\n\nRemember that each class has its own precision, recall, and F1.\nBut Scikit-Learn requires that the scoring parameter be a single metric.\nFor this, we can average the score over the metrics:\n\n\"precision_macro\"\n\"recall_macro\"\n\"f1_macro\"",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#f1-score-1",
    "href": "slides/11-classification.html#f1-score-1",
    "title": "Classification",
    "section": "F1 Score",
    "text": "F1 Score\n\ncross_val_score(\n    pipeline,\n    X = X_train,\n    y = y_train,\n    scoring = \"f1_macro\",\n    cv = 10\n    ).mean()\n\nnp.float64(0.647384563849488)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#precision-recall-curve",
    "href": "slides/11-classification.html#precision-recall-curve",
    "title": "Classification",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\nAnother way to illustrate the trade off between precision and recall is to graph the precision-recall curve.\nFirst, we need the predicted probabilities.\n\ny_train_probs_ = pipeline.predict_proba(X_train)\ny_train_probs_\n\narray([[1. , 0. ],\n       [1. , 0. ],\n       [0.4, 0.6],\n       ...,\n       [1. , 0. ],\n       [0.8, 0.2],\n       [1. , 0. ]], shape=(59054, 2))",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#precision-recall-curve-1",
    "href": "slides/11-classification.html#precision-recall-curve-1",
    "title": "Classification",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\n\nBy default, Scikit-Learn classifies a transaction as fraud if this probability is \\(&gt; 0.5\\).\nWhat if we instead used a threshold t other than \\(0.5\\)?\nDepending on which t we pick, we’ll get a different precision and recall.\n\nWe can graph this trade off!",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#precision-recall-curve-2",
    "href": "slides/11-classification.html#precision-recall-curve-2",
    "title": "Classification",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\nLet’s graph the precision-recall curve together in a Colab.\nhttps://colab.research.google.com/drive/1T-0iQOQZFldHNmOXdZf4GU0b8j3kWMc_?usp=sharing",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/11-classification.html#takeaways-1",
    "href": "slides/11-classification.html#takeaways-1",
    "title": "Classification",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nWe can do KNN for Classification by letting the nearest neighbors “vote”\nThe number of votes is a “probability”\nA classification model must be evaluated differently than a regression model.\nOne possible metric is accuracy, but this is a bad choice in situations with imbalanced data.\nPrecision measures “if we say it’s in Class A, is it really?”\nRecall measures “if it’s really in Class A, did we find it?”\nF1 Score is a balance of precision and recall\nMacro F1 Score averages the F1 scores of all classes",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#predictive-modeling",
    "href": "slides/13-kmeans.html#predictive-modeling",
    "title": "Unsupervised Learning with K-Means",
    "section": "Predictive Modeling",
    "text": "Predictive Modeling\n\n\n\nIn predictive modeling, a.k.a. supervised machine learning, we have a target variable we want to predict.\nWe expect to have observations where we know the predictors but not the target.\nOur goal is to choose a modeling procedure to guess the value of the target variable based on the predictors.\nWe use cross-validation to estimate the test error of various procedure options.\nWe might compare different:\n\nfeature sets\npreprocessing choices\nmodel specifications/algorithms\ntuning parameters",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#unsupervised-learning-1",
    "href": "slides/13-kmeans.html#unsupervised-learning-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\nIn unsupervised situations, we do not have a target variable \\(y\\).\nWe do still have features that we observe.\nOur goal: Find an interesting structure in the features we have.\n\n\n\n\n\n\n\n\n\nSupervised vs Unsupervised\n\n\nThink of children playing with Legos. They might be supervised by parents who help them follow instructions, or they might be left alone to build whatever they want!",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#clustering",
    "href": "slides/13-kmeans.html#clustering",
    "title": "Unsupervised Learning with K-Means",
    "section": "Clustering",
    "text": "Clustering\n\n\n\nNearly all unsupervised learning algorithms can be called clustering.\nThe goal is to use the observed features (columns) to sort the observations (rows) into similar clusters (groups).\nFor example: Suppose I take all of your grades in the gradebook as features and then use these to find clusters of students. These clusters might represent…\n\npeople who studied together\npeople who are in the same section\npeople who have the same major or background\n… or none of the above!",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#applications-for-clustering",
    "href": "slides/13-kmeans.html#applications-for-clustering",
    "title": "Unsupervised Learning with K-Means",
    "section": "Applications for Clustering",
    "text": "Applications for Clustering\n\n\n\nEcology: An ecologist wants to group organisms into types to define different species. (rows = organisms; features = habitat, size, etc.)\nBiology: A geneticist wants to know which groups of genes tend to be activated at the same time. (rows = genes; features = activation at certain times)\nMarket Segmentation: A business wants to group their customers into types. (rows = customers, features = age, location, etc.)\nLanguage: A linguist might want to identify different uses of ambiguous words like “set” or “run”. (rows = words; features = other words they are used with)\nDocuments: A historian might want to find groups of articles that are on similar topics. (rows = articles; features = tf-idf transformed n-grams)",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#the-k-means-algorithm",
    "href": "slides/13-kmeans.html#the-k-means-algorithm",
    "title": "Unsupervised Learning with K-Means",
    "section": "The K-Means Algorithm",
    "text": "The K-Means Algorithm\n\nIdea: Two observations are similar if they are close in distance.\n\n\nDoes this sound familiar?!\n\n\n\nQ: How do we find “groups” of observations?\nA: We should look for groups of observations that are close to the same centroid.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#the-k-means-algorithm-1",
    "href": "slides/13-kmeans.html#the-k-means-algorithm-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "The K-Means Algorithm",
    "text": "The K-Means Algorithm\nProcedure (3-means):\n\nChoose 3 random observations to be the initial centroids.\nFor each observation, determine which is the closest centroid.\nCreate 3 clusters based on the closest centroid.\nFind the new centroid of each cluster.\nRepeat until the clusters don’t change.\n\n\nhttps://www.naftaliharris.com/blog/visualizing-k-means-clustering/",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#example-penguin-data",
    "href": "slides/13-kmeans.html#example-penguin-data",
    "title": "Unsupervised Learning with K-Means",
    "section": "Example: Penguin Data",
    "text": "Example: Penguin Data\n\n\n\nCode\ndf_penguins = pd.read_csv(\"https://dlsun.github.io/stats112/data/penguins.csv\")\ndf_penguins\n\n\n       species     island  bill_length_mm  ...  body_mass_g     sex  year\n0       Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1       Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2       Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3       Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n4       Adelie  Torgersen            36.7  ...       3450.0  female  2007\n..         ...        ...             ...  ...          ...     ...   ...\n339  Chinstrap      Dream            55.8  ...       4000.0    male  2009\n340  Chinstrap      Dream            43.5  ...       3400.0  female  2009\n341  Chinstrap      Dream            49.6  ...       3775.0    male  2009\n342  Chinstrap      Dream            50.8  ...       4100.0    male  2009\n343  Chinstrap      Dream            50.2  ...       3775.0  female  2009\n\n[344 rows x 8 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#penguin-data-investigating-missing-values",
    "href": "slides/13-kmeans.html#penguin-data-investigating-missing-values",
    "title": "Unsupervised Learning with K-Means",
    "section": "Penguin Data: Investigating Missing Values",
    "text": "Penguin Data: Investigating Missing Values\nIt looks like there are missing values in these data…\n\n\ndf_penguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#penguin-data-removing-missing-values",
    "href": "slides/13-kmeans.html#penguin-data-removing-missing-values",
    "title": "Unsupervised Learning with K-Means",
    "section": "Penguin Data: Removing Missing Values",
    "text": "Penguin Data: Removing Missing Values\nFor this analysis, we are interested in a penguin’s bill length and flipper length, so I will remove the missing values from those columns.\n\n\ndf_penguins = (\n  df_penguins\n  .dropna(subset = [\"bill_length_mm\", \"flipper_length_mm\"])\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#penguin-data-plot",
    "href": "slides/13-kmeans.html#penguin-data-plot",
    "title": "Unsupervised Learning with K-Means",
    "section": "Penguin Data: Plot",
    "text": "Penguin Data: Plot\n\n\nCode\n(\n  ggplot(data = df_penguins, \n         mapping = aes(x = \"bill_length_mm\", y = \"flipper_length_mm\")) + \n         geom_point() + \n         theme_bw() +\n         labs(x = \"Bill Length (mm)\", \n              y = \"Flipper Length (mm)\"\n              )\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-0-standardize-the-data",
    "href": "slides/13-kmeans.html#step-0-standardize-the-data",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 0: Standardize the data",
    "text": "Step 0: Standardize the data\n\n \n\n\nWhy is this important?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-0-standardize-the-data-1",
    "href": "slides/13-kmeans.html#step-0-standardize-the-data-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 0: Standardize the data",
    "text": "Step 0: Standardize the data\nSpecify\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().set_output(transform = \"pandas\")\n\n\nFit & Transform\n\n\ndf_scaled = (\n  scaler\n  .fit_transform(df_penguins[[\"bill_length_mm\", \"flipper_length_mm\"]])\n  )\n\ndf_scaled.head()\n\n   bill_length_mm  flipper_length_mm\n0       -0.884499          -1.418347\n1       -0.811126          -1.062250\n2       -0.664380          -0.421277\n4       -1.324737          -0.563715\n5       -0.847812          -0.777373",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-1-choose-3-random-points-to-be-centroids",
    "href": "slides/13-kmeans.html#step-1-choose-3-random-points-to-be-centroids",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 1: Choose 3 random points to be centroids",
    "text": "Step 1: Choose 3 random points to be centroids\n\n\ncentroids = df_scaled.sample(n = 3, random_state = 1234)\ncentroids.index = [\"orange\", \"purple\", \"green\"]\n\ncentroids\n\n        bill_length_mm  flipper_length_mm\norange       -0.425917          -0.634935\npurple        0.674678          -0.421277\ngreen        -0.077396           0.860670",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-1-choose-3-random-points-to-be-centroids-1",
    "href": "slides/13-kmeans.html#step-1-choose-3-random-points-to-be-centroids-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 1: Choose 3 random points to be centroids",
    "text": "Step 1: Choose 3 random points to be centroids\n\n\n\nCode\n(\n  ggplot(data = df_scaled, \n         mapping = aes(x = \"bill_length_mm\", y = \"flipper_length_mm\")) + \n         geom_point() + \n         geom_point(data = centroids, color = centroids.index, size = 4) +\n         theme_bw() +\n         labs(x = \"Bill Length (mm)\", \n              y = \"Flipper Length (mm)\"\n              )\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid",
    "href": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 2: Assign each point to nearest centroid",
    "text": "Step 2: Assign each point to nearest centroid\n\n\nfrom sklearn.metrics import pairwise_distances\n\ndists = pairwise_distances(df_scaled, centroids)\ndists[1:5]\n\narray([[0.57531222, 1.61816529, 2.0581506 ],\n       [0.32017798, 1.3390574 , 1.40994282],\n       [0.90163652, 2.00448173, 1.89333953],\n       [0.44529088, 1.5635793 , 1.8101736 ]])",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid-1",
    "href": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 2: Assign each point to nearest centroid",
    "text": "Step 2: Assign each point to nearest centroid\n\n\nclosest_centroid = dists.argmin(axis = 1)\nclosest_centroid\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1])\n\n\n\n\n\n\n\n\n\naxis = 1 operates on each row",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid-2",
    "href": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid-2",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 2: Assign each point to nearest centroid",
    "text": "Step 2: Assign each point to nearest centroid\n\ndf_scaled.index = centroids.index[closest_centroid]\ndf_scaled.head(n = 10)\n\n        bill_length_mm  flipper_length_mm\norange       -0.884499          -1.418347\norange       -0.811126          -1.062250\norange       -0.664380          -0.421277\norange       -1.324737          -0.563715\norange       -0.847812          -0.777373\norange       -0.921185          -1.418347\norange       -0.866155          -0.421277\norange       -1.801661          -0.563715\norange       -0.352544          -0.777373\norange       -1.122961          -1.062250",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid-3",
    "href": "slides/13-kmeans.html#step-2-assign-each-point-to-nearest-centroid-3",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 2: Assign each point to nearest centroid",
    "text": "Step 2: Assign each point to nearest centroid\n\n\n\nCode\n(\n  ggplot(data = df_scaled, \n         mapping = aes(x = \"bill_length_mm\", y = \"flipper_length_mm\")) + \n         geom_point(color = df_scaled.index) + \n         geom_point(data = centroids, color = centroids.index, size = 3) +\n         theme_bw() +\n         labs(x = \"Bill Length (mm)\", \n              y = \"Flipper Length (mm)\"\n              )\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-3-find-new-centroids",
    "href": "slides/13-kmeans.html#step-3-find-new-centroids",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 3: Find new centroids",
    "text": "Step 3: Find new centroids\n\ncentroids = df_scaled.groupby(df_scaled.index).mean()\ncentroids\n\n        bill_length_mm  flipper_length_mm\ngreen         0.628892           1.140501\norange       -0.957138          -0.821529\npurple        0.980022          -0.332526\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nAre these centroids observations in df_scaled?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-3-find-new-centroids-1",
    "href": "slides/13-kmeans.html#step-3-find-new-centroids-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 3: Find new centroids",
    "text": "Step 3: Find new centroids\n\n\n\nCode\n(\n  ggplot(data = df_scaled, \n         mapping = aes(x = \"bill_length_mm\", y = \"flipper_length_mm\")) + \n         geom_point(color = df_scaled.index) + \n         geom_point(data = centroids, color = centroids.index, size = 3) +\n         theme_bw() +\n         labs(x = \"Bill Length (mm)\", \n              y = \"Flipper Length (mm)\"\n              )\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#step-4-repeat-over-and-over",
    "href": "slides/13-kmeans.html#step-4-repeat-over-and-over",
    "title": "Unsupervised Learning with K-Means",
    "section": "Step 4: Repeat over and over!",
    "text": "Step 4: Repeat over and over!\n\n\nfor i in range(1, 6):\n  dists = pairwise_distances(df_scaled, centroids)\n  df_scaled.index = centroids.index[closest_centroid]\n  centroids = df_scaled.groupby(df_scaled.index).mean()\n  print(centroids)\n\n        bill_length_mm  flipper_length_mm\ngreen        -0.957138          -0.821529\norange        0.980022          -0.332526\npurple        0.628892           1.140501\n        bill_length_mm  flipper_length_mm\ngreen        -0.957138          -0.821529\norange        0.980022          -0.332526\npurple        0.628892           1.140501\n        bill_length_mm  flipper_length_mm\ngreen        -0.957138          -0.821529\norange        0.980022          -0.332526\npurple        0.628892           1.140501\n        bill_length_mm  flipper_length_mm\ngreen        -0.957138          -0.821529\norange        0.980022          -0.332526\npurple        0.628892           1.140501\n        bill_length_mm  flipper_length_mm\ngreen        -0.957138          -0.821529\norange        0.980022          -0.332526\npurple        0.628892           1.140501\n\n\n\n\n\n\n\n\n\n\nAt what point do we stop finding new centroids / reassigning points?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#k-means-in-sklearn",
    "href": "slides/13-kmeans.html#k-means-in-sklearn",
    "title": "Unsupervised Learning with K-Means",
    "section": "K-means in sklearn",
    "text": "K-means in sklearn\nSpecify\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\n\nmodel = KMeans(n_clusters = 3, random_state = 1234)\n\npipeline = make_pipeline(\n    StandardScaler(),\n    model\n)\n\n\nFit\n\n\npipeline.fit(df_penguins[[\"bill_length_mm\", \"flipper_length_mm\"]]);",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#k-means-in-sklearn-1",
    "href": "slides/13-kmeans.html#k-means-in-sklearn-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "K-means in sklearn",
    "text": "K-means in sklearn\n\nclusters = model.labels_\nclusters\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2,\n       2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2], dtype=int32)",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#interpreting-k-means",
    "href": "slides/13-kmeans.html#interpreting-k-means",
    "title": "Unsupervised Learning with K-Means",
    "section": "Interpreting K-Means",
    "text": "Interpreting K-Means\nThe key takeaway here is the cluster centers:\n\ncentroids = model.cluster_centers_\ncentroids\n\narray([[-0.95823619, -0.80850204],\n       [ 0.66658932,  1.14779076],\n       [ 0.93807532, -0.37008779]])\n\n\n\n\n\nCluster 1 has a short bill, shortish flipper\nCluster 2 has medium bill, long flipper\nCluster 3 has long bill, and fairly average flipper",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#interpreting-k-means-1",
    "href": "slides/13-kmeans.html#interpreting-k-means-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Interpreting K-Means",
    "text": "Interpreting K-Means\nWe also might check if these clusters match any labels that we already know:\n\n\nCode\nresults = pd.DataFrame({\n  \"cluster\": clusters,\n  \"species\": df_penguins['species']\n})\n\n(\n  results\n  .groupby(\"species\")[\"cluster\"]\n  .value_counts()\n  .unstack()\n  .fillna(0)\n  )\n\n\ncluster        0      1     2\nspecies                      \nAdelie     146.0    1.0   4.0\nChinstrap    5.0    4.0  59.0\nGentoo       0.0  122.0   1.0",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#activity",
    "href": "slides/13-kmeans.html#activity",
    "title": "Unsupervised Learning with K-Means",
    "section": "Activity",
    "text": "Activity\n\nFit a 3-means model using all the numeric predictors in the penguins data.\n\n\nDescribe what each cluster represents.\nDo these clusters match up to the species?\n\n\nNext, fit a 6-means model.\n\n\nDo those clusters match up to species and island?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/13-kmeans.html#takeaways-1",
    "href": "slides/13-kmeans.html#takeaways-1",
    "title": "Unsupervised Learning with K-Means",
    "section": "Takeaways",
    "text": "Takeaways\n\nUnsupervised learning is a way to find structure in data.\nK-means is the most common clustering method.\nWe have to choose K ahead of time.\n\n\n\n\n\n\n\nThis is a big problem!\n\n\nWhy can’t we tune????",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 1 - Unsupervised Learning"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#upcoming-tasks-deadlines",
    "href": "slides/09-modeling.html#upcoming-tasks-deadlines",
    "title": "Introduction to Modeling",
    "section": "Upcoming Tasks / Deadlines",
    "text": "Upcoming Tasks / Deadlines\n\nForm a group of up to 3 students\nFind a dataset\nForm some research questions\nDo some preliminary explorations of the data\nWrite up your project proposal\n\n\n\n\n\n\n\nTip\n\n\nThe Final Project instructions page has suggestions for where to find datasets.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#project-proposal---due-sunday-february-23",
    "href": "slides/09-modeling.html#project-proposal---due-sunday-february-23",
    "title": "Introduction to Modeling",
    "section": "Project Proposal - Due Sunday, February 23",
    "text": "Project Proposal - Due Sunday, February 23\n\n\nYour group member names.\nInformation about the dataset(s) you intend to analyze:\n\nWhere are the data located?\nWho collected the data and why?\nWhat information (variables) are in the dataset?\n\nResearch Questions: You should have one primary research question and a few secondary questions\nPreliminary exploration of your dataset(s): A few simple plots or summary statistics that relate to the variables you plan to study.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#steps-for-data-analysis",
    "href": "slides/09-modeling.html#steps-for-data-analysis",
    "title": "Introduction to Modeling",
    "section": "Steps for Data Analysis",
    "text": "Steps for Data Analysis\n\nRead and then clean the data\n\nAre there missing values? Will we drop those rows, or replace the missing values with something?\nAre there quantitative variables that Python thinks are categorical?\nAre there categorical variables that Python thinks are quantitative?\nAre there any anomalies in the data that concern you?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#steps-for-data-analysis-contd",
    "href": "slides/09-modeling.html#steps-for-data-analysis-contd",
    "title": "Introduction to Modeling",
    "section": "Steps for Data Analysis (cont’d)",
    "text": "Steps for Data Analysis (cont’d)\n\nExplore the data by visualizing and summarizing.\n\nDifferent approaches for different combos of quantitative and categorical variables\nThink about conditional calculations (split-apply-combine)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#steps-for-data-analysis-contd-1",
    "href": "slides/09-modeling.html#steps-for-data-analysis-contd-1",
    "title": "Introduction to Modeling",
    "section": "Steps for Data Analysis (cont’d)",
    "text": "Steps for Data Analysis (cont’d)\n\nIdentify a research question of interest.\nPerform preprocessing steps\n\nShould we scale the quantitative variables?\nShould we one-hot-encode the categorical variables?\nShould we log-transform any variables?\n\nMeasure similarity between observations by calculating distances.\n\nWhich features should be included?\nWhich distance metric should we use?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#modeling",
    "href": "slides/09-modeling.html#modeling",
    "title": "Introduction to Modeling",
    "section": "Modeling",
    "text": "Modeling\nEvery analysis we will do assumes a structure like:\n\n\n(output) = f(input) + (noise)\n\n\n… or, if you prefer…\n\ntarget = f(predictors) + noise",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#generative-process",
    "href": "slides/09-modeling.html#generative-process",
    "title": "Introduction to Modeling",
    "section": "Generative Process",
    "text": "Generative Process\nIn either case: we are trying to reconstruct information in data, and we are hindered by random noise.\nThe function \\(f\\) might be very simple…\n\n\\[y_i = \\mu + \\epsilon_i\\]\n“A person’s height is the true average height of people in the world, plus some randomness.”",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#generative-process-1",
    "href": "slides/09-modeling.html#generative-process-1",
    "title": "Introduction to Modeling",
    "section": "Generative Process",
    "text": "Generative Process\n… or more complex…\n\\[y_i = 0.5*x_{1i} + 0.5*x_{2i} + \\epsilon_i\\]\n“A person’s height is equal to the average of their biological mother’s height and biological father’s height, plus some randomness”\n\n\n\n\n\n\nTip\n\n\nDo you think there is “more randomness” in the first function or this one?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#generative-process-2",
    "href": "slides/09-modeling.html#generative-process-2",
    "title": "Introduction to Modeling",
    "section": "Generative Process",
    "text": "Generative Process\n… or extremely, ridiculously complex…",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#generative-process-3",
    "href": "slides/09-modeling.html#generative-process-3",
    "title": "Introduction to Modeling",
    "section": "Generative Process",
    "text": "Generative Process\n… and it doesn’t have to be a mathematical function at all!\n\nThe process can just a procedure:\n\n\\[y_i = \\text{(average of heights of 5 people with most similar weights)} + \\epsilon_i\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#modeling-1",
    "href": "slides/09-modeling.html#modeling-1",
    "title": "Introduction to Modeling",
    "section": "Modeling",
    "text": "Modeling\n\n\nOur goal is to reconstruct or estimate or approximate the function / process \\(f\\) based on training data.\n\nFor example: Instead of the 5 most similar weights in the whole world, we can estimate with the 5 most similar weights in our training set.\n\nInstead of committing to one \\(f\\) to estimate, we might propose many options and see which one “leaves behind” the least randomness (has the smallest errors).",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#setup",
    "href": "slides/09-modeling.html#setup",
    "title": "Introduction to Modeling",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv(\"https://dlsun.github.io/pods/data/bordeaux.csv\")\n\n\n\n\nTraining Data\n\nknown = df[\"year\"] &lt; 1981\ndf_train = df[known]\n\n\n\n\nTesting Data\n\nunknown = df[\"year\"] &gt; 1980\ndf_test = df[unknown]",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#knn-revisited",
    "href": "slides/09-modeling.html#knn-revisited",
    "title": "Introduction to Modeling",
    "section": "KNN Revisited",
    "text": "KNN Revisited\n\nColumn Transformer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfeatures = ['summer', 'har', 'sep', 'win', 'age']\n\nct = make_column_transformer(\n  (StandardScaler(), features),\n  remainder = \"drop\"\n)\n\nPipeline\n\npipeline = make_pipeline(\n  ct,\n  KNeighborsRegressor(n_neighbors = 5)\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#knn-revisited-1",
    "href": "slides/09-modeling.html#knn-revisited-1",
    "title": "Introduction to Modeling",
    "section": "KNN Revisited",
    "text": "KNN Revisited\n\nFit\n\n\npipeline.fit(X = df_train,\n             y = df_train['price'])\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor()",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#knn-revisited-2",
    "href": "slides/09-modeling.html#knn-revisited-2",
    "title": "Introduction to Modeling",
    "section": "KNN Revisited",
    "text": "KNN Revisited\n\nPredict\n\npred_y_train = pipeline.predict(X = df_test)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#measuring-error",
    "href": "slides/09-modeling.html#measuring-error",
    "title": "Introduction to Modeling",
    "section": "Measuring Error",
    "text": "Measuring Error\n\nThe most common way to measure “leftover noise” is the sum of squared error or equivalently, the mean squared error.\n\n\n\n\npred_y_train = pipeline.predict(X = df_train)\n\nresults = pd.DataFrame({\n  \"real_prices\": df_train['price'],\n  \"predicted_prices\": pred_y_train,\n})\nresults[\"squared error\"] = (results[\"predicted_prices\"] - results[\"real_prices\"])**2\nresults\n\n\n\n    real_prices  predicted_prices  squared error\n0          37.0              36.2           0.64\n1          63.0              41.4         466.56\n2          45.0              46.6           2.56\n3          22.0              28.8          46.24\n4          18.0              46.4         806.56\n5          66.0              35.2         948.64\n6          14.0              13.0           1.00\n7         100.0              56.6        1883.56\n8          33.0              40.4          54.76\n9          17.0              16.0           1.00\n10         31.0              37.0          36.00\n11         11.0              12.2           1.44\n12         47.0              32.0         225.00\n13         19.0              26.0          49.00\n14         11.0              11.8           0.64\n15         12.0              18.2          38.44\n16         40.0              29.8         104.04\n17         27.0              24.8           4.84\n18         10.0              13.6          12.96\n19         16.0              28.8         163.84\n20         11.0              19.8          77.44\n21         30.0              24.0          36.00\n22         25.0              18.8          38.44\n23         11.0              25.0         196.00\n24         27.0              18.4          73.96\n25         21.0              18.4           6.76\n26         14.0              25.8         139.24",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#measuring-error-1",
    "href": "slides/09-modeling.html#measuring-error-1",
    "title": "Introduction to Modeling",
    "section": "Measuring Error",
    "text": "Measuring Error\nThe most common way to measure “leftover noise” is the sum of squared error or equivalently, the mean squared error.\n\nresults[\"squared error\"].mean()\n\nnp.float64(200.57629629629628)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#best-k",
    "href": "slides/09-modeling.html#best-k",
    "title": "Introduction to Modeling",
    "section": "Best K",
    "text": "Best K\nNow let’s try it for some different values of \\(k\\)\n\nfor k in [1, 3, 5, 10, 25]:\n  pipeline = make_pipeline(\n    ct,\n    KNeighborsRegressor(n_neighbors = k)\n    )\n  pipeline = pipeline.fit(X = df_train, y = df_train['price'])\n  pred_y_train = pipeline.predict(X = df_train)\n  ((df_train['price'] - pred_y_train)**2).mean()\n\nnp.float64(0.0)\nnp.float64(123.2304526748971)\nnp.float64(200.57629629629628)\nnp.float64(241.37518518518516)\nnp.float64(378.9575703703703)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#training-error-versus-test-error",
    "href": "slides/09-modeling.html#training-error-versus-test-error",
    "title": "Introduction to Modeling",
    "section": "Training Error Versus Test Error",
    "text": "Training Error Versus Test Error\n\n\nOh no! Why did we get an error of 0 for \\(k = 1\\)?\nBecause the closest wine in the training set is… itself.\nSo, our problem is:\n\nIf we predict on the new data, we don’t know the true prices and we can’t evaluate our models.\nIf we predict on the training data, we are “cheating,” because we are using the data to both train and test.\n\nSolution: Let’s make a pretend test data set!",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#another-test-training-split",
    "href": "slides/09-modeling.html#another-test-training-split",
    "title": "Introduction to Modeling",
    "section": "Another Test / Training Split",
    "text": "Another Test / Training Split\n\ntest = (df[\"year\"] &gt; 1970) & (df[\"year\"] &lt; 1981)\ntrain = df[\"year\"] &lt; 1971\n\ndf_train_new = df[train].copy()\ndf_test_new = df[test].copy()\n\n\n\nWe will train on the years up to 1970\nWe will test on the years 1971 to 1980\nWe will evaluate based on model performance on the test data.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#try-again-best-k",
    "href": "slides/09-modeling.html#try-again-best-k",
    "title": "Introduction to Modeling",
    "section": "Try Again: Best K",
    "text": "Try Again: Best K\n\n\nfor k in range(1,15):\n  pipeline = make_pipeline(\n    ct,\n    KNeighborsRegressor(n_neighbors = k))\n \n  pipeline = pipeline.fit(X = df_train_new, \n                          y = df_train_new['price'])\n  \n  pred_y_test = pipeline.predict(X = df_test_new)\n  \n  print(str(k) + \":\" + str(((df_test_new['price'] - pred_y_test)**2).mean()))\n\n1:183.0\n2:139.85\n3:123.78888888888892\n4:159.34375\n5:121.81199999999998\n6:83.28333333333333\n7:86.08163265306123\n8:86.465625\n9:73.12839506172841\n10:72.263\n11:89.03801652892564\n12:133.99791666666667\n13:162.23136094674555\n14:169.534693877551",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#tuning",
    "href": "slides/09-modeling.html#tuning",
    "title": "Introduction to Modeling",
    "section": "Tuning",
    "text": "Tuning\n\nHere we tried the same type of model (KNN) each time.\nBut we tried different models because we used different values of \\(k\\).\nThis is called model tuning!",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#activity",
    "href": "slides/09-modeling.html#activity",
    "title": "Introduction to Modeling",
    "section": "Activity",
    "text": "Activity\nPerform tuning for a KNN model, but with all possible values of k.\nDo this for three column transformers:\n\nUsing all predictors.\nUsing just winter rainfall and summer temperature.\nUsing only age.\n\nWhich of the many model options performed best?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#things-to-think-about",
    "href": "slides/09-modeling.html#things-to-think-about",
    "title": "Introduction to Modeling",
    "section": "Things to think about",
    "text": "Things to think about\n\n\n\nWhat other types of models could we have tried?\n\nLinear regression, decision tree, neural network, …\n\nWhat other column transformers could we have tried?\n\nDifferent combinations of variables, different standardizing, log transforming…\n\nWhat other measures of error could we have tried?\n\nMean absolute error, log-error, percent error, …\n\nWhat if we had used a different test set?\n\nComing soon: Cross-validation\n\nWhat if our target variable was categorical?\n\nLogistic regression, multinomial regression, decision trees,…",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#modeling-2",
    "href": "slides/09-modeling.html#modeling-2",
    "title": "Introduction to Modeling",
    "section": "Modeling",
    "text": "Modeling\nFor each model proposed:\n\n\nEstablish a pipeline with transformers and a model.\nFit the pipeline on the training data (with known outcome).\nPredict with the fitted pipeline on test data (with known outcome).\nEvaluate our success (i.e., measure noise “left over”).\n\n\n\nThen:\n\n\nSelect the best model.\nFit on all the data.\nPredict on any future data (with unknown outcome).",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "slides/09-modeling.html#big-decisions",
    "href": "slides/09-modeling.html#big-decisions",
    "title": "Introduction to Modeling",
    "section": "Big decisions",
    "text": "Big decisions\n\nWhich models to try\nWhich column transformers to try\nHow much to tune\nHow to measure the “success” of a model",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 1 - Introduction to Modeling"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#course-objectives",
    "href": "course-materials/syllabus.html#course-objectives",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Course Objectives",
    "text": "Course Objectives\nAfter taking this course, you will be able to:\n\nAcquire and process tabular, textual, hierarchical, and geospatial data.\nUncover patterns by summarizing and visualizing data.\nApply machine learning to answer real-world prediction problems.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#prerequisites",
    "href": "course-materials/syllabus.html#prerequisites",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Prerequisites",
    "text": "Prerequisites\nI expect you to enter this class with:\n\nBasic knowledge of Python and computer programming concepts.\nFamiliarity with computers and technology (e.g., Internet browsing, word processing, opening/saving files, converting files to PDF format, sending and receiving e-mail, etc.).\nA positive attitude, a curious mind, and a respect for ethical use of data science tools.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#lecture-check-ins-5",
    "href": "course-materials/syllabus.html#lecture-check-ins-5",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Lecture Check-Ins (5%)",
    "text": "Lecture Check-Ins (5%)\nEvery lecture will be accompanied by a simple “Check-In Quiz” on Canvas. This will ask you to input a few answers covered in lecture, and one short mid-lecture practice exercise. Infinite submissions are allowed without penalty, so there is no reason anyone should not get 100% in this category.\nIf you miss lecture, you can still complete the Check-Ins on your own time, by midnight the next day.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#lab-attendance-and-activities-10",
    "href": "course-materials/syllabus.html#lab-attendance-and-activities-10",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Lab Attendance and Activities (10%)",
    "text": "Lab Attendance and Activities (10%)\nAttendance and participation in the lab portion of class is required. Do not take this class if you cannot commit to attending every lecture and every lab.\n\nSwapping Lab Sections\nIf you cannot make it to your assigned lab, but you can attend another section’s lab on the same day, please e-mail Dr. Theobold to inform them that you wish to swap lab sections. You may do this two times throughout the quarter.\n\n\nMissing Lab\nIf you cannot make it to any lab sections that day, you may complete the Colab assignment from lab on your own, and e-mail a PDF to Dr. Theobold by midnight that night (the day you missed class). Dr. Theobold will grade your Colab, and this score will replace your attendance for that day. You may do this two times throughout the quarter.\nTo allow for emergencies, we will also forgive one absence at the end of the quarter.\n\n\n\n\n\n\nNote\n\n\n\nWe are effectively allowing you to miss / reschedule up to three labs out of 10. If you need to miss more than three, then you will need to retake this course next quarter.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#lab-assignments-25",
    "href": "course-materials/syllabus.html#lab-assignments-25",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Lab Assignments (25%)",
    "text": "Lab Assignments (25%)\nEach week, you will be assigned longer homeworks, which will ask you to analyze a real-world data scenario. These assignments are due every Saturday at 11:59pm. See the Late Work section below for information on extensions or deductions for late submissions.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#exams-15-each",
    "href": "course-materials/syllabus.html#exams-15-each",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Exams (15% each)",
    "text": "Exams (15% each)\nYou will have two in-class exams, in Week 5 and Week 10. These will each cover half of the class material. Except in very extreme unforeseeable circumstances, no alternate exams will be given; please plan to be in class these days.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#project-30",
    "href": "course-materials/syllabus.html#project-30",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Project (30%)",
    "text": "Project (30%)\nAt the end of the quarter, you will formally present a poster of your findings on a real-world data question. To ensure that you start thinking about your project early, you are required to submit a 1-page abstract proposal in Week 7.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#communication",
    "href": "course-materials/syllabus.html#communication",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Communication",
    "text": "Communication\nFor questions of general interest, such as deadline clarifications or conceptual questions, please use the Class Discord Server. You should check the relevant thread of the server, as well as the syllabus, before reaching out to Dr. Theobold.\nOf course, if your question is truly private, such as a grade inquiry or a personal concern, you may email me directly.\n\n\n\n\n\n\nNote\n\n\n\nIf you email me to ask a question that should be public, I will likely ask you to post your question on Discord instead. Please don’t take this personally! It just means that you asked a good question, and I think the rest of the class could benefit from seeing the answer.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#late-work",
    "href": "course-materials/syllabus.html#late-work",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Late Work",
    "text": "Late Work\nLate assignments will automatically be docked -10% per day, up to a maximum grade penalty of 50%. That is, as long as your work is turned in by the end of Week 10, you will still get half credit for it!\n\n\n\n\n\n\nNote\n\n\n\nNote that turning in an assignment late means you will not get any feedback, and using lecture or section time to work on late assignments will not be tolerated.\n\n\n\nDeadline Extensions\nIn case of emergency, you have two deadline “extensions” to use throughout the quarter on any Lab Assignment. This will grant you a 72-hour (3-day) extension.\nThe rules for these are as follows:\n\nYou must request the extension through the Google form linked on Canvas. Any other request (e.g., by email, Discord message, verbally, etc.) does not count unless the Google form is filled out.\nThe extension must be requested before the deadline has passed (i.e., before Friday at 11:59pm). I do not grant after-deadline extensions for any reason.\n\nProperly requested extensions are automatically granted; you will not get a confirmation email or message, you will simply see your late penalties disappear at some point.\n\n\n\n\n\n\nNote\n\n\n\nThese deadline extensions are automatic! You don’t have to tell why you need the extension - maybe you have a busy week with other work, maybe you are traveling with a sports team, maybe you partied too hard for your friend’s birthday. It doesn’t matter to me!\nThe flip side of this, though, is that if you use your deadline extensions early on in the course, and then run into a bigger issues later on, you’re out of luck.\n\n\n\n\nSpecial cases\nSometimes, issues arise require more time than the auto-extension gives. In general, if something comes up in your life, I always want to find a way to help. Please let me know what your situation is, and we’ll work together to find a good solution.\nThe most important thing is that you tell me early. As a rule, I do not grant extensions after the deadline.\n\n\n\n\n\n\nNote\n\n\n\nOf course, in the case of a major crisis, that is truly exceptional and unforeseen, all these rules go out the window. I want you to feel comfortable reaching out to me when you are facing something extra difficult. We’ll figure it out.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#academic-integrity",
    "href": "course-materials/syllabus.html#academic-integrity",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nYou are expected to abide by the Cal Poly Code of Conduct at all times.\n\nPlagiarism\nYou are encouraged to work with other classmates on all but the exam portions of this class. You are also encouraged (realistically, required!) to make use of online resources to accomplish tasks.\nWhen dealing with code, follow these guidelines:\n\nNever copy-paste more than small snippets of code. That is, you might borrow a little three-line function from StackOverflow, but you should not copy over a full analysis you find on Kaggle.\nAttribute all code that is not completely your own. If you do borrow that StackOverflow snippet, provide a link to the source. If you reference a similar analysis for ideas, mention that in your description.\n\n\n\n\n\n\n\nWarning\n\n\n\nA good “rule of thumb” is: If I sat you down by yourself in a room with no internet, could you explain to me roughly what each line of code is doing? If not, you are probably borrowing more than you should from your online source.\n(In fact, this is exactly what I will do if I need to investigate possible cheating.)\n\n\n\nAI tools\nNew AI models like Chat GPT offer a whole new world of online coding resources. This is exciting! You should absolutely feel free to get help from these tools, they are excellent at answering questions.\nHowever, from an academic integrity perspective, treat these AI generative chat resources like, say, a tutor. Asking a tutor to help explain a homework concept to you or help debug your code? Totally fine! Giving the tutor a homework question and having them answer the whole thing? Nope. Talking to a tutor at all, about anything, during the course of the exam? Unacceptable.\n\n\n\nIntegrity Violations\nIf you accidentally forget a small citation, or go a little overboard in how much you “borrow” from StackOverflow, you’ll get a warning and a grade deduction on that assignment.\nAny instance of willful and deliberate cheating will result in a failing grade on the assignment and I will file a academic integrity report with the Office of Student Rights and responsibilities.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about being on the giving end as well as the taking end. For example: If you send your finished assignment to a friend, and that friend copies it, you have both received a failing grade on the assignment and a report filed to OSRR.\n\n\n\n\nIntellectual property\nThe materials for this course are legally the professor’s intellectual property.\nMost class materials are publicly shared, and you are welcome to direct others to this resource at any time. You are also welcome to publicly share any or all of your work on the class project.\nNon-public class materials—most importantly, assignment solutions and any exam materials—may never be shared.\n\n\n\n\n\n\nWarning\n\n\n\nThis is not just an issue of academic honestly, it is quite literally a legal copyright scenario. Please do not distribute solutions or exam questions from this class anywhere, for any reason. Doing so is a violation of the Code of Conduct, and it may constitute a violation of U.S. copyright law.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#class-conduct",
    "href": "course-materials/syllabus.html#class-conduct",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Class Conduct",
    "text": "Class Conduct\nIn this classroom, I expect you to be polite, respectful, inclusive, and open-minded.\nSome examples of how to be a good classmate include:\n\nDoing your best to avoid language that is ableist, racist, sexist, transphobic, or classist; or that perpetuates harmful stereotypes.\nAddressing your classmates (and your professor!) by their preferred name and pronouns.\nDoing your best to be aware of your own biases, privileges, and areas of ignorance.\nListening to others’ opinions, and making an effort to understand their perspective.\nTaking the time to help your classmates grasp concepts or solve problems, even when you are ready to move on.\n\n\nAttendance\nIt is my general expectation that you will attend lecture, and remain present until you have finished the day’s in-class work. However, I do not take formal attendance in class; as long as you engage with the material and complete the small check-ins, you can decide which lectures are useful to you.\nPlease do not email me letting me know when you are missing class - I don’t need to know if you are attending, it is your responsibility to catch up on the materials and check-ins you miss.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#about-me",
    "href": "slides/00-welcome.html#about-me",
    "title": "Welcome, Intro, and Setup",
    "section": "About Me",
    "text": "About Me\n\n\n\n\n\n\nGrew up in Grand Junction, CO\nBA in Stats: 2014 from Colorado Mesa University\nPhD in Stats: 2020 from Montana State University\n2020 - now: Professor of Stats at Cal Poly\nMy research: Statistics and data science education, R programming\nThings I like: Spending time outside (e.g., running, hiking, biking), travelling, going to concerts and musicals",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#grade-brakedown",
    "href": "slides/00-welcome.html#grade-brakedown",
    "title": "Welcome, Intro, and Setup",
    "section": "Grade Brakedown",
    "text": "Grade Brakedown\n\n5%: Check-Ins – Questions interspersed throughout lecture\n10%: Lab Activities – Lab attendance is required\n25%: Weekly Assignments – due Saturdays at midnight\n15% each: Exams in Week 5 and Week 10.\n30%: Final project",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#important-dates",
    "href": "slides/00-welcome.html#important-dates",
    "title": "Welcome, Intro, and Setup",
    "section": "Important Dates",
    "text": "Important Dates\n\nThurs, February 6: Exam 1 (in-class)\nFri, February 21: 1-page Project Proposal Due\nThurs, March 13: Exam 2 (in-class)\nSaturday, March 16 (before Finals Week: Final Project Poster Presentations",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#homework-late-policy",
    "href": "slides/00-welcome.html#homework-late-policy",
    "title": "Welcome, Intro, and Setup",
    "section": "Homework Late Policy",
    "text": "Homework Late Policy\n\nTwo deadline extensions: Fill out the form on Canvas before the deadline, get an automatic 3-day (72-hour) deadline extension.\nIf you’ve used your deadline extensions, late work has a 10% deduction per day for up to 5 days.\n\nThis policy also applies to deadline extension requests that are not submitted before the assignment’s deadline.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#missing-lab",
    "href": "slides/00-welcome.html#missing-lab",
    "title": "Welcome, Intro, and Setup",
    "section": "Missing Lab",
    "text": "Missing Lab\n\nBest option: Go to another section.\nSecond-best option: Turn in your work by email to me.\nWorst option: Miss it entirely, get a 0.\nYou may miss lab up to two times during the quarter. If you need to miss / reschedule more than two of the labs this quarter, you will need to retake the class.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#discord-and-email",
    "href": "slides/00-welcome.html#discord-and-email",
    "title": "Welcome, Intro, and Setup",
    "section": "Discord and Email",
    "text": "Discord and Email\n\nClass questions go on DISCORD\nUse my email only for personal concerns that you want to talk about privately to me.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#colab-notebooks",
    "href": "slides/00-welcome.html#colab-notebooks",
    "title": "Welcome, Intro, and Setup",
    "section": "Colab Notebooks",
    "text": "Colab Notebooks\n\nIn Data Science, everyone uses Notebooks, not scripts, for coding.\nWe will be using Jupyter Notebooks (invented at Cal Poly!), hosted for free by Google Colab.\nIf you want to work offline, you can install Anaconda on your laptop.\nI recommend an IDE like PyCharm (or Positron) as well.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#academic-integrity",
    "href": "slides/00-welcome.html#academic-integrity",
    "title": "Welcome, Intro, and Setup",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nIf you copy text from a website into your essay, it’s cheating.\nIf you ask your friend to write your essay, it’s cheating.\nIf you pay someone else to write your essay, it’s cheating.\nIf you ask GenAI to do your work and then copy the answers, it’s cheating.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#conversation-not-copying",
    "href": "slides/00-welcome.html#conversation-not-copying",
    "title": "Welcome, Intro, and Setup",
    "section": "Conversation, not copying",
    "text": "Conversation, not copying\nIt is okay (in fact, encouraged) to …\n\nAsk GenAI for tips on how to get started on a coding problem\nAsk GenAI to help find bugs in your code\nAsk GenAI to help explain concepts or functions\nPretend ChatGPT is your human tutor or TA!",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#activity-good-and-bad-use-of-ai",
    "href": "slides/00-welcome.html#activity-good-and-bad-use-of-ai",
    "title": "Welcome, Intro, and Setup",
    "section": "Activity: Good and bad use of AI",
    "text": "Activity: Good and bad use of AI\n\nFind your group member\n\nIn the “People” tab on Canvas, find your “Day 1 Group”\n\nOnce you have found your group member, open the Activity 1.1 notebook (linked on Canvas).\n\n\n15-minutes:\n\nFirst, Person A follows the instructions in Part 1 of the notebook.\nThen, Person B follows the instructions in Part 2 of the notebook.\nFinally, discuss and answer the questions at the bottom.\n\n\n\n\n\n\n\n\n\nIf it is not your turn to type, you watch!\n\n\nDo not give input unless your partner asks for help.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science (in Python)",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the quarter. Note that this schedule will be updated as the quarter progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nRequired Reading\nLecture Slides\nLab Activity\nWeekly Assignment\nExams & Project\n\n\n\n\n0\nSunday, January 5\n\nWelcome to DATA 301!\n\n\n\n\n\n1 Summarizing Tabular Data\nTuesday, January 7\n\nTabular Data & Variable Summaries\nGenAI Activity (Link to Collab)\nActivity 1.1 (Link to Collab)\n\n\n\n\n\nThursday, January 9\n\nVisualizing and Comparing Categorical Variables\nActivity 1.2 (Link to Collab)\nLab 1A\nLab 1B\n\n\n\n\nSunday, January 12\n\n\n\nLab 1A, 1B Due by Midnight\n\n\n\n2 Summarizing & Visualizing Quantitative Data\nTuesday, January 14\n(Dr. T’s b-day)\n\nVisualizing and Summarizing Quantitative Variables\nLecture 2.1 Activity\nActivity 2.1 (Link to Collab)\n\n\n\n\n\nThursday, January 16\n\nMultivariate Summaries\nLecture 2.2 Activity\nActivity 2.2\nLab 2A\nLab 2B\n\n\n\n\nSunday, January 19\n\n\n\nLab 2A, 2B Due by Midnight\n\n\n\n3\nMeasuring Similarity with Distances\nTuesday, January 21\nNo Class (Classes Follow Monday Schedule)\n\n\n\n\n\n\n\nThursday, January 23\n\nDistances Between Observations\n\nLecture Activity 3.1\nActivity 3.1\nLab 3\n\n\n\n\nSunday, January 26\n\n\n\nLab 3 Due by Midnight\n\n\n\n4\nDummy Variables & TF-IDF\nTuesday, January 28\n\nDummy Variables and Column Transformers\nActivity 4.1\n\n\n\n\n\nThursday, January 30\n\nBag-of-Words and TF-IDF\nActivity 4.2\nLab 4A\nLab 4B\n\n\n\n\nSunday, February 2\n\n\n\nLab 4A, 4B Due by Midnight\n\n\n\n5\nK-Nearest Neighbors & Midterm Exam\nTuesday, February 4\n\nK-Nearest-Neighbors\nLecture 5.1 Activity\nActivity 5.1\n\n\n\n\n\nThursday, February 6\n\nSpicing up Your Visualizations\nActivity 5.2\n\nExam 1 (in-class)\n\n\n\nFriday, February 7\n\n\n\nNo lab this week!\n\n\n\n6\nClassification & Model Selection\nTuesday, February 11\n\nIntroduction to Modeling\nLecture 6.1 Activity\nActivity 6.1\n\n\n\n\n\nThursday, February 13\n\nCross-Validation and Grid Search\nLecture 6.2 Activity\nActivity 6.2\nLab 6\n\n\n\n\nSunday, February 16\n\n\n\nLab 6 Due by Midnight\n\n\n\n7\nLogistic Regression & Unsupervised Learning\nTuesday, February 18\n\nClassification\nLecture 7.1 Activity\nActivity 7.1\n\n\n\n\n\nThursday, February 20\n\nLogistic Regression\nLecture 7.2 Activity\nActivity 7.2\nLab 7\n\n\n\n\nSunday, February 23\n\n\n\nLab 7 Due by Midnight\nProject Proposal Due\n\n\n8\nJoining Data & Hierarchical Data\nTuesday, February 25\n\nUnsupervised Learning with K-Means\nLecture 8.1 Activity\nActivity 8.1\n\n\n\n\n\nThursday, February 27\n\nCombining Datasets\nActivity 8.2\n\n\n\n\n\nSunday, March 2\n\n\n\nLab 8 Due by Midnight\n\n\n\n9\nWebscraping\nTuesday, March 4\n\nHierarchical Data\nActivity 9.1\n\n\n\n\n\nThursday, March 6\n\nWebscraping\nActivity 9.2\n\n\n\n\n\nSunday, March 9\n\n\n\nLab 9 Due by Midnight\n\n\n\n10\nFinal Posters & Final Exam\nTuesday, March 11\n\n\n\n\n\n\n\n\nThursday, March 13\n\n\n\n\nExam 2 (in-class)\n\n\nFinals Week\nSaturday, March 15 (time TBD)\n\n\n\n\nPoster Presentations",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "project/project-description.html",
    "href": "project/project-description.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "Overview\nYour final project is your chance to demonstrate your knowledge from DATA 301 in a real-world setting. You will choose a dataset, or possibly multiple related datasets, and perform an analysis to address your group’s research question(s).\nYou will summarize your findings into a poster, and present it during the common hour final poster session (with both sections of DATA 301) on Saturday, March 15th from 1:10pm to 2:30pm. The poster session will be in our regularly scheduled classroom!\nProjects may be done in groups of up to 3 students. The guidelines are the same regardless; that is, if you choose to work alone, I will expect the same level of quality as if you choose to work in a group of 3.\n\n\n\n\n\n\nCaution\n\n\n\nNo extensions will be granted for projects, except in highly unusual, extenuating circumstances. Please start early!\n\n\n\n\nCheckpoint 1: Project Proposal\nTo make sure you are on track, you have a two-page project proposal due on Sunday, February 23.\nThis proposal must be two pages only, and it must contain:\n\nYour group member names.\nInformation about the dataset(s) you intend to analyze:\n\nWhere are the data located?\nWho collected the data and why?\nWhat information (variables) are in the dataset?\n\nResearch Questions: You should have one primary research question and a few secondary questions\nPreliminary exploration of your dataset(s): A few simple plots or summary statistics that relate to the variables you plan to study.\n\n\n\nFinding Data\nSome good places to find datasets are:\n\nKaggle\nTidy Tuesday\nGoogle Data Set Search\nReddit Datasets\nList of JSON APIs\nProject Gutenberg (good source of textual data)\nState or federal government websites, such as the U.S. Government’s Open Data\n\n\n\nPrinting Your Final Poster\nThis website has some poster templates that you can (but are not required to) use. Your poster will need to be printed on 24” x 36” paper. You may select all the most basic printing options (e.g., matte paper, no lamination, not mounted). I would recommend printing you poster at Cal Poly since basic posters cost about $18. You might need to check on their turnaround time, since they may be busy during Week 10.\nIf you need assistance with the cost of printing a poster, please let Dr. Theobold know.\n\n\nFinal Poster Grading\nYour posters will be graded on:\n\nPresentation:\n\nWas the poster clear and visually appealing?\nDo the visualizations tell a story?\nWere the presenters able to describe the project and answer questions?\n\nCorrectness:\n\nWere the correct summaries and analyses chosen for the data?\nWere the results correctly interpreted and described?\nWas the choice of analysis reasonable for the research question?\n\nComplexity:\n\nDid the project address multiple rich aspects of the dataset(s)?\nDid the project use skills from across the quarter to answer questions?\nDid the project incorporate something from Week 9 or 10? (Webscraping, APIs, data joining) or from beyond the course?\n\nData insight:\n\nWere the research questions interesting and insightful?\nDid the project make clear and relevant data conclusions?\nDoes the project tell a story with real-world impact?"
  },
  {
    "objectID": "slides/better-data-visualization.html",
    "href": "slides/better-data-visualization.html",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nchildcare_costs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-09/childcare_costs.csv')\n\nRows: 34567 Columns: 61\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (61): county_fips_code, study_year, unr_16, funr_16, munr_16, unr_20to64...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounties &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-09/counties.csv')\n\nRows: 3144 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): county_name, state_name, state_abbreviation\ndbl (1): county_fips_code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nca_childcare &lt;- full_join(counties,\n                 childcare_costs,\n                 ) |&gt; \n  filter(state_abbreviation == \"CA\",\n         county_name %in% c(\"San Luis Obispo County\", \n                            \"Orange County\", \n                            \"San Francisco County\")\n         ) |&gt; \n  pivot_longer(cols = starts_with(\"mfcc_\"),\n               names_to = \"development_stage\",\n               values_to = \"median_weekly_childcare_cost\") |&gt;\n  filter(development_stage %in% c(\"mfcc_infant\", \"mfcc_toddler\")) |&gt; \n  mutate(across(.cols = c(county_name, state_abbreviation, development_stage), \n                .fns = ~ as.factor(.x)\n                ),\n         development_stage = as.factor(str_remove(development_stage, pattern = \"mfcc_\"))\n         ) |&gt; \n  select(county_name, study_year, development_stage, median_weekly_childcare_cost)\n\nJoining with `by = join_by(county_fips_code)`\n\n#write_csv(ca_childcare, file = \"ca_childcare_clean.csv\")\n\n\n\n\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             fill = county_name)\n         ) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +\n  facet_wrap(~ development_stage) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Weekly Median Childcare Cost ($)\",\n       fill = \"County\")\n\n\n\n\n\n\n\n\n\n\n\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ county_name) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Weekly Median Childcare Cost ($)\",\n       color = \"Development \\nStage\",\n       shape = \"Development \\nStage\")\n\n\n\n\n\n\n\n\n\n\n\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Weekly Median Childcare Cost ($)\",\n       color = \"Development \\nStage\",\n       shape = \"Development \\nStage\")\n\n\n\n\n\n\n\n\n\n\n\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"\",\n       title = \"Weekly Median Cost of Childcare by CA County\",\n       color = \"Development \\nStage\",\n       shape = \"Development \\nStage\")\n\n\n\n\n\n\n\n\n\n\n\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  labs(x = \"Year\",\n       y = \"\",\n       title = \"Weekly Median Cost of Childcare by CA County\",\n       color = \"by Development Stage\",\n       shape = \"by Development Stage\")\n\n\n\n\n\n\n\n\n\n\n\n\nlegend_text &lt;- ca_childcare %&gt;% \n  filter(county_name == \"San Francisco County\", \n         study_year %in% c(2012, 2014)\n         ) %&gt;% \n  group_by(development_stage) %&gt;% \n  mutate(plot_year = if_else(development_stage == \"infant\",\n                             2012, \n                             2014), \n         plot_median = if_else(development_stage == \"infant\", \n                               median_weekly_childcare_cost + 50, \n                               median_weekly_childcare_cost - 15)\n         ) %&gt;% \n  distinct(plot_year, development_stage, .keep_all = TRUE)\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\",\n       y = \"\",\n       title = \"Weekly Median Cost of Childcare by CA County\",\n       subtitle = \"by Development Stage\",\n       shape = \"\", \n       color = \"\") +\n  geom_text(data = legend_text, \n            mapping = aes(x = plot_year, \n                          y = plot_median, \n                          color = development_stage, \n                          label = development_stage)\n            )\n\n\n\n\n\n\n\n# Makes the same annotation for each facet!\n  # annotate(geom = \"text\", \n  #          x = 2012, \n  #          y = 250, \n  #          color = \"steelblue\", \n  #          label = \"infant\"\n  #          )"
  },
  {
    "objectID": "slides/better-data-visualization.html#join-clean-data",
    "href": "slides/better-data-visualization.html#join-clean-data",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "ca_childcare &lt;- full_join(counties,\n                 childcare_costs,\n                 ) |&gt; \n  filter(state_abbreviation == \"CA\",\n         county_name %in% c(\"San Luis Obispo County\", \n                            \"Orange County\", \n                            \"San Francisco County\")\n         ) |&gt; \n  pivot_longer(cols = starts_with(\"mfcc_\"),\n               names_to = \"development_stage\",\n               values_to = \"median_weekly_childcare_cost\") |&gt;\n  filter(development_stage %in% c(\"mfcc_infant\", \"mfcc_toddler\")) |&gt; \n  mutate(across(.cols = c(county_name, state_abbreviation, development_stage), \n                .fns = ~ as.factor(.x)\n                ),\n         development_stage = as.factor(str_remove(development_stage, pattern = \"mfcc_\"))\n         ) |&gt; \n  select(county_name, study_year, development_stage, median_weekly_childcare_cost)\n\nJoining with `by = join_by(county_fips_code)`\n\n#write_csv(ca_childcare, file = \"ca_childcare_clean.csv\")"
  },
  {
    "objectID": "slides/better-data-visualization.html#childcare-costs-over-time-bar-plot",
    "href": "slides/better-data-visualization.html#childcare-costs-over-time-bar-plot",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "ca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             fill = county_name)\n         ) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +\n  facet_wrap(~ development_stage) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Weekly Median Childcare Cost ($)\",\n       fill = \"County\")"
  },
  {
    "objectID": "slides/better-data-visualization.html#childcare-costs-over-time-line-graph",
    "href": "slides/better-data-visualization.html#childcare-costs-over-time-line-graph",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "ca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ county_name) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Weekly Median Childcare Cost ($)\",\n       color = \"Development \\nStage\",\n       shape = \"Development \\nStage\")"
  },
  {
    "objectID": "slides/better-data-visualization.html#childcare-costs-over-time-line-graph-one-column",
    "href": "slides/better-data-visualization.html#childcare-costs-over-time-line-graph-one-column",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "ca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Weekly Median Childcare Cost ($)\",\n       color = \"Development \\nStage\",\n       shape = \"Development \\nStage\")"
  },
  {
    "objectID": "slides/better-data-visualization.html#removing-y-axis-label",
    "href": "slides/better-data-visualization.html#removing-y-axis-label",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "ca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"\",\n       title = \"Weekly Median Cost of Childcare by CA County\",\n       color = \"Development \\nStage\",\n       shape = \"Development \\nStage\")"
  },
  {
    "objectID": "slides/better-data-visualization.html#moving-legend",
    "href": "slides/better-data-visualization.html#moving-legend",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "ca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  labs(x = \"Year\",\n       y = \"\",\n       title = \"Weekly Median Cost of Childcare by CA County\",\n       color = \"by Development Stage\",\n       shape = \"by Development Stage\")"
  },
  {
    "objectID": "slides/better-data-visualization.html#removing-legend",
    "href": "slides/better-data-visualization.html#removing-legend",
    "title": "Making your Data Visualizations More Effective",
    "section": "",
    "text": "legend_text &lt;- ca_childcare %&gt;% \n  filter(county_name == \"San Francisco County\", \n         study_year %in% c(2012, 2014)\n         ) %&gt;% \n  group_by(development_stage) %&gt;% \n  mutate(plot_year = if_else(development_stage == \"infant\",\n                             2012, \n                             2014), \n         plot_median = if_else(development_stage == \"infant\", \n                               median_weekly_childcare_cost + 50, \n                               median_weekly_childcare_cost - 15)\n         ) %&gt;% \n  distinct(plot_year, development_stage, .keep_all = TRUE)\n\nca_childcare |&gt; \n  ggplot(aes(x = study_year,\n             y = median_weekly_childcare_cost,\n             color = development_stage,\n             shape = development_stage)\n  ) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  facet_wrap(~ county_name, ncol = 1) +\n  scale_x_continuous(breaks = seq(2008, 2018, 2)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange3\")) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\",\n       y = \"\",\n       title = \"Weekly Median Cost of Childcare by CA County\",\n       subtitle = \"by Development Stage\",\n       shape = \"\", \n       color = \"\") +\n  geom_text(data = legend_text, \n            mapping = aes(x = plot_year, \n                          y = plot_median, \n                          color = development_stage, \n                          label = development_stage)\n            )\n\n\n\n\n\n\n\n# Makes the same annotation for each facet!\n  # annotate(geom = \"text\", \n  #          x = 2012, \n  #          y = 250, \n  #          color = \"steelblue\", \n  #          label = \"infant\"\n  #          )"
  },
  {
    "objectID": "slides/14-merge.html#the-story-so-far",
    "href": "slides/14-merge.html#the-story-so-far",
    "title": "Combining Datasets",
    "section": "The story so far",
    "text": "The story so far\nData analysis: the whole game\n\nAcquire data and clean it by fixing variable types, dropping or replacing missing data, and looking for other issues.\nExplore the dataset by making summaries and plots of one variable.\nEstablish research questions to answer with this data.\nCreate visualizations of two or more variables that address simple questions.\nFit predictive models to address more complex questions, and/or to prepare for prediction on future data.\nFit unsupervised models to answer open-ended questions.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joining-on-a-key",
    "href": "slides/14-merge.html#joining-on-a-key",
    "title": "Combining Datasets",
    "section": "Joining on a Key",
    "text": "Joining on a Key\nExample: Planes and flights\n\nSometimes, information is spread across multiple data sets.\nFor example, suppose we want to know which manufacturer’s planes made the most flights in November 2013.\nOne data set contains information about flights in Nov. 2013…\n\n\nimport pandas as pd\ndata_dir = \"https://datasci112.stanford.edu/data/nycflights13/\"\ndf_flights = pd.read_csv(f\"{data_dir}/flights11.csv\")\ndf_flights.head()\n\n   year  month  day  dep_time  ...  distance  hour  minute  tailnum\n0  2013     11    1    2108.0  ...      1167    20      56   N10156\n1  2013     11    1    1154.0  ...       541    12       0   N102UW\n2  2013     11    1     854.0  ...       946     8      29   N10575\n3  2013     11    1    1643.0  ...       594    15       5   N10575\n4  2013     11    1     603.0  ...       282     6       0   N11109\n\n[5 rows x 18 columns]\n\n\nExample: Planes and Flights\n…while another contains information about planes.\n\ndf_planes = pd.read_csv(f\"{data_dir}/planes.csv\")\ndf_planes.head()\n\n  tailnum    year                     type  ... seats speed     engine\n0  N10156  2004.0  Fixed wing multi engine  ...    55   NaN  Turbo-fan\n1  N102UW  1998.0  Fixed wing multi engine  ...   182   NaN  Turbo-fan\n2  N103US  1999.0  Fixed wing multi engine  ...   182   NaN  Turbo-fan\n3  N104UW  1999.0  Fixed wing multi engine  ...   182   NaN  Turbo-fan\n4  N10575  2002.0  Fixed wing multi engine  ...    55   NaN  Turbo-fan\n\n[5 rows x 9 columns]\n\n\nIn order to answer the question of which manufacturer made the most flights, we have to join these two data sets together.\nKeys\n\nA primary key is a column (or a set of columns) that uniquely identifies observations in a data frame.\nThe primary key is the column(s) you would think of as the index.\nA foreign key is a column (or a set of columns) that points to the primary key of another data frame.\nPlanes are uniquely identified by their tail number (tailnum).\n\nJoining on a Key\nEach value of the primary key should only appear once, but it could appear many times in a foreign key.\n\ndf_flights['tailnum'].value_counts().head()\n\ntailnum\nN353JB    48\nN955UW    47\nN184JB    47\nN375JB    42\nN281JB    42\nName: count, dtype: int64\n\ndf_planes['tailnum'].value_counts().head()\n\ntailnum\nN10156    1\nN709EV    1\nN706JB    1\nN706SW    1\nN706TW    1\nName: count, dtype: int64\n\n\nJoining on a Key\nThe Pandas function .merge() can be used to join two DataFrames on a key.\n\ndf_joined = df_flights.merge(df_planes, on=\"tailnum\")\ndf_joined.head()   \n\n   year_x  month  day  dep_time  ...  engines  seats  speed     engine\n0    2013     11    1    2108.0  ...        2     55    NaN  Turbo-fan\n1    2013     11    1    1154.0  ...        2    182    NaN  Turbo-fan\n2    2013     11    1     854.0  ...        2     55    NaN  Turbo-fan\n3    2013     11    1    1643.0  ...        2     55    NaN  Turbo-fan\n4    2013     11    1     603.0  ...        2     55    NaN  Turbo-fan\n\n[5 rows x 26 columns]\n\n\nOverlapping Column Names\n\nJoining two data frames results in a wider data frame, with more columns.\nBy default, Pandas adds the suffixes _x and _y to overlapping column names, but this can be customized.\n\n\ndf_joined = df_flights.merge(df_planes, on=\"tailnum\",\n                             suffixes=(\"_flight\", \"_plane\"))\ndf_joined.columns\n\nIndex(['year_flight', 'month', 'day', 'dep_time', 'sched_dep_time',\n       'dep_delay', 'arr_time', 'sched_arr_time', 'arr_delay', 'carrier',\n       'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute',\n       'tailnum', 'year_plane', 'type', 'manufacturer', 'model', 'engines',\n       'seats', 'speed', 'engine'],\n      dtype='object')\n\n\nAnalyzing the Joined Data\nNow that we have joined the two data sets, we can answer the question: which manufacturer’s planes made the most flights?\n\ndf_joined[\"manufacturer\"].value_counts()\n\nmanufacturer\nBOEING                           6557\nEMBRAER                          5175\nAIRBUS                           3954\nAIRBUS INDUSTRIE                 3456\nBOMBARDIER INC                   2632\nMCDONNELL DOUGLAS AIRCRAFT CO     811\nMCDONNELL DOUGLAS                 330\nCANADAIR                          122\nMCDONNELL DOUGLAS CORPORATION     121\nCESSNA                             36\nCIRRUS DESIGN CORP                 21\nROBINSON HELICOPTER CO             19\nBARKER JACK L                      15\nPIPER                               9\nFRIEDEMANN JON                      8\nSIKORSKY                            6\nBELL                                5\nDEHAVILLAND                         4\nAGUSTA SPA                          3\nAMERICAN AIRCRAFT INC               3\nDOUGLAS                             2\nLAMBERT RICHARD                     2\nKILDALL GARY                        2\nLEARJET INC                         2\nMARZ BARRY                          1\nAVIAT AIRCRAFT INC                  1\nPAIR MIKE E                         1\nLEBLANC GLENN T                     1\nSTEWART MACO                        1\nName: count, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joining-on-multiple-keys",
    "href": "slides/14-merge.html#joining-on-multiple-keys",
    "title": "Combining Datasets",
    "section": "Joining on Multiple Keys",
    "text": "Joining on Multiple Keys\nExample: Weather and Flights\n\nResearch question: What weather factors cause flight delays?\nHere is a data set containing hourly weather data at each airport in 2013.\n\n\ndf_weather = pd.read_csv(f\"{data_dir}/weather.csv\")\ndf_weather.head()\n\n  airport  year  month  day  ...  wind_gust  precip  pressure  visib\n0     EWR  2013      1    1  ...        NaN     0.0    1012.0   10.0\n1     EWR  2013      1    1  ...        NaN     0.0    1012.3   10.0\n2     EWR  2013      1    1  ...        NaN     0.0    1012.5   10.0\n3     EWR  2013      1    1  ...        NaN     0.0    1012.2   10.0\n4     EWR  2013      1    1  ...        NaN     0.0    1011.9   10.0\n\n[5 rows x 14 columns]\n\n\nWhat is the primary key of this data set?\nA Key with Multiple Columns\nLet’s start by looking at flights out of JFK only, for simplicity.\nWe need to join to the weather data on year, month, day, and hour.\n\ndf_flights_jfk = df_flights[df_flights[\"origin\"] == \"JFK\"]\ndf_weather_jfk = df_weather[df_weather[\"airport\"] == \"JFK\"]\n\ndf_jfk = df_flights_jfk.merge(df_weather_jfk, on=(\"year\",\"month\",\"day\",\"hour\"))\n\ndf_jfk.head()\n\n   year  month  day  dep_time  ...  wind_gust  precip  pressure  visib\n0  2013     11    1    1154.0  ...        NaN    0.00    1002.8   10.0\n1  2013     11    1    2055.0  ...        NaN    0.00    1005.0   10.0\n2  2013     11    1    1814.0  ...        NaN    0.00    1003.9   10.0\n3  2013     11    1    1014.0  ...        NaN    0.07       NaN   10.0\n4  2013     11    1    1852.0  ...        NaN    0.00    1003.9   10.0\n\n[5 rows x 28 columns]\n\n\nLet’s see how rain affects departure delays.\n\n\nCode\nfrom plotnine import *\n\n(ggplot(df_jfk, aes(x=\"precip\", y=\"dep_delay\")) +\ngeom_point(alpha=0.2) +\ntheme_classic())\n\n\n&lt;plotnine.ggplot.ggplot object at 0x10c813d90&gt;\n\n\nJoining on Keys with Different Names\n\nSometimes, the join keys have different names in the two data sets.\nThis frequently happens if the data sets come from different sources.\nFor example, if we want to join the (entire) flights data to the weather data, we would need to include the airport in the key.\nBut the airport is called origin in df_flights and airport in df_weather.\nThe .merge() function provides left_on= and right_on= arguments for specifying different column names in the left (first) and right (second) data frames.\n\nJoining on Keys with Different Names\n\ndf_flights_weather = df_flights.merge(\n    df_weather,\n    left_on=(\"origin\", \"year\", \"month\", \"day\", \"hour\"),\n    right_on=(\"airport\", \"year\", \"month\", \"day\", \"hour\"))\n\nRain and delays by airport\nNow we can visualize how rain impacts delays at each airport:\n\n\nCode\ndf_rain = df_flights_weather.groupby([\"airport\", \"precip\"])[\"dep_delay\"].mean().to_frame().reset_index()\n\n(ggplot(df_rain, aes(x='precip', y='dep_delay', color = 'airport')) +\ngeom_point() +\ngeom_line() +\ntheme_classic())\n\n\n&lt;plotnine.ggplot.ggplot object at 0x10c834c00&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joins-with-missing-keys",
    "href": "slides/14-merge.html#joins-with-missing-keys",
    "title": "Combining Datasets",
    "section": "Joins with missing keys",
    "text": "Joins with missing keys\nExample: Baby names\nThe data below contains counts of names for babies born in 1920 and 2020:\n\nimport pandas as pd\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\n\ndf_1920 = pd.read_csv(data_dir + \"yob1920.txt\", header=None,\n                      names=[\"Name\", \"Sex\", \"Count\"])\ndf_2020 = pd.read_csv(data_dir + \"yob2020.txt\", header=None,\n                      names=[\"Name\", \"Sex\", \"Count\"])\n                      \ndf_1920.head()\n\n       Name Sex  Count\n0      Mary   F  70975\n1   Dorothy   F  36645\n2     Helen   F  35098\n3  Margaret   F  27997\n4      Ruth   F  26100\n\ndf_2020.head()\n\n        Name Sex  Count\n0     Olivia   F  17641\n1       Emma   F  15656\n2        Ava   F  13160\n3  Charlotte   F  13065\n4     Sophia   F  13036\n\n\nJoins\nWe can merge these two datasets on a primary key…\n\ndf_joined = df_1920.merge(df_2020, on=[\"Name\", \"Sex\"], suffixes=(\"_1920\", \"_2020\"))\ndf_joined.head()\n\n       Name Sex  Count_1920  Count_2020\n0      Mary   F       70975        2210\n1   Dorothy   F       36645         562\n2     Helen   F       35098         721\n3  Margaret   F       27997        2190\n4      Ruth   F       26100        1323\n\n\nMissing Keys?\n… but what happened to some of the names?\n\ndf_joined[df_joined[\"Name\"] == \"Maya\"]\n\nEmpty DataFrame\nColumns: [Name, Sex, Count_1920, Count_2020]\nIndex: []\n\n\nMissing keys\n\nWhy isn’t Maya in the joined data?\n\nIt is there in the 2020 data…\n\ndf_2020[df_2020[\"Name\"] == \"Maya\"]\n\n       Name Sex  Count\n60     Maya   F   3724\n28914  Maya   M      6\n\n\n…but not in the 1920 data.\n\ndf_1920[df_1920[\"Name\"] == \"Maya\"]\n\nEmpty DataFrame\nColumns: [Name, Sex, Count]\nIndex: []\n\n\nMissing keys\n\nHow does the merge() function determine which keys get kept?\nBy default, in order to appear in the joined data, a key must be present in both tables.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#other-types-of-joins",
    "href": "slides/14-merge.html#other-types-of-joins",
    "title": "Combining Datasets",
    "section": "Other Types of Joins",
    "text": "Other Types of Joins\nTypes of Joins\n\nHow can we customize the behavior of joins for missing keys?\nBy default, Pandas does an inner join, which only keeps keys that are present in both tables.\nAn outer join keeps any key that is present in either table.\nA left join keeps all keys in the left table, even if they are not in the right table. But any keys that are only in the right table are dropped.\nA right join keeps all keys in the right table, even if they are not in the left table. But any keys that are only in the left table are dropped.\n\nTypes of joins\nWe can customize the type of join using the how= parameter of .merge(). By default, how=\"inner\".\n\ndf_joined_outer = df_1920.merge(df_2020, on=[\"Name\", \"Sex\"],\n                                how=\"outer\")\ndf_joined_outer[df_joined_outer[\"Name\"] == \"Maya\"]\n\n       Name Sex  Count_x  Count_y\n24999  Maya   F      NaN   3724.0\n25000  Maya   M      NaN      6.0\n\n\nTypes of Joins\n\nNote the missing values for other columns, like Count, for 1920!\nWhat other type of join would have produced this output in the Maya row?\n\nTypes of Joins\n\nNote the missing values for other columns, like Count, for 1920!\nWhat other type of join would have produced this output in the Maya row?\n\n\ndf_joined_right = df_1920.merge(df_2020, on=[\"Name\", \"Sex\"],\n                                how=\"right\")\ndf_joined_right[df_joined_right[\"Name\"] == \"Maya\"]\n\n       Name Sex  Count_x  Count_y\n60     Maya   F      NaN     3724\n28914  Maya   M      NaN        6",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#types-of-joins-4",
    "href": "slides/14-merge.html#types-of-joins-4",
    "title": "Combining Datasets",
    "section": "Types of Joins",
    "text": "Types of Joins\n\n\nNote the missing values for other columns, like Count_1920!\nWhat other type of join would have produced this output in the Maya row?\n\n\ndf_joined_right = (\n  df_1920\n  .merge(df_2020, \n         on = [\"Name\", \"Sex\"],\n         suffixes = (\"_1920\", \"_2020\"), \n         how = \"right\")\n    )\n\n\n\ndf_joined_right[df_joined_right[\"Name\"] == \"Maya\"]\n\n       Name Sex  Count_1920  Count_2020\n60     Maya   F         NaN        3724\n28914  Maya   M         NaN           6",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#many-to-many-joins",
    "href": "slides/14-merge.html#many-to-many-joins",
    "title": "Combining Datasets",
    "section": "Many-to-Many Joins",
    "text": "Many-to-Many Joins\nMany-to-Many Relationships\n\nSo far, the keys we’ve joined on have been the primary key of (at least) one table.\nIf we join to the primary key of another table, then the relationship is one-to-one (since primary keys uniquely identify rows).\nIf we join to the foreign key of another table, then the relationship is one-to-many.\nWhat if we join on a key that is not a primary key?\nThat is, what if the key does not uniquely identify rows in either table so that each value of the key might appear multiple times?\nThis relaionship is called many-to-many.\n\nMany-to-Many Example\nWhat if we only joined on the name?\n\ndf_1920.merge(df_2020, on=\"Name\")\n\n          Name Sex_x  Count_x Sex_y  Count_y\n0         Mary     F    70975     F     2210\n1         Mary     F    70975     M        5\n2      Dorothy     F    36645     F      562\n3        Helen     F    35098     F      721\n4     Margaret     F    27997     F     2190\n...        ...   ...      ...   ...      ...\n6158    Xavier     M        5     M     3876\n6159      York     M        5     F        6\n6160      York     M        5     M       14\n6161      Zeke     M        5     M      382\n6162      Zera     M        5     F       11\n\n[6163 rows x 5 columns]\n\n\nPreventing Bugs\n\nMost of the time, many-to-many joins are a bug, caused by a misunderstanding about the primary key.\nPandas allows us to specify the relationship we are expecting. It will fail with an error if the relationship is a different kind.\nFor example, suppose we thought that “name” was the primary key of the baby name tables.\n\n\ndf_1920.merge(df_2020, on=\"Name\",\n              validate=\"one_to_one\")\n\nMergeError: Merge keys are not unique in either left or right dataset; not a one-to-one merge\nErrors are (sometimes) your friend. They can prevent you from making even bigger mistakes!",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#filtering-joins",
    "href": "slides/14-merge.html#filtering-joins",
    "title": "Combining Datasets",
    "section": "Filtering Joins",
    "text": "Filtering Joins\nFiltering Joins\n\nInner, outer, left, and right are known as mutating joins, because they create new combined datasets.\nThere are two other types of joins that we use for filtering to get rid of some rows:\nA semi-join tells us which keys in the left are present in the right.\nAn anti-join tells us which keys in the left are not present in the right.\nIn Pandas, we can’t do these using .merge()\n\nFiltering joins\nWhich names existed in 1920 but don’t in 2020?\n\nin_both = df_1920['Name'].isin(df_2020['Name'])\ndf_1920.loc[~in_both, 'Name']\n\n67          Myrtle\n245            Sue\n257         Nannie\n284         Virgie\n300      Bernadine\n           ...    \n10748         Ynes\n10750     Zaragoza\n10751        Zearl\n10752     Zeferino\n10755      Zygmont\nName: Name, Length: 5638, dtype: object\n\n\nYour turn\n\nDid your name exist in 2020 but not 1920?\nIf so, how has the popularity changed?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#takeaways",
    "href": "slides/14-merge.html#takeaways",
    "title": "Combining Datasets",
    "section": "Takeaways",
    "text": "Takeaways\nTakeaways\n\nA primary key is one or more columns that uniquely indentify the rows.\nWe can join (a.k.a. merge) datasets if they share a primary key, or if one has a foreign key.\nThe default of .merge() is an inner join: only keys in both datasets are kept.\nWe can instead specify a left join, right join, or outer join; think about which rows we want to keep.\nFilterting joins like anti-join and semi-join can help you answer questions about the data.\nUse .isin() to see which keys in one dataset exist in the other.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#data-are-stored-in-plain-text-files",
    "href": "slides/01-tabular-data-summaries.html#data-are-stored-in-plain-text-files",
    "title": "Tabular Data and Variable Summaries",
    "section": "Data are stored in plain text files",
    "text": "Data are stored in plain text files\n\nname,pclass,survived,sex,age,sibsp,parch,ticket,fare,cabin,embarked,boat,body,home.dest\n\"Allen, Miss. Elisabeth Walton\",1,1,female,29,0,0,24160,211.3375,B5,S,2,,\"St Louis, MO\"\n\"Allison, Master. Hudson Trevor\",1,1,male,0.9167,1,2,113781,151.5500,C22 C26,S,11,,\"Montreal, PQ / Chesterville, ON\"\n\"Allison, Miss. Helen Loraine\",1,0,female,2,1,2,113781,151.5500,C22 C26,S,,,\"Montreal, PQ / Chesterville, ON\"\n\"Allison, Mr. Hudson Joshua Creighton\",1,0,male,30,1,2,113781,151.5500,C22 C26,S,,135,\"Montreal, PQ / Chesterville, ON\"\n\"Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\",1,0,female,25,1,2,113781,151.5500,C22 C26,S,,,\"Montreal, PQ / Chesterville, ON\"\n\"Anderson, Mr. Harry\",1,1,male,48,0,0,19952,26.5500,E12,S,3,,\"New York, NY\"\n\"Andrews, Miss. Kornelia Theodosia\",1,1,female,63,1,0,13502,77.9583,D7,S,10,,\"Hudson, NY\"\n\"Andrews, Mr. Thomas Jr\",1,0,male,39,0,0,112050,0.0000,A36,S,,,\"Belfast, NI\"\n\"Appleton, Mrs. Edward Dale (Charlotte Lamson)\",1,1,female,53,2,0,11769,51.4792,C101,S,D,,\"Bayside, Queens, NY\"\n\"Artagaveytia, Mr. Ramon\",1,0,male,71,0,0,PC 17609,49.5042,,C,,22,\"Montevideo, Uruguay\"\n\"Astor, Col. John Jacob\",1,0,male,47,1,0,PC 17757,227.5250,C62 C64,C,,124,\"New York, NY\"\n\n\n\nThis is called a csv (comma-separated) file.\nYou might see it stored as something.csv or something.txt\n.txt files might have different delimiters (separators)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#reading-data",
    "href": "slides/01-tabular-data-summaries.html#reading-data",
    "title": "Tabular Data and Variable Summaries",
    "section": "Reading data",
    "text": "Reading data\nWe read the data into a program like Python by specifying:\n\nwhat type of file it is (e.g., .csv, .txt, .xlsx)\nwhere the csv file is located (the “path”)\nif the file has a header\n… and other information in special cases!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#example-using-pandas-data-frame",
    "href": "slides/01-tabular-data-summaries.html#example-using-pandas-data-frame",
    "title": "Tabular Data and Variable Summaries",
    "section": "Example using pandas data frame:",
    "text": "Example using pandas data frame:\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\n\n\n\n\n\n\nread_csv() lives in pandas\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n                                              name  ...                        home.dest\n0                    Allen, Miss. Elisabeth Walton  ...                     St Louis, MO\n1                   Allison, Master. Hudson Trevor  ...  Montreal, PQ / Chesterville, ON\n2                     Allison, Miss. Helen Loraine  ...  Montreal, PQ / Chesterville, ON\n3             Allison, Mr. Hudson Joshua Creighton  ...  Montreal, PQ / Chesterville, ON\n4  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  ...  Montreal, PQ / Chesterville, ON\n\n[5 rows x 14 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in",
    "href": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in",
    "title": "Tabular Data and Variable Summaries",
    "section": "Lecture 1.1 Check in",
    "text": "Lecture 1.1 Check in\n\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\n\n\n\n\nQuestion 1: What if this file lived on a computer instead of online?\nQuestion 2: Why didn’t we have to specify that this dataset has a header?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#looking-at-the-rows",
    "href": "slides/01-tabular-data-summaries.html#looking-at-the-rows",
    "title": "Tabular Data and Variable Summaries",
    "section": "Looking at the rows",
    "text": "Looking at the rows\n\n\n\n\ndf.loc[1, :]\n\nname          Allison, Master. Hudson Trevor\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: 1, dtype: object\n\n\n\n\n\n\n\n\ndf.iloc[1, :]\n\nname          Allison, Master. Hudson Trevor\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: 1, dtype: object\n\n\n\n\n\n\nWhat is the difference between .loc and .iloc?\nWhat type of object is returned?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#loc-iloc-and-index",
    "href": "slides/01-tabular-data-summaries.html#loc-iloc-and-index",
    "title": "Tabular Data and Variable Summaries",
    "section": "loc, iloc, and index",
    "text": "loc, iloc, and index\n\ndf2 = df.set_index('name')\n\n\n\n\n                                                 pclass  ...                        home.dest\nname                                                     ...                                 \nAllen, Miss. Elisabeth Walton                         1  ...                     St Louis, MO\nAllison, Master. Hudson Trevor                        1  ...  Montreal, PQ / Chesterville, ON\nAllison, Miss. Helen Loraine                          1  ...  Montreal, PQ / Chesterville, ON\nAllison, Mr. Hudson Joshua Creighton                  1  ...  Montreal, PQ / Chesterville, ON\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)       1  ...  Montreal, PQ / Chesterville, ON\n\n[5 rows x 13 columns]\n\n\n\n\n\n\n\n\n\n\nWhy are there 13 columns now? (There were 14 before!)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-1",
    "href": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-1",
    "title": "Tabular Data and Variable Summaries",
    "section": "loc, iloc, and index",
    "text": "loc, iloc, and index\n\n\n\ndf2.loc[1, :]\n\nKeyError: 1\n\n\n\n\n\n\ndf2.iloc[1, :]\n\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: Allison, Master. Hudson Trevor, dtype: object\n\n\n\n\n\n\nWhy is .loc returning an error?\n\n\n\n\n\nWhy is .iloc not returning an error?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-2",
    "href": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-2",
    "title": "Tabular Data and Variable Summaries",
    "section": "loc, iloc, and index",
    "text": "loc, iloc, and index\n\n\n.loc – label-based location\n\n\nUses labels from rows (rownames) to select data\n\n\n\ndf2.loc[\"Allison, Master. Hudson Trevor\", :]\n\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: Allison, Master. Hudson Trevor, dtype: object\n\n\n\n\n\n\n\n\n.iloc – integer location\n\n\nUses indices (positions) from rows to select data\n\n\n\ndf2.iloc[1, :]\n\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: Allison, Master. Hudson Trevor, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#looking-at-columns",
    "href": "slides/01-tabular-data-summaries.html#looking-at-columns",
    "title": "Tabular Data and Variable Summaries",
    "section": "Looking at columns",
    "text": "Looking at columns\n\n\n\ndf.columns\n\nIndex(['name', 'pclass', 'survived', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n      dtype='object')\n\n\n\n\n\n\n\ndf['home.dest']\n\n0                          St Louis, MO\n1       Montreal, PQ / Chesterville, ON\n2       Montreal, PQ / Chesterville, ON\n3       Montreal, PQ / Chesterville, ON\n4       Montreal, PQ / Chesterville, ON\n                     ...               \n1304                                NaN\n1305                                NaN\n1306                                NaN\n1307                                NaN\n1308                                NaN\nName: home.dest, Length: 1309, dtype: object\n\n\n\n\n\n\n\n\n\n\n\nNaN (Not a Number) represents missing or null data",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#caution-object-types",
    "href": "slides/01-tabular-data-summaries.html#caution-object-types",
    "title": "Tabular Data and Variable Summaries",
    "section": "Caution: Object types",
    "text": "Caution: Object types\n\ntype(df)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\ntype(df.iloc[1, :])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\ntype(df['name'])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\n\nA Series is a one-dimensional labeled array (a vector with labels)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#questions-to-ask",
    "href": "slides/01-tabular-data-summaries.html#questions-to-ask",
    "title": "Tabular Data and Variable Summaries",
    "section": "Questions to ask",
    "text": "Questions to ask\n\nWhich variables (columns) are categorical?\nWhich variables are quantitative?\nWhich variables are labels (e.g. names or ID numbers)?\nWhich variables are text?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#a-quick-look-at-the-data",
    "href": "slides/01-tabular-data-summaries.html#a-quick-look-at-the-data",
    "title": "Tabular Data and Variable Summaries",
    "section": "A quick look at the data",
    "text": "A quick look at the data\n\n\ndf.describe()\n\n            pclass     survived  ...         fare        body\ncount  1309.000000  1309.000000  ...  1308.000000  121.000000\nmean      2.294882     0.381971  ...    33.295479  160.809917\nstd       0.837836     0.486055  ...    51.758668   97.696922\nmin       1.000000     0.000000  ...     0.000000    1.000000\n25%       2.000000     0.000000  ...     7.895800   72.000000\n50%       3.000000     0.000000  ...    14.454200  155.000000\n75%       3.000000     1.000000  ...    31.275000  256.000000\nmax       3.000000     1.000000  ...   512.329200  328.000000\n\n[8 rows x 7 columns]\n\n\n\n\nLecture 1.1 Check in\n\n\n\nQuestion 3: What percent of Titanic passengers survived?\nQuestion 4: What was the average (mean) fare paid for a ticket?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#changing-variable-types",
    "href": "slides/01-tabular-data-summaries.html#changing-variable-types",
    "title": "Tabular Data and Variable Summaries",
    "section": "Changing Variable Types",
    "text": "Changing Variable Types\n\nThe variable pclass was categorical, but Python assumed it was quantitative.\nIt’s our job to check and fix data!\n\n\n\ndf[\"pclass\"] = df[\"pclass\"].astype(\"category\")\n\n\n\n\n\n\n\n\n\n\nWhy choose to store pclass as a \"category\" instead of a \"string\"?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#summary-of-categorical-variable",
    "href": "slides/01-tabular-data-summaries.html#summary-of-categorical-variable",
    "title": "Tabular Data and Variable Summaries",
    "section": "Summary of categorical variable",
    "text": "Summary of categorical variable\n\ndf[\"pclass\"].value_counts()\n\npclass\n3    709\n1    323\n2    277\nName: count, dtype: int64\n\n\n\n\n\ndf[\"pclass\"].value_counts(normalize = True)\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in-2",
    "href": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in-2",
    "title": "Tabular Data and Variable Summaries",
    "section": "Lecture 1.1 Check in",
    "text": "Lecture 1.1 Check in\nQuestion 5: What percent of Titanic passengers were in First Class?\nQuestion 6: Which is the correct way to change a numeric column to a categorical variable?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#distances",
    "href": "slides/06-preprocessing.html#distances",
    "title": "Dummy Variables and Column Transformers",
    "section": "Distances",
    "text": "Distances\n\nWe measure similarity between observations by calculating distances.\n\n\n\nEuclidean distance: sum of squared differences, then square root\nManhattan distance: sum of absolute differences\n\n\n\n\n\n\n\n\n\nscikit-learn\n\n\nUse the pairwise_distances() function to get back a 2D numpy array of distances.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#scaling",
    "href": "slides/06-preprocessing.html#scaling",
    "title": "Dummy Variables and Column Transformers",
    "section": "Scaling",
    "text": "Scaling\n\nIt is important that all our features be on the same scale for distances to be meaningful.\n\n\n\nStandardize: Subtract the mean (of the column) and divide by the standard deviation (of the column).\nMinMax: Subtract the minimum value, divide by the range.\n\n\n\n\n\n\n\n\n\nscikit-learn\n\n\nFollow the specify - fit - transform code structure. In the specify step, you should use the StandardScaler() or MinMaxScaler() functions.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#recall-ames-housing-data",
    "href": "slides/06-preprocessing.html#recall-ames-housing-data",
    "title": "Dummy Variables and Column Transformers",
    "section": "Recall: AMES Housing data",
    "text": "Recall: AMES Housing data\n\n\ndf = pd.read_table(\"https://datasci112.stanford.edu/data/housing.tsv\")\ndf.head()\n\n         PID  Gr Liv Area  Bedroom AbvGr  ...  Sale Type  Sale Condition  SalePrice\n0  526301100         1656              3  ...        WD           Normal     215000\n1  526350040          896              2  ...        WD           Normal     105000\n2  526351010         1329              3  ...        WD           Normal     172000\n3  526353030         2110              3  ...        WD           Normal     244000\n4  527105010         1629              3  ...        WD           Normal     189900\n\n[5 rows x 81 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#what-about-categorical-variables",
    "href": "slides/06-preprocessing.html#what-about-categorical-variables",
    "title": "Dummy Variables and Column Transformers",
    "section": "What about categorical variables?",
    "text": "What about categorical variables?\nSuppose we want to include the variable Bldg Type in our distance calculation…\n\ndf[\"Bldg Type\"].value_counts()\n\nBldg Type\n1Fam      2425\nTwnhsE     233\nDuplex     109\nTwnhs      101\n2fmCon      62\nName: count, dtype: int64\n\n\n\n\nThen we need a way to calculate \\((\\texttt{1Fam} - \\texttt{Twnhs} )^ 2\\).",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#converting-to-binary",
    "href": "slides/06-preprocessing.html#converting-to-binary",
    "title": "Dummy Variables and Column Transformers",
    "section": "Converting to Binary",
    "text": "Converting to Binary\nLet’s instead think about a variable that summarizes whether an observation is a single family home or not.\n\ndf[\"is_single_fam\"] = df[\"Bldg Type\"] == \"1Fam\"\ndf[\"is_single_fam\"].value_counts()\n\nis_single_fam\nTrue     2425\nFalse     505\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nWhat does a value of True represent? False?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummy-variables",
    "href": "slides/06-preprocessing.html#dummy-variables",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nWhen we transform a variable into binary (True / False), we call this variable a dummy variable or we say the variable has been one-hot-encoded.\n\n\nRemember that that computers interpret logical values (True / False) the same as 1 / 0:\n\ndf[\"is_single_fam\"] = df[\"is_single_fam\"].astype(\"int\")\ndf[\"is_single_fam\"].value_counts()\n\nis_single_fam\n1    2425\n0     505\nName: count, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#now-we-can-do-math",
    "href": "slides/06-preprocessing.html#now-we-can-do-math",
    "title": "Dummy Variables and Column Transformers",
    "section": "Now we can do math!",
    "text": "Now we can do math!\nSpecify\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import pairwise_distances\n\nscaler = StandardScaler()\n\n\n\nFit\n\n\n\ndf_orig = df[['Gr Liv Area', 'Bedroom AbvGr', 'is_single_fam']]\nscaler.fit(df_orig)\n\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\n\n\n\n\nTransform\n\n\ndf_scaled = scaler.transform(df_orig)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#calculating-distances",
    "href": "slides/06-preprocessing.html#calculating-distances",
    "title": "Dummy Variables and Column Transformers",
    "section": "Calculating Distances",
    "text": "Calculating Distances\n\n\ndists = pairwise_distances(df_scaled[[1707]], df_scaled)\nbest = (\n  dists\n  .argsort()\n  .flatten()\n  [1:10]\n  )\ndf_orig.iloc[best]\n\n      Gr Liv Area  Bedroom AbvGr  is_single_fam\n160          2978              5              1\n909          3082              5              1\n1288         2792              5              1\n2350         2784              5              1\n253          3222              5              1\n585          2640              5              1\n2027         2526              5              1\n2330         3390              5              1\n2501         2520              5              1",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-variables-1",
    "href": "slides/06-preprocessing.html#dummifying-variables-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying Variables",
    "text": "Dummifying Variables\n\n\n\nWhat if we don’t just want to study is_single_fam, but rather, all categories of the Bldg Type variable?\nIn principle, we just make dummy variables for each category: is_single_fam, is_twnhse, etc.\nEach category becomes one column, with 0’s and 1’s to show if the observation in that row matches that category.\nThat sounds pretty tedious, especially if you have a lot of categories…\nLuckily, we have shortcuts in both pandas and sklearn!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-pandas",
    "href": "slides/06-preprocessing.html#dummifying-in-pandas",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in Pandas",
    "text": "Dummifying in Pandas\n\n\npd.get_dummies(df[[\"Bldg Type\"]])\n\n      Bldg Type_1Fam  Bldg Type_2fmCon  ...  Bldg Type_Twnhs  Bldg Type_TwnhsE\n0               True             False  ...            False             False\n1               True             False  ...            False             False\n2               True             False  ...            False             False\n3               True             False  ...            False             False\n4               True             False  ...            False             False\n...              ...               ...  ...              ...               ...\n2925            True             False  ...            False             False\n2926            True             False  ...            False             False\n2927            True             False  ...            False             False\n2928            True             False  ...            False             False\n2929            True             False  ...            False             False\n\n[2930 rows x 5 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-pandas-1",
    "href": "slides/06-preprocessing.html#dummifying-in-pandas-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in Pandas",
    "text": "Dummifying in Pandas\n\n\n\n      Bldg Type_1Fam  Bldg Type_2fmCon  ...  Bldg Type_Twnhs  Bldg Type_TwnhsE\n0               True             False  ...            False             False\n1               True             False  ...            False             False\n2               True             False  ...            False             False\n3               True             False  ...            False             False\n4               True             False  ...            False             False\n...              ...               ...  ...              ...               ...\n2925            True             False  ...            False             False\n2926            True             False  ...            False             False\n2927            True             False  ...            False             False\n2928            True             False  ...            False             False\n2929            True             False  ...            False             False\n\n[2930 rows x 5 columns]\n\n\n\n\nSome things to notice here…\n\n\n\n\nWhat is the naming convention for the new columns?\nDoes this change the original dataframe df? If not, what would you need to do to add this information back in?\nWhat happens if you put the whole dataframe into the get_dummies function? What problems might arise from this?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-sklearn",
    "href": "slides/06-preprocessing.html#dummifying-in-sklearn",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in sklearn",
    "text": "Dummifying in sklearn\nSpecify\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\n\n\n\nFit\n\n\n\nencoder.fit(df[[\"Bldg Type\"]])\n\n\nOneHotEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder?Documentation for OneHotEncoderiFittedOneHotEncoder() \n\n\n\n\n\nTransform\n\n\ndf_bldg = encoder.transform(df[[\"Bldg Type\"]])\ndf_bldg\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 2930 stored elements and shape (2930, 5)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-sklearn-1",
    "href": "slides/06-preprocessing.html#dummifying-in-sklearn-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in sklearn",
    "text": "Dummifying in sklearn\n\n\n\n\ndf_bldg.todense()\n\n\n\n\n\n\n\n\nmatrix([[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]], shape=(2930, 5))\n\n\n\n\n\nThings to notice:\n\n\n\nWhat object type was the result?\nDoes this change the original dataframe df? If not, what would you need to do to add this information back in?\nWhat pros and cons do you see for the pandas approach vs the sklearn approach?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#preprocessing",
    "href": "slides/06-preprocessing.html#preprocessing",
    "title": "Dummy Variables and Column Transformers",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nSo far, we have now seen two preprocessing steps that might need to happen to do an analysis of distances:\n\nScaling the quantitative variables\nDummifying the categorical variables\n\n\n\n\nPreprocessing steps are things you do only to make the following analysis/visualization better.\n\nThis is not the same as data cleaning, where you make changes to fix the data (e.g., changing data types).\nThis is not the same as data wrangling, where you change the structure of the data (e.g., adding or deleting rows or columns).",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#lecture-4.1-quiz",
    "href": "slides/06-preprocessing.html#lecture-4.1-quiz",
    "title": "Dummy Variables and Column Transformers",
    "section": "Lecture 4.1 Quiz",
    "text": "Lecture 4.1 Quiz\nIdentify the following as cleaning, wrangling, or preprocessing:\n\n\nRemoving the $ symbol from a column and converting it to numeric.\nNarrowing your data down to only first class Titanic passengers, because you are not studying the others.\nConverting a Zip Code variable from numeric to categorical using .astype().\nCreating a new column called n_investment that counts the number of people who invested in a project.\nLog-transforming a column because it is very skewed.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#preprocessing-in-sklearn",
    "href": "slides/06-preprocessing.html#preprocessing-in-sklearn",
    "title": "Dummy Variables and Column Transformers",
    "section": "Preprocessing in sklearn",
    "text": "Preprocessing in sklearn\n\n\nUnlike cleaning and wrangling, the preprocessing steps are “temporary” changes to the dataframe.\n\n\n\n\n\nIt would be nice if we could trigger these changes as part of our analysis, instead of doing them “by hand”.\n\nThis is why the specify - fit - transform process is useful!\nWe will first specify all our preprocessing steps.\nThen we will fit the whole preprocess\nThen we will save the transform step for only when we need it.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#column-transformers-specify",
    "href": "slides/06-preprocessing.html#column-transformers-specify",
    "title": "Dummy Variables and Column Transformers",
    "section": "Column Transformers – Specify",
    "text": "Column Transformers – Specify\n\nfrom sklearn.compose import make_column_transformer\n\n\n\n\nfeatures = [\"Gr Liv Area\", \"Bedroom AbvGr\", \"Full Bath\", \"Half Bath\", \"Bldg Type\", \"Neighborhood\"]\n\npreproc = make_column_transformer(\n  (OneHotEncoder(), [\"Bldg Type\", \"Neighborhood\"]),\n    remainder = \"passthrough\")",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#column-transformers-fit",
    "href": "slides/06-preprocessing.html#column-transformers-fit",
    "title": "Dummy Variables and Column Transformers",
    "section": "Column Transformers – Fit",
    "text": "Column Transformers – Fit\n\npreproc.fit(df[features])\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Bldg Type', 'Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Bldg Type', 'Neighborhood'])]) onehotencoder['Bldg Type', 'Neighborhood'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder() remainder['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath', 'Half Bath'] passthroughpassthrough",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#column-transformers-transform",
    "href": "slides/06-preprocessing.html#column-transformers-transform",
    "title": "Dummy Variables and Column Transformers",
    "section": "Column Transformers – Transform",
    "text": "Column Transformers – Transform\n\npreproc.transform(df[features])\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 15717 stored elements and shape (2930, 37)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#things-to-notice",
    "href": "slides/06-preprocessing.html#things-to-notice",
    "title": "Dummy Variables and Column Transformers",
    "section": "Things to notice…",
    "text": "Things to notice…\n\nWhat submodule did we import make_column_transformer from?\nWhat are the two arguments to the make_column_transformer() function? What object structures are they?\nWhat happens if you fit and transform on the whole dataset, not just df[features]? Why might this be useful?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#lecture-activity-4.2",
    "href": "slides/06-preprocessing.html#lecture-activity-4.2",
    "title": "Dummy Variables and Column Transformers",
    "section": "Lecture Activity 4.2",
    "text": "Lecture Activity 4.2\nTry the following:\n\nWhat happens if you change remainder = \"passthrough\" to remainder = \"drop\"?\nWhat happens if you add the argument sparse_output = False to the OneHotEncoder() function?\nWhat happens if you add this line before the transform step: preproc.set_output(transform = \"pandas\") (keep the sparse_output = False when you try this)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#multiple-preprocessing-steps",
    "href": "slides/06-preprocessing.html#multiple-preprocessing-steps",
    "title": "Dummy Variables and Column Transformers",
    "section": "Multiple Preprocessing Steps",
    "text": "Multiple Preprocessing Steps\nWhy are column transformers so useful? We can do multiple preprocessing steps at once!\n\n\nfrom sklearn.preprocessing import StandardScaler\n\npreproc = make_column_transformer(\n        (StandardScaler(), \n        [\"Gr Liv Area\", \"Bedroom AbvGr\", \"Full Bath\", \"Half Bath\"]),\n        (OneHotEncoder(sparse_output = False), \n        [\"Bldg Type\", \"Neighborhood\"]),\n        remainder = \"drop\")",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#fit",
    "href": "slides/06-preprocessing.html#fit",
    "title": "Dummy Variables and Column Transformers",
    "section": "Fit!",
    "text": "Fit!\n\npreproc.fit(df)\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath',\n                                  'Half Bath']),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type', 'Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath',\n                                  'Half Bath']),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type', 'Neighborhood'])]) standardscaler['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath', 'Half Bath'] StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoder['Bldg Type', 'Neighborhood'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(sparse_output=False) \n\n\n\npreproc.set_output(transform = \"pandas\")\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath',\n                                  'Half Bath']),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type', 'Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath',\n                                  'Half Bath']),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type', 'Neighborhood'])]) standardscaler['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath', 'Half Bath'] StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoder['Bldg Type', 'Neighborhood'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(sparse_output=False)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#transform",
    "href": "slides/06-preprocessing.html#transform",
    "title": "Dummy Variables and Column Transformers",
    "section": "Transform!",
    "text": "Transform!\n\n\ndf_transformed = preproc.transform(df)\ndf_transformed\n\n      standardscaler__Gr Liv Area  ...  onehotencoder__Neighborhood_Veenker\n0                        0.309265  ...                                  0.0\n1                       -1.194427  ...                                  0.0\n2                       -0.337718  ...                                  0.0\n3                        1.207523  ...                                  0.0\n4                        0.255844  ...                                  0.0\n...                           ...  ...                                  ...\n2925                    -0.982723  ...                                  0.0\n2926                    -1.182556  ...                                  0.0\n2927                    -1.048015  ...                                  0.0\n2928                    -0.219006  ...                                  0.0\n2929                     0.989884  ...                                  0.0\n\n[2930 rows x 37 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#finding-all-categorical-variables",
    "href": "slides/06-preprocessing.html#finding-all-categorical-variables",
    "title": "Dummy Variables and Column Transformers",
    "section": "Finding All Categorical Variables",
    "text": "Finding All Categorical Variables\n\nWhat if we want to tell sklearn, “Please dummify every categorical variable.”? Use a selector instead of exact column names!\n\n\n\n\nfrom sklearn.compose import make_column_selector\n\npreproc = make_column_transformer(\n    (StandardScaler(),  \n     make_column_selector(dtype_include = np.number)\n     ),\n    (OneHotEncoder(sparse_output = False), \n     make_column_selector(dtype_include = object)\n     ),\n    remainder = \"passthrough\")\n\n\n\n\n\n\npreproc.set_output(transform = \"pandas\")\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x154bd2ba0&gt;),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x154a37890&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriNot fittedColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x154bd2ba0&gt;),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x154a37890&gt;)]) standardscaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x154bd2ba0&gt; StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoder&lt;sklearn.compose._column_transformer.make_column_selector object at 0x154a37890&gt; OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(sparse_output=False) remainder passthroughpassthrough",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#fit-and-transform",
    "href": "slides/06-preprocessing.html#fit-and-transform",
    "title": "Dummy Variables and Column Transformers",
    "section": "Fit AND Transform!",
    "text": "Fit AND Transform!\n\n\npreproc.fit_transform(df[features])\n\n      standardscaler__Gr Liv Area  ...  onehotencoder__Neighborhood_Veenker\n0                        0.309265  ...                                  0.0\n1                       -1.194427  ...                                  0.0\n2                       -0.337718  ...                                  0.0\n3                        1.207523  ...                                  0.0\n4                        0.255844  ...                                  0.0\n...                           ...  ...                                  ...\n2925                    -0.982723  ...                                  0.0\n2926                    -1.182556  ...                                  0.0\n2927                    -1.048015  ...                                  0.0\n2928                    -0.219006  ...                                  0.0\n2929                     0.989884  ...                                  0.0\n\n[2930 rows x 37 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#think-about-it",
    "href": "slides/06-preprocessing.html#think-about-it",
    "title": "Dummy Variables and Column Transformers",
    "section": "Think about it",
    "text": "Think about it\n\nWhat are the advantages of using a selector?\nWhat are the possible disadvantages of using a selector?\nDoes the order matter when using selectors? Try switching the steps and see what happens!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#takeaways-1",
    "href": "slides/06-preprocessing.html#takeaways-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe dummify or one-hot-encode categorical variables to make them numbers.\nWe can do this with pd.get_dummies() or with OneHotEncoder() from sklearn.\nColumn Transformers let us apply multiple preprocessing steps at the same time.\n\nThink about which variables you want to apply the steps to\nThink about options for the steps, like sparseness\nThink about passthrough in your transformer",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#if-you-are-having-trouble-finding-a-group-to-work-with",
    "href": "slides/10-cross-val.html#if-you-are-having-trouble-finding-a-group-to-work-with",
    "title": "Cross-Validation and Grid Search",
    "section": "If you are having trouble finding a group to work with",
    "text": "If you are having trouble finding a group to work with\nfill out this Google Form and Dr. T will help you!\n\n\nLoading…",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#modeling",
    "href": "slides/10-cross-val.html#modeling",
    "title": "Cross-Validation and Grid Search",
    "section": "Modeling",
    "text": "Modeling\nWe assume some process \\(f\\) is generating our target variable:\n\ntarget = f(predictors) + noise\n\n\nOur goal is to come up with an approximation of \\(f\\).",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#test-error-vs-training-error",
    "href": "slides/10-cross-val.html#test-error-vs-training-error",
    "title": "Cross-Validation and Grid Search",
    "section": "Test Error vs Training Error",
    "text": "Test Error vs Training Error\n\n\nWe don’t need to know how well our model does on training data.\nWe want to know how well it will do on test data.\nIn general, test error \\(&gt;\\) training error.\n\n\n\n\n\nAnalogy: A professor posts a practice exam before an exam.\n\n\nIf the actual exam is the same as the practice exam, how many points will students miss? That’s training error.\nIf the actual exam is different from the practice exam, how many points will students miss? That’s test error.\n\n\n\n\n\n\nIt’s always easier to answer questions that you’ve seen before than questions you haven’t seen.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#modeling-procedure",
    "href": "slides/10-cross-val.html#modeling-procedure",
    "title": "Cross-Validation and Grid Search",
    "section": "Modeling Procedure",
    "text": "Modeling Procedure\nFor each model proposed:\n\n\nEstablish a pipeline with transformers and a model.\nFit the pipeline on the training data (with known outcome)\nPredict with the fitted pipeline on test data (with known outcome)\nEvaluate our success; i.e., measure noise “left over”\n\n\n\nThen:\n\n\nSelect the best model\nFit on all the data\nPredict on any future data (with unknown outcome)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#simple-linear-model",
    "href": "slides/10-cross-val.html#simple-linear-model",
    "title": "Cross-Validation and Grid Search",
    "section": "Simple Linear Model",
    "text": "Simple Linear Model\n\nWe assume that the target (\\(Y\\)) is generated from an equation of the predictor (\\(X\\)), plus random noise (\\(\\epsilon\\))\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nGoal: Use observations \\((x_1, y_1), ..., (x_n, y_n)\\) to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\n\n\n\n\n\nWhat are these parameters???\n\n\nIn Statistics, we use \\(\\beta_0\\) to represent the population intercept and \\(\\beta_1\\) to represent the slope. By “population” we mean the true slope of the line for every observation in the population of interest.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#measures-of-success",
    "href": "slides/10-cross-val.html#measures-of-success",
    "title": "Cross-Validation and Grid Search",
    "section": "Measures of Success",
    "text": "Measures of Success\nWhat is the “best” choice of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) (the estimates of \\(\\beta_0\\) and \\(\\beta_1\\))?\n\nThe ones that are statistically most justified, under certain assumptions about \\(Y\\) and \\(X\\)?\nThe ones that are “closest to” the observed points?\n\n\\(|\\widehat{y}_i - y_i|\\)?\n\\((\\widehat{y}_i - y_i)^2\\)?\n\\((\\widehat{y}_i - y_i)^4\\)?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#example-wine-data",
    "href": "slides/10-cross-val.html#example-wine-data",
    "title": "Cross-Validation and Grid Search",
    "section": "Example: Wine Data",
    "text": "Example: Wine Data\n\ndf = pd.read_csv(\"https://dlsun.github.io/pods/data/bordeaux.csv\")\n\nknown = df[\"year\"] &lt; 1981\ndf_known = df[known]\nunknown = df[\"year\"] &gt; 1980\ndf_unknown = df[unknown]",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#price-predicted-by-age-of-wine",
    "href": "slides/10-cross-val.html#price-predicted-by-age-of-wine",
    "title": "Cross-Validation and Grid Search",
    "section": "Price Predicted by Age of Wine",
    "text": "Price Predicted by Age of Wine\n\n\n\nCode\nfrom mizani.formatters import currency_format\n\n(\n  ggplot(df_known, aes(x = \"age\", y = \"price\")) + \n  geom_point() +\n  labs(x = \"Age of Wine (Years Since 1992)\", \n       y = \"Price of Wine (in 1992 USD)\") +\n  scale_y_continuous(labels = currency_format(precision = 0))\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#candidate-regression-lines",
    "href": "slides/10-cross-val.html#candidate-regression-lines",
    "title": "Cross-Validation and Grid Search",
    "section": "“Candidate” Regression Lines",
    "text": "“Candidate” Regression Lines\n\nConsider five possible regression equations:\n\\[\\text{price} = 25 + 0*\\text{age}\\] \\[\\text{price} = 0 + 10*\\text{age}\\] \\[\\text{price} = 20 + 1*\\text{age}\\] \\[\\text{price} = -40 + 3*\\text{age}\\]\nWhich one do you think will be “closest” to the points on the scatterplot?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#candidate-regression-lines-1",
    "href": "slides/10-cross-val.html#candidate-regression-lines-1",
    "title": "Cross-Validation and Grid Search",
    "section": "“Candidate” Regression Lines",
    "text": "“Candidate” Regression Lines\n\n\n\nCode\n(\n  ggplot(data = df_known, mapping = aes(x = \"age\", y = \"price\")) +\n  geom_point() + \n  labs(x = \"Age of Wine (Years Since 1992)\", \n       y = \"Price of Wine (in 1992 USD)\") +\n  scale_y_continuous(labels = currency_format(precision = 0)) +\n  geom_abline(intercept = 25, \n              slope = 0, \n              color = \"blue\", \n              linetype = \"solid\") + \n  geom_abline(intercept = 0, \n              slope = 1, \n              color = \"orange\", \n              linetype = \"dashed\") + \n  geom_abline(intercept = 20, \n              slope = 1, \n              color = \"green\") + \n  geom_abline(intercept = -40, \n              slope = 3, \n              color = \"magenta\")\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#the-best-slope-and-intercept",
    "href": "slides/10-cross-val.html#the-best-slope-and-intercept",
    "title": "Cross-Validation and Grid Search",
    "section": "The “best” slope and intercept",
    "text": "The “best” slope and intercept\n\nIt’s clear that some of these lines are better than others.\nHow to choose the best? Math!\nWe’ll let the computer do it for us.\n\n\n\n\n\n\n\nCaution\n\n\nThe estimated slope and intercept are calculated from the training data at the .fit() step.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#linear-regression-in-sklearn",
    "href": "slides/10-cross-val.html#linear-regression-in-sklearn",
    "title": "Cross-Validation and Grid Search",
    "section": "Linear Regression in sklearn",
    "text": "Linear Regression in sklearn\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n\nSpecify\n\n\npipeline = make_pipeline(\n    LinearRegression()\n    )\n\n\nFit\n\n\n\npipeline.fit(\n  X = df_known[['age']], \n  y = df_known['price']\n  )\n\n\nPipeline(steps=[('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('linearregression', LinearRegression())]) LinearRegression?Documentation for LinearRegressionLinearRegression()",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#estimated-intercept-and-slope",
    "href": "slides/10-cross-val.html#estimated-intercept-and-slope",
    "title": "Cross-Validation and Grid Search",
    "section": "Estimated Intercept and Slope",
    "text": "Estimated Intercept and Slope\n\n\nEstimated Intercept\n\n(\n  pipeline\n  .named_steps['linearregression']\n  .intercept_\n  )\n\nnp.float64(-0.2997193011856538)\n\n\n\n\n\nEstimated Slope\n\n(\n  pipeline\n  .named_steps['linearregression']\n  .coef_\n  )\n\narray([1.15601827])",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#fitting-and-predicting",
    "href": "slides/10-cross-val.html#fitting-and-predicting",
    "title": "Cross-Validation and Grid Search",
    "section": "Fitting and Predicting",
    "text": "Fitting and Predicting\nTo predict from a linear regression, we just plug in the values to the equation…\n\n-0.3 + 1.16 * df_unknown[\"age\"] \n\n27    12.46\n28    11.30\n29    10.14\n30     8.98\n31     7.82\n32     6.66\n33     5.50\n34     4.34\n35     3.18\n36     2.02\n37     0.86\nName: age, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#fitting-and-predicting-with-.predict",
    "href": "slides/10-cross-val.html#fitting-and-predicting-with-.predict",
    "title": "Cross-Validation and Grid Search",
    "section": "Fitting and Predicting with .predict()",
    "text": "Fitting and Predicting with .predict()\nTo predict from a linear regression, we just plug in the values to the equation…\n\npipeline.predict(df_unknown[['age']])\n\narray([12.41648163, 11.26046336, 10.1044451 ,  8.94842683,  7.79240856,\n        6.6363903 ,  5.48037203,  4.32435376,  3.1683355 ,  2.01231723,\n        0.85629897])",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#questions-to-ask-yourself",
    "href": "slides/10-cross-val.html#questions-to-ask-yourself",
    "title": "Cross-Validation and Grid Search",
    "section": "Questions to ask yourself",
    "text": "Questions to ask yourself\n\n\n\nQ: Is there only one “best” regression line?\nA: No, you can justify many choices of slope and intercept! But there is a generally accepted approach called Least Squares Regression that we will always use.\nQ: How do you know which variables to include in the equation?\nA: Try them all, and see what predicts best.\nQ: How do you know whether to use linear regression or KNN to predict?\nA: Try them both, and see what predicts best!",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#resampling-methods",
    "href": "slides/10-cross-val.html#resampling-methods",
    "title": "Cross-Validation and Grid Search",
    "section": "Resampling methods",
    "text": "Resampling methods\n\n\nWe saw that a “fair” way to evaluate models was to randomly split into training and test sets.\nBut what if this randomness was misleading? (e.g., a major outlier in one of the sets)\nWhat do we usually do in Statistics to address randomness? Take many samples and compute an average!\nA resampling method is when we take many random test / training splits and average the resulting metrics.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#resampling-method-example",
    "href": "slides/10-cross-val.html#resampling-method-example",
    "title": "Cross-Validation and Grid Search",
    "section": "Resampling Method Example",
    "text": "Resampling Method Example\nImport all our functions:\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#creating-testing-training-splits",
    "href": "slides/10-cross-val.html#creating-testing-training-splits",
    "title": "Cross-Validation and Grid Search",
    "section": "Creating Testing & Training Splits",
    "text": "Creating Testing & Training Splits\n\nX_train, X_test, y_train, y_test = train_test_split(df_known, \n                                                    df_known['price'], \n                                                    test_size = 0.1)\n\n\n\nThis creates four new objects, X_train, X_test, y_train, and y_test.\n\nNote that the objects are created in this order!\n\n\n\n\n\nThe “testing” data are 10% (0.1) of the total size of the training data (df_known).",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#pipeline-for-predicting-on-test-data",
    "href": "slides/10-cross-val.html#pipeline-for-predicting-on-test-data",
    "title": "Cross-Validation and Grid Search",
    "section": "Pipeline for Predicting on Test Data",
    "text": "Pipeline for Predicting on Test Data\n\nSpecify\n\n\n\nfeatures = ['summer', 'har', 'sep', 'win', 'age']\nct = make_column_transformer(\n  (StandardScaler(), features),\n  remainder = \"drop\"\n)\n\npipeline = make_pipeline(\n    ct,\n    LinearRegression()\n    )\n\n\n\nFit for Training Data\n\n\n\npipeline = pipeline.fit(X = X_train, y = y_train)\n\n\n\nPredict for Test Data\n\n\n\npred_y_test = pipeline.predict(X = X_test)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#estimating-error-for-test-data",
    "href": "slides/10-cross-val.html#estimating-error-for-test-data",
    "title": "Cross-Validation and Grid Search",
    "section": "Estimating Error for Test Data",
    "text": "Estimating Error for Test Data\n\nmean_squared_error(y_test, pred_y_test)\n\n411.0692047173348",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#cross-validation-1",
    "href": "slides/10-cross-val.html#cross-validation-1",
    "title": "Cross-Validation and Grid Search",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nIt makes sense to do test / training many times…\nBut! Remember the original reason for test / training: we don’t want to use the same data in fitting and evaluation.\n\n\nIdea: Let’s make sure that each observation only gets to be in the test set once\n\n\n\nCross-validation: Divide the data into 10 random “folds”. Each fold gets a “turn” as the test set.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#cross-validation-5-fold",
    "href": "slides/10-cross-val.html#cross-validation-5-fold",
    "title": "Cross-Validation and Grid Search",
    "section": "Cross-Validation (5-Fold)",
    "text": "Cross-Validation (5-Fold)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#cross-validation-in-sklearn",
    "href": "slides/10-cross-val.html#cross-validation-in-sklearn",
    "title": "Cross-Validation and Grid Search",
    "section": "Cross-Validation in sklearn",
    "text": "Cross-Validation in sklearn\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(pipeline, \n                X = df_known, \n                y = df_known['price'], \n                cv = 10)\n\narray([  0.53885509,   0.36267134,   0.6164344 ,  -2.52293886,\n         0.75103464,   0.89242533, -49.63018969, -21.17272593,\n        -0.16083366,   0.21125165])",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#cross-validation-in-sklearn-1",
    "href": "slides/10-cross-val.html#cross-validation-in-sklearn-1",
    "title": "Cross-Validation and Grid Search",
    "section": "Cross-Validation in sklearn",
    "text": "Cross-Validation in sklearn\n\nWhat are these numbers?\n\n\nsklearn chooses a default metric for you based on the model.\nIn the case of regression, the default metric is R-Squared.\n\n\n\n\n\nWhy use negative root mean squared error?\n\n\nTo be consistent! We will always want to maximize this score.\nLarger R-Squared values explain more of the variance in the response (y).\nLarger negative RMSE (smaller RMSE) means the “leftover” variance in y is minimized.",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#what-do-we-do-with-these-numbers",
    "href": "slides/10-cross-val.html#what-do-we-do-with-these-numbers",
    "title": "Cross-Validation and Grid Search",
    "section": "What do we do with these numbers?",
    "text": "What do we do with these numbers?\n\nfrom sklearn.model_selection import cross_val_score\n\ncvs = cross_val_score(pipeline, \n                      X = df_known, \n                      y = df_known['price'], \n                      cv = 10)\n\nSince we have 10 different values, what would you expect us to do?\n\nWell, this is a statistics class after all. So, you probably guessed we would take the mean.\n\ncvs.mean()\n\nnp.float64(-7.011401568335392)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#cross-validation-in-sklearn-2",
    "href": "slides/10-cross-val.html#cross-validation-in-sklearn-2",
    "title": "Cross-Validation and Grid Search",
    "section": "Cross-Validation in sklearn",
    "text": "Cross-Validation in sklearn\nWhat if you want MSE?\n\ncv_scores = cross_val_score(pipeline, \n                            X = df_known, \n                            y = df_known['price'], \n                            cv = 10, \n                            scoring = \"neg_mean_squared_error\")\ncv_scores\n\narray([ -54.51757584, -301.38564331, -521.90492178, -247.38859562,\n        -59.30908038,  -14.0803285 , -348.78575122, -138.57953707,\n        -74.29335417,   -9.66216728])",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#cross-validation-faq",
    "href": "slides/10-cross-val.html#cross-validation-faq",
    "title": "Cross-Validation and Grid Search",
    "section": "Cross-Validation: FAQ",
    "text": "Cross-Validation: FAQ\n\n\n\nQ: How many cross validations should we do?\nA: It doesn’t matter much! Typical choices are 5 or 10.\nA: Think about the trade-offs:\n\nlarger training sets = more accurate models\nsmaller test sets = more uncertainty in evaluation\n\nQ: What metric should we use?\nA: This is also your choice! What captures your idea of a “successful prediction”? MSE / RMSE is a good default, but you might find other options that are better!\nQ: I took a statistics class before, and I remember some things like “adjusted R-Squared” or “AIC” for model selection. What about those?\nA: Those are Old School, from a time when computers were not powerful enough to do cross-validation. Modern data science uses resampling!",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#your-turn",
    "href": "slides/10-cross-val.html#your-turn",
    "title": "Cross-Validation and Grid Search",
    "section": "Your turn",
    "text": "Your turn\n\nUse cross-validation to choose between Linear Regression and KNN with k = 7 based on \"neg_mean_squared_error\", for:\n\nUsing all predictors.\nUsing just winter rainfall and summer temperature.\nUsing only age.\n\nRe-run #1, but instead use mean absolute error. (You will need to look in the documentation of cross_val_score() for this!)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#tuning",
    "href": "slides/10-cross-val.html#tuning",
    "title": "Cross-Validation and Grid Search",
    "section": "Tuning",
    "text": "Tuning\n\nIn previous classes, we tried many different values of \\(k\\) for KNN.\nWe also mentioned using absolute distance (Manhattan) instead of euclidean distance.\nNow, we would like to use cross-validation to decide between these options.\n\n\nsklearn provides a nice shortcut for this!",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#initializing-gridsearchcv",
    "href": "slides/10-cross-val.html#initializing-gridsearchcv",
    "title": "Cross-Validation and Grid Search",
    "section": "Initializing GridSearchCV()",
    "text": "Initializing GridSearchCV()\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_cv = GridSearchCV(\n    pipeline,\n    param_grid = {\n        \"kneighborsregressor__n_neighbors\": range(1, 7),\n        \"kneighborsregressor__metric\": [\"euclidean\", \"manhattan\"],\n    },\n    scoring = \"neg_mean_squared_error\", \n    cv = 5)\n\n\n\n\n\n\n\n\n\nSame column transformer and pipeline!\n\n\n\nfeatures = ['summer', 'har', 'sep', 'win', 'age']\n\nct = make_column_transformer(\n  (StandardScaler(), features),\n  remainder = \"drop\"\n)\n\npipeline = make_pipeline(\n    ct,\n    KNeighborsRegressor()\n    )",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#fitting-gridsearchcv",
    "href": "slides/10-cross-val.html#fitting-gridsearchcv",
    "title": "Cross-Validation and Grid Search",
    "section": "Fitting GridSearchCV()",
    "text": "Fitting GridSearchCV()\n\n\n\ngrid_cv.fit(df_known, \n            df_known['price'])\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         ['summer',\n                                                                          'har',\n                                                                          'sep',\n                                                                          'win',\n                                                                          'age'])])),\n                                       ('kneighborsregressor',\n                                        KNeighborsRegressor())]),\n             param_grid={'kneighborsregressor__metric': ['euclidean',\n                                                         'manhattan'],\n                         'kneighborsregressor__n_neighbors': range(1, 7)},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         ['summer',\n                                                                          'har',\n                                                                          'sep',\n                                                                          'win',\n                                                                          'age'])])),\n                                       ('kneighborsregressor',\n                                        KNeighborsRegressor())]),\n             param_grid={'kneighborsregressor__metric': ['euclidean',\n                                                         'manhattan'],\n                         'kneighborsregressor__n_neighbors': range(1, 7)},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor',\n                 KNeighborsRegressor(metric='manhattan', n_neighbors=4))]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=4) \n\n\n\n\n\nHow many different models were fit with this grid?",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#getting-the-cross-validation-results",
    "href": "slides/10-cross-val.html#getting-the-cross-validation-results",
    "title": "Cross-Validation and Grid Search",
    "section": "Getting the Cross Validation Results",
    "text": "Getting the Cross Validation Results\n\n\npd.DataFrame(grid_cv.cv_results_)\n\n\n\n\n\n    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n0        0.001543      0.000311  ...      219.338831               12\n1        0.001271      0.000002  ...      209.511631                9\n2        0.001261      0.000008  ...      178.819362                6\n3        0.001263      0.000005  ...      217.989122               11\n4        0.001400      0.000230  ...      212.797774                7\n5        0.001243      0.000007  ...      205.240907                3\n6        0.001454      0.000346  ...      227.978005               10\n7        0.001237      0.000004  ...      175.175403                2\n8        0.001246      0.000023  ...      174.981203                5\n9        0.001229      0.000008  ...      175.186322                1\n10       0.001224      0.000008  ...      200.091917                4\n11       0.001223      0.000008  ...      232.710455                8\n\n[12 rows x 15 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#what-about-k-and-the-distances",
    "href": "slides/10-cross-val.html#what-about-k-and-the-distances",
    "title": "Cross-Validation and Grid Search",
    "section": "What about k and the distances?",
    "text": "What about k and the distances?\n\n\npd.DataFrame(grid_cv.cv_results_)[['param_kneighborsregressor__metric',\n                                   'param_kneighborsregressor__n_neighbors',\n                                   'mean_test_score']]\n\n\n\n\n\n   param_kneighborsregressor__metric  ...  mean_test_score\n0                          euclidean  ...      -321.833333\n1                          euclidean  ...      -292.281667\n2                          euclidean  ...      -278.301481\n3                          euclidean  ...      -308.950417\n4                          euclidean  ...      -279.847733\n5                          euclidean  ...      -259.804074\n6                          manhattan  ...      -305.793333\n7                          manhattan  ...      -254.886667\n8                          manhattan  ...      -274.401481\n9                          manhattan  ...      -232.973333\n10                         manhattan  ...      -267.988800\n11                         manhattan  ...      -282.869444\n\n[12 rows x 3 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#what-were-the-best-parameters",
    "href": "slides/10-cross-val.html#what-were-the-best-parameters",
    "title": "Cross-Validation and Grid Search",
    "section": "What were the best parameters?",
    "text": "What were the best parameters?\n\ngrid_cv.best_params_\n\n{'kneighborsregressor__metric': 'manhattan', 'kneighborsregressor__n_neighbors': 4}",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#model-evaluation",
    "href": "slides/10-cross-val.html#model-evaluation",
    "title": "Cross-Validation and Grid Search",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nYou have now encountered three types of decisions for finding your best model:\n\n\nWhich predictors should we include, and how should we preprocess them? (Feature selection)\nShould we use Linear Regression or KNN or something else? (Model selection)\nWhich value of \\(k\\) should we use? (Hyperparameter tuning)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/10-cross-val.html#model-evaluation-1",
    "href": "slides/10-cross-val.html#model-evaluation-1",
    "title": "Cross-Validation and Grid Search",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nThink of this like a college sports bracket:\n\nGather all your candidate pipelines (combinations of column transformers and model specifications)\nTune each pipeline with cross-validation (regional championships!)\nDetermine the best model type for each feature set (state championships!)\nDetermine the best pipeline (national championships!)",
    "crumbs": [
      "Lecture Slides",
      "Week 6, Part 2 - Cross Validation & Grid Search"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#changes-from-week-1",
    "href": "slides/03-quantitative-variables.html#changes-from-week-1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Changes from Week 1",
    "text": "Changes from Week 1\n\nWeek 1 taught me that I need to make some adjustments!\n\n\n\n\n\nLab Attendance\n\n\nIs not required. However, if you do not attend lab and come to student hours or post questions on Discord about the lab, I will be displeased.\n\n\n\n\n\n\n\nDeadlines\n\n\n\n\nLabs are due the day after lab (like lecture quizzes / activities).\n\nTuesday’s lab is due on Wednesday at 11:59pm.\nThursday’s lab is due on Friday at 11:59pm.\n\nEnd of week assignments are due on Sundays at 11:59pm (not Saturdays).\n\n\n\n\n\n\n\n\n\nLab Submissions\n\n\n\nPDFs will be required for every Collab submission.\n\nYour code cannot be more than 80 characters—where the grey line appears in Collab.\nUse returns!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#getting-prepping-and-summarizing-data",
    "href": "slides/03-quantitative-variables.html#getting-prepping-and-summarizing-data",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Getting, prepping, and summarizing data",
    "text": "Getting, prepping, and summarizing data\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\ndf[\"pclass\"] = df[\"pclass\"].astype(\"category\")\ndf[\"survived\"] = df[\"survived\"].astype(\"category\")",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#marginal-distributions",
    "href": "slides/03-quantitative-variables.html#marginal-distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Marginal Distributions",
    "text": "Marginal Distributions\nIf I choose a passenger at random, what is the probability they rode in 1st class?\n\nmarginal_class = (\n  df['pclass']\n  .value_counts(normalize = True)\n  )\nmarginal_class\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#joint-distributions",
    "href": "slides/03-quantitative-variables.html#joint-distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Joint Distributions",
    "text": "Joint Distributions\nIf I choose a passenger at random, what is the probability they are a woman who rode in first class?\n\njoint_class_sex = (\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize=True)\n  .unstack()\n  )\n  \njoint_class_sex\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#conditional-distributions",
    "href": "slides/03-quantitative-variables.html#conditional-distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nIf I choose a woman at random, what is the probability they rode in first class?\n\nmarginal_sex = (\n  df['sex']\n  .value_counts(normalize = True)\n  )\n  \njoint_class_sex.divide(marginal_sex)\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#visualizing-with-plotnine",
    "href": "slides/03-quantitative-variables.html#visualizing-with-plotnine",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Visualizing with plotnine",
    "text": "Visualizing with plotnine\n\n\n(\n  ggplot(df, aes(x = \"sex\", fill = \"pclass\")) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#quantitative-variables-1",
    "href": "slides/03-quantitative-variables.html#quantitative-variables-1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\nWe have analyzed a quantitative variable already. Where?\nIn the Colombia COVID data!\n\n\ndf_CO = pd.read_csv(\"http://dlsun.github.io/pods/data/covid/colombia_2020-05-28.csv\")\ndf_CO\n\n            Departamento  Edad  ... Fecha de diagnóstico Fecha recuperado\n0            Bogotá D.C.    19  ...           2020-03-06       2020-03-13\n1        Valle del Cauca    34  ...           2020-03-09       2020-03-19\n2              Antioquia    50  ...           2020-03-09       2020-03-15\n3              Antioquia    55  ...           2020-03-11       2020-03-26\n4              Antioquia    25  ...           2020-03-11       2020-03-23\n...                  ...   ...  ...                  ...              ...\n25361  Buenaventura D.E.    48  ...           2020-05-28              NaN\n25362    Valle del Cauca    55  ...           2020-05-28              NaN\n25363  Buenaventura D.E.    39  ...           2020-05-28              NaN\n25364    Valle del Cauca    13  ...           2020-05-28              NaN\n25365            Córdoba     0  ...           2020-05-28              NaN\n\n[25366 rows x 10 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#option-1-convert-it-to-categorical",
    "href": "slides/03-quantitative-variables.html#option-1-convert-it-to-categorical",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Option 1: Convert it to categorical",
    "text": "Option 1: Convert it to categorical\nTo visualize the age variable, we did the following:\n\ndf_CO[\"age\"] = pd.cut(\n    df_CO[\"Edad\"],\n    bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 120],\n    labels = [\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80+\"],\n    right = False, \n    ordered = True)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#option-1-then-make-a-barplot",
    "href": "slides/03-quantitative-variables.html#option-1-then-make-a-barplot",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Option 1: Then make a barplot",
    "text": "Option 1: Then make a barplot\nThen, we could treat age as categorical and make a barplot:\n\n\n\nCode\n(\n  ggplot(df_CO, aes(x = \"age\")) + \n  geom_bar() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#option-2-treat-it-as-a-quantitative-variable",
    "href": "slides/03-quantitative-variables.html#option-2-treat-it-as-a-quantitative-variable",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Option 2: Treat it as a quantitative variable!",
    "text": "Option 2: Treat it as a quantitative variable!\nA histogram uses equal sized bins to summarize a quantitative variable.\n\n\n\nCode\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram() + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#changing-binwidth",
    "href": "slides/03-quantitative-variables.html#changing-binwidth",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Changing Binwidth",
    "text": "Changing Binwidth\nTo tweak your histogram, you can change the binwith:\n\n\n\n\n\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram(binwidth = 1) + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram(binwidth = 10) + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#adding-color-outline",
    "href": "slides/03-quantitative-variables.html#adding-color-outline",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Adding Color & Outline",
    "text": "Adding Color & Outline\n\n\n\nCode\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram(binwidth = 10, \n                 color = \"white\", \n                 fill = \"gray\") + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#using-percents-instead-of-counts",
    "href": "slides/03-quantitative-variables.html#using-percents-instead-of-counts",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Using Percents Instead of Counts",
    "text": "Using Percents Instead of Counts\n\n\n\nCode\n(\n  ggplot(df_CO, mapping = aes(x = \"Edad\")) + \n  geom_histogram(mapping = aes(y = '..density..'), \n                 binwidth = 10, \n                 color = \"white\", \n                 fill = \"gray\") + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#distributions",
    "href": "slides/03-quantitative-variables.html#distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Distributions",
    "text": "Distributions\n\nRecall the distribution of a categorical variable:\n\nWhat are the possible values and how common is each?\n\nThe distribution of a quantitative variable is similar:\n\nThe total area in the histogram is 1.0 (or 100%).",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#densities",
    "href": "slides/03-quantitative-variables.html#densities",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Densities",
    "text": "Densities\n\n\nIn this example, we have a limited set of possible values for age: 0, 1, 2, …., 100.\n\nWe call this a discrete variable.\n\n\n\n\n\n\nWhat if had a quantitative variable with infinite values?\n\nFor example: Price of a ticket on Titanic.\nWe call this a continuous variable.\n\n\n\n\n\n\n\nIn this case, it is not possible to list all possible values and how likely each one is.\n\nOne person paid $2.35\nTwo people paid $12.50\nOne person paid $34.98\n\\(\\vdots\\)\n\n\n\n\n\n\n\nInstead, we talk about ranges of values.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#densities-1",
    "href": "slides/03-quantitative-variables.html#densities-1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Densities",
    "text": "Densities\nAbout what percent of people in this dataset are below 18?\n\n\n\nCode\n(\n  ggplot(data = df_CO, mapping = aes(x = \"Edad\")) + \n  geom_histogram(mapping = aes(y = '..density..'), \n                 bins = 10) + \n  geom_vline(xintercept = 18, \n             color = \"red\", \n             size = 2, \n             linetype = \"dashed\") +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nHow would you code it?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#mean",
    "href": "slides/03-quantitative-variables.html#mean",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Mean",
    "text": "Mean\n\n\nOne summary of the center of a quantitative variable is the mean.\nWhen you hear “The average age is…” or the “The average income is…”, this probably refers to the mean.\nSuppose we have five people, ages: 4, 84, 12, 27, 7\nThe mean age is: \\[(4 + 84 + 12 + 27 + 7) / 5 = 134 / 5 = 26.8\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#notation-interlude",
    "href": "slides/03-quantitative-variables.html#notation-interlude",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Notation Interlude",
    "text": "Notation Interlude\n\n\n\nTo refer to our data without having to list all the numbers, we use \\(x_1, x_2, ..., x_n\\)\nIn the previous example, \\(x_1 = 4, x_2 = 84, x_3 = 12, x_4 = 27, x_5 = 7\\). So, \\(n = 5\\).\nTo add up all the numbers, we use the summation notation: \\[ \\sum_{i = 1}^5 x_i = 134\\]\nTherefore, the mean is: \\[\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#means-in-python",
    "href": "slides/03-quantitative-variables.html#means-in-python",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Means in Python",
    "text": "Means in Python\nLong version: find the sum and the number of observations\n\nsum_age = df_CO[\"Edad\"].sum()\nn = len(df_CO)\n\nsum_age / n\n\nnp.float64(39.04742568792872)\n\n\n\nShort version: use the built-in .mean() function!\n\ndf_CO[\"Edad\"].mean()\n\nnp.float64(39.04742568792872)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#activity-2.1",
    "href": "slides/03-quantitative-variables.html#activity-2.1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Activity 2.1",
    "text": "Activity 2.1\nThe mean is only one option for summarizing the center of a quantitative variable. It isn’t perfect!\nLet’s investigate this.\n\nOpen the Activity 2.1 Collab notebook\nRead in the Titanic data\nPlot the density of ticket prices on titanic\nCalculate the mean price\nSee how many people paid more than mean price",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#what-happened",
    "href": "slides/03-quantitative-variables.html#what-happened",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "What happened",
    "text": "What happened\n\nOur fare data was skewed right: Most values were small, but a few values were very large.\nThese large values “pull” the mean up; just how the value 84 pulled the average age up in our previous example.\nSo, why do we like the mean?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#squared-error",
    "href": "slides/03-quantitative-variables.html#squared-error",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Squared Error",
    "text": "Squared Error\n\n\nRecall: Ages 4, 84, 12, 27, 7.\n\n\nages = np.array([4, 84, 12, 27, 7])\n\n\nImagine that we had to “guess” the age of the next person.\n\n\n\n\n\n\n\nIf we guess 26.8, then our “squared error” for these five people is:\n\n\nsq_error = (ages - 26.8) ** 2\n\n(\n  sq_error\n  .round(decimals = 1)\n  .sum()\n  )\n\nnp.float64(4402.6)\n\n\n\n\n\n\n\n\n\n\nIf we guess 20, then our “squared error” for these five people is:\n\n\nsq_error = (ages - 20) ** 2\n(\n  sq_error\n  .round(decimals = 1)\n  .sum()\n  )\n\nnp.int64(4634)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#minimizing-squared-error",
    "href": "slides/03-quantitative-variables.html#minimizing-squared-error",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Minimizing squared error",
    "text": "Minimizing squared error\n\n\n\nCode\ncs = range(1, 60)\nsum_squared_distances = []\n\nfor c in cs:\n  (\n    sum_squared_distances\n    .append(\n      (\n        (df_CO[\"Edad\"] - c) ** 2\n      )\n      .sum()\n      )\n\nres_df = pd.DataFrame({\"center\": cs, \"sq_error\": sum_squared_distances})\n\n(\n  ggplot(res_df, aes(x = 'center', y = 'sq_error')) + \n  geom_line() +\n  labs(x = \"Mean\", \n       y = \"\", \n       title = \"Changes in Sum of Squared Error Based on Choice of Center\")\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#median",
    "href": "slides/03-quantitative-variables.html#median",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Median",
    "text": "Median\n\nAnother summary of center is the median, which is the “middle” of the sorted values.\nTo calculate the median of a quantitative variable with values \\(x_1, x_2, x_3, ..., x_n\\), we do the following steps:\n\n\n\n\nSort the values from smallest to largest: \\[x_{(1)}, x_{(2)}, x_{(3)}, ..., x_{(n)}.\\]\nThe “middle” value depends on whether we have an odd or an even number of observations.\n\nIf \\(n\\) is odd, then the middle value is \\(x_{(\\frac{n + 1}{2})}\\).\nIf \\(n\\) is even, then there are two middle values, \\(x_{(\\frac{n}{2})}\\) and \\(x_{(\\frac{n}{2} + 1)}\\).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is conventional to report the mean of the two values (but you can actually pick any value between them)!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#median-in-python",
    "href": "slides/03-quantitative-variables.html#median-in-python",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Median in Python",
    "text": "Median in Python\nAges: 4, 84, 12, 7, 27. What is the median?\nMedian age in the Columbia data:\n\ndf_CO[\"Edad\"].median()\n\nnp.float64(37.0)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#variance",
    "href": "slides/03-quantitative-variables.html#variance",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Variance",
    "text": "Variance\n\n\n\nOne measure of spread is the variance.\nThe variance of a variable whose values are \\(x_1, x_2, x_3, ..., x_n\\) is calculated using the formula \\[\\textrm{var(X)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\]\n\n\n\n\n\n\n\n\n\n\nDoes this look familiar?\n\n\nIt’s the sum of squared error! Well, divided by \\(n-1\\), the “degrees of freedom”.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#variance-in-python",
    "href": "slides/03-quantitative-variables.html#variance-in-python",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Variance in Python",
    "text": "Variance in Python\nSimilar to calculating the mean, we could find the variance manually:\n\n(\n  ((df_CO[\"Edad\"] - df_CO[\"Edad\"].mean()) ** 2)\n  .sum() / (len(df_CO) - 1)\n  )\n\nnp.float64(348.0870469898451)\n\n\n\n…or using a built-in Python function.\n\ndf_CO[\"Edad\"].var()\n\nnp.float64(348.0870469898451)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#standard-deviation",
    "href": "slides/03-quantitative-variables.html#standard-deviation",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nNotice that the variance isn’t very intuitive. What do we mean by “The spread is 348”?\nThis is because it is the squared error!\n\n\n\n\n\nTo get it in more interpretable language, we can take the square root:\n\n\nnp.sqrt(df_CO[\"Edad\"].var())\n\nnp.float64(18.65709106452142)\n\n\n\n\n\n\nOr, we use the built-in function!\n\ndf_CO[\"Edad\"].std()\n\nnp.float64(18.65709106452142)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#takeaway-messages",
    "href": "slides/03-quantitative-variables.html#takeaway-messages",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Takeaway Messages",
    "text": "Takeaway Messages\n\n\n\nVisualize quantitative variables with histograms or densities.\nSummarize the center of a quantitative variable with mean or median.\nDescribe the shape of a quantitative variable with skew.\nSummarize the spread of a quantitative variable with the variance or the standard deviation.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#classification",
    "href": "slides/12-logistic.html#classification",
    "title": "Logistic Regression",
    "section": "Classification",
    "text": "Classification\n\n\n\nWe can do KNN for Classification by letting the nearest neighbors “vote”.\nThe number of votes is a “probability”.\nA classification model must be evaluated differently than a regression model.\nOne possible metric is accuracy, but this is a bad choice in situations with imbalanced data.\nPrecision measures “if we say it’s in Class A, is it really?”\nRecall measures “if it’s really in Class A, did we find it?”\nF1 Score is a balance of precision and recall.\nMacro F1 Score averages the F1 scores of all classes.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#breast-tissue-classification",
    "href": "slides/12-logistic.html#breast-tissue-classification",
    "title": "Logistic Regression",
    "section": "Breast Tissue Classification",
    "text": "Breast Tissue Classification\nElectrical signals can be used to detect whether tissue is cancerous.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#analysis-goal",
    "href": "slides/12-logistic.html#analysis-goal",
    "title": "Logistic Regression",
    "section": "Analysis Goal",
    "text": "Analysis Goal\nThe goal is to determine whether a sample of breast tissue is:\n\n\nNot Cancerous\n\nconnective tissue\nadipose tissue\nglandular tissue\n\n\n\n\nCancerous\n\ncarcinoma\nfibro-adenoma\nmastopathy",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#binary-response-cancer-or-not",
    "href": "slides/12-logistic.html#binary-response-cancer-or-not",
    "title": "Logistic Regression",
    "section": "Binary response: Cancer or Not",
    "text": "Binary response: Cancer or Not\nLet’s read the data, and also make a new variable called “Cancerous”.\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/BreastTissue.csv\")\n\ncancer_levels = [\"car\", \"fad\", \"mas\"]\ndf['Cancerous'] = df['Class'].isin(cancer_levels)\n\n\n\n   Case # Class          I0  ...          DR           P  Cancerous\n0       1   car  524.794072  ...  220.737212  556.828334       True\n1       2   car  330.000000  ...   99.084964  400.225776       True\n2       3   car  551.879287  ...  253.785300  656.769449       True\n3       4   car  380.000000  ...  105.198568  493.701814       True\n4       5   car  362.831266  ...  103.866552  424.796503       True\n\n[5 rows x 12 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression",
    "href": "slides/12-logistic.html#counter-example-linear-regression",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\nWe know that in computers, True = 1 and False = 0. So, why not convert our response variable, Cancerous, to numbers and fit a regression?\n\n\ndf['Cancerous'] = df['Cancerous'].astype('int')\n\n\ndf.head()\n\n   Case # Class          I0  ...          DR           P  Cancerous\n0       1   car  524.794072  ...  220.737212  556.828334          1\n1       2   car  330.000000  ...   99.084964  400.225776          1\n2       3   car  551.879287  ...  253.785300  656.769449          1\n3       4   car  380.000000  ...  105.198568  493.701814          1\n4       5   car  362.831266  ...  103.866552  424.796503          1\n\n[5 rows x 12 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression-1",
    "href": "slides/12-logistic.html#counter-example-linear-regression-1",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n  LinearRegression()\n  )\n\npipeline = pipeline.fit(X = df[[\"I0\", \"PA500\"]],\n                        y = df['Cancerous'])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression-2",
    "href": "slides/12-logistic.html#counter-example-linear-regression-2",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\nProblem 1: Did we get “reasonable” predictions?\n\npred_cancer = pipeline.predict(df[[\"I0\", \"PA500\"]])\n\n\n\n\n\npred_cancer.min()\n\nnp.float64(-0.2666656752041746)\n\n\n\n\n\n\npred_cancer.max()\n\nnp.float64(1.1443978861600579)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression-3",
    "href": "slides/12-logistic.html#counter-example-linear-regression-3",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\nProblem 2: How do we translate these predictions into categories???\n\npred_cancer = pipeline.predict(df[[\"I0\", \"PA500\"]])\npred_cancer\n\narray([ 0.74019045,  0.89022436,  0.82501565,  0.90210945,  0.82407215,\n        0.70880368,  0.73084215,  0.75641256,  0.81285854,  0.84265773,\n        1.05274916,  0.83203994,  0.82221736,  0.98879611,  0.84321428,\n        1.14439789,  0.87065951,  0.82357191,  0.88721004,  0.86017708,\n        0.75973787,  0.56958917,  0.50364921,  0.84044159,  0.66291026,\n        0.5291845 ,  0.58770322,  0.58565156,  0.53944124,  0.54086431,\n        0.626959  ,  0.60392716,  0.84459104,  0.65514535,  0.83767168,\n        0.73230038,  0.82568408,  0.75333313,  0.54012943,  0.54183212,\n        0.50716066,  0.70881584,  0.52734464,  0.66456375,  0.73629418,\n        0.85237593,  0.84451578,  0.62061155,  0.60509501,  0.47440789,\n        0.78797208,  0.74240828,  0.91841366,  0.71160435,  0.63338505,\n        0.7122256 ,  0.82410488,  0.55742465,  0.62545421,  0.73912902,\n        0.73912902,  0.70136245,  0.78113432,  0.77907358,  0.51597274,\n        0.49247652,  0.65694204,  0.72574607,  0.76292245,  0.82501906,\n        0.04266066,  0.24283615,  0.30785716,  0.52977377,  0.12835422,\n        0.35012437,  0.39042852,  0.34447888,  0.10874534,  0.22844426,\n        0.35125209,  0.25457208,  0.12820727,  0.18275452, -0.06826015,\n       -0.02235371,  0.05662524, -0.02161184,  0.03275109, -0.03722978,\n        0.05434445, -0.11400134,  0.0985149 , -0.01465065,  0.05067869,\n        0.04211471,  0.05774736, -0.26666568, -0.13914905, -0.12585178,\n       -0.02264956,  0.06060737,  0.04839979,  0.12583274, -0.17299233,\n       -0.22474136])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression-4",
    "href": "slides/12-logistic.html#counter-example-linear-regression-4",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\nProblem 3: Was the relationship really linear???\n\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(data = df, \n         mapping = aes(x = \"I0\", y = \"Cancerous\")) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = False) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression-5",
    "href": "slides/12-logistic.html#counter-example-linear-regression-5",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\nProblem 4: Are the errors really random???\n\n\n\nCode\nresiduals = df['Cancerous'] - pred_cancer\n\n(\n  ggplot(data = df, \n         mapping = aes(x = \"I0\", y = residuals)) + \n  geom_point()\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#counter-example-linear-regression-6",
    "href": "slides/12-logistic.html#counter-example-linear-regression-6",
    "title": "Logistic Regression",
    "section": "Counter-Example: Linear Regression",
    "text": "Counter-Example: Linear Regression\nProblem 5: Are the errors normally distributed???\n\n\n\nCode\n(\n  ggplot(data = df, \n         mapping = aes(x = residuals)) + \n  geom_density() +\n  theme_bw() +\n  labs(x = \"Residual from Linear Regression Model\")\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#logistic-regression-1",
    "href": "slides/12-logistic.html#logistic-regression-1",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIdea: Instead of predicting 0 or 1, try to predict the probability of cancer.\n\n\nProblem: We don’t observe probabilities before diagnosis; we only know if that person ended up with cancer or not.\nSolution: (Fancy statistics and math.)\nWhy is it called Logistic Regression?\nBecause the “fancy math” uses a logistic function in it.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#logistic-regression-2",
    "href": "slides/12-logistic.html#logistic-regression-2",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nWhat you need to know:\n\nIt’s used for binary classification problems.\nThe predicted values are the “log-odds” of having cancer, i.e.\n\n\\[\\text{log-odds} = \\log \\left(\\frac{p}{1-p}\\right)\\]\n\n\n\n\nWe are more interested in the predicted probabilities.\nAs with KNN, we predict categories by choosing a threshold.\nBy default if \\(p &gt; 0.5\\) -&gt; we predict cancer",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#logistic-regression-in-sklearn",
    "href": "slides/12-logistic.html#logistic-regression-in-sklearn",
    "title": "Logistic Regression",
    "section": "Logistic Regression in sklearn",
    "text": "Logistic Regression in sklearn\n\nfrom sklearn.linear_model import LogisticRegression\n\npipeline = make_pipeline(\n  LogisticRegression(penalty = None)\n  )\n\npipeline.fit(X = df[[\"I0\", \"PA500\"]], \n             y = df['Cancerous']\n             );",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#logistic-regression-in-sklearn-1",
    "href": "slides/12-logistic.html#logistic-regression-in-sklearn-1",
    "title": "Logistic Regression",
    "section": "Logistic Regression in sklearn",
    "text": "Logistic Regression in sklearn\n\npred_cancer = pipeline.predict(df[[\"I0\", \"PA500\"]])\npred_cancer\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#confusion-matrix",
    "href": "slides/12-logistic.html#confusion-matrix",
    "title": "Logistic Regression",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\n\npd.DataFrame(\n  confusion_matrix(df['Cancerous'], pred_cancer), \n  columns = pipeline.classes_, \n  index = pipeline.classes_)\n\n\n    0   1\n0  38  14\n1   3  51\n\n\n\nCalculate the precision for predicting cancer.\nCalculate the recall for predicting cancer.\nCalculate the precision for predicting non-cancer.\nCalculate the recall for predicting non-cancer.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#threshold",
    "href": "slides/12-logistic.html#threshold",
    "title": "Logistic Regression",
    "section": "Threshold",
    "text": "Threshold\nWhat if we had used different cutoffs besides \\(p &gt; 0.5\\)?\n\nprob_cancer = pipeline.predict_proba(df[[\"I0\", \"PA500\"]])\nprob_cancer.round(2)[1:10]\n\narray([[0.11, 0.89],\n       [0.19, 0.81],\n       [0.11, 0.89],\n       [0.16, 0.84],\n       [0.27, 0.73],\n       [0.22, 0.78],\n       [0.2 , 0.8 ],\n       [0.18, 0.82],\n       [0.15, 0.85]])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#higher-threshold",
    "href": "slides/12-logistic.html#higher-threshold",
    "title": "Logistic Regression",
    "section": "Higher Threshold",
    "text": "Higher Threshold\nWhat we used \\(p &gt; 0.7\\)?\n\nprob_cancer = pipeline.predict_proba(df[[\"I0\", \"PA500\"]])\n\npred_cancer_70 = prob_cancer[:, 1] &gt; .7\npred_cancer_70[1:10]\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#higher-threshold-1",
    "href": "slides/12-logistic.html#higher-threshold-1",
    "title": "Logistic Regression",
    "section": "Higher Threshold",
    "text": "Higher Threshold\nWhat we used \\(p &gt; 0.7\\)?\n\n\nCode\nconf_mat = confusion_matrix(df['Cancerous'], pred_cancer_70)\npd.DataFrame(conf_mat, \n             columns = pipeline.classes_, \n             index = pipeline.classes_)\n\n\n    0   1\n0  41  11\n1  18  36\n\n\n\n\nprecision_1 = conf_mat[1,1] / conf_mat[:,1].sum()\nprecision_1\n\nnp.float64(0.7659574468085106)\n\nrecall_1 = conf_mat[1,1] / conf_mat[1, :].sum()\nrecall_1\n\nnp.float64(0.6666666666666666)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#lower-threshold",
    "href": "slides/12-logistic.html#lower-threshold",
    "title": "Logistic Regression",
    "section": "Lower Threshold",
    "text": "Lower Threshold\nWhat we used \\(p &gt; 0.2\\)?\n\nprob_cancer = pipeline.predict_proba(df[[\"I0\", \"PA500\"]])\npred_cancer_20 = prob_cancer[:,1] &gt; .2\npred_cancer_20[1:10]\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True])",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#lower-threshold-1",
    "href": "slides/12-logistic.html#lower-threshold-1",
    "title": "Logistic Regression",
    "section": "Lower Threshold",
    "text": "Lower Threshold\n\n\nCode\nconf_mat = confusion_matrix(df['Cancerous'], pred_cancer_20)\npd.DataFrame(conf_mat, \n             columns = pipeline.classes_, \n             index = pipeline.classes_)\n\n\n    0   1\n0  33  19\n1   0  54\n\n\n\n\nprecision_1 = conf_mat[1,1] / conf_mat[:,1].sum()\nprecision_1\n\nnp.float64(0.7397260273972602)\n\nrecall_1 = conf_mat[1,1] / conf_mat[1, :].sum()\nrecall_1\n\nnp.float64(1.0)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#precision-recall-curve",
    "href": "slides/12-logistic.html#precision-recall-curve",
    "title": "Logistic Regression",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(\n    df['Cancerous'], prob_cancer[:, 1])\n\ndf_pr = pd.DataFrame({\n  \"precision\": precision,\n  \"recall\": recall\n})",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#precision-recall-curve-1",
    "href": "slides/12-logistic.html#precision-recall-curve-1",
    "title": "Logistic Regression",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\n\n\nCode\n(\n  ggplot(data = df_pr, \n         mapping = aes(x = \"recall\", y = \"precision\")) +\n  geom_line() + \n  theme_bw() + \n  labs(x = \"Recall\", \n       y = \"Precision\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#activity",
    "href": "slides/12-logistic.html#activity",
    "title": "Logistic Regression",
    "section": "Activity",
    "text": "Activity\n\nSuppose you want to predict Cancer vs. No Cancer from breast tissue using a Logistic Regression. Should you use…\n\nJust I0 and PA500?\nJust DA and DP?\nI0, PA500, DA, and DP?\nor all predictors?\n\nUse cross-validation with F1 Score to decide!\nThen, fit your final model and report the confusion matrix.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#looking-at-coefficients",
    "href": "slides/12-logistic.html#looking-at-coefficients",
    "title": "Logistic Regression",
    "section": "Looking at Coefficients",
    "text": "Looking at Coefficients\n\n\npd.DataFrame({\n  \"Coefficients\": pipeline['logisticregression'].coef_[0],\n  \"Column\": [\"I0\", \"PA500\"]\n  })\n\n   Coefficients Column\n0     -0.003078     I0\n1     11.737963  PA500\n\n\n\n\n\n“For every unit of I0 higher, we predict 0.003 lower log-odds of cancer.”\n“For every unit of PA500 higher, we predict 11.73 higher log-odds of cancer.”",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#feature-importance",
    "href": "slides/12-logistic.html#feature-importance",
    "title": "Logistic Regression",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nDoes this mean that PA500 is more important than I0?\n\n\n\n\n\n\nCode\n(\n  ggplot(data = df, \n         mapping = aes(x = \"PA500\", \n                       group = \"Cancerous\", \n                       fill = \"Cancerous\")) + \n  geom_density(alpha = 0.5, show_legend = False) +\n  theme_bw()\n) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(data = df, \n         mapping = aes(x = \"I0\", \n                       group = \"Cancerous\", \n                       fill = \"Cancerous\")) + \n  geom_density(alpha = 0.5, show_legend = False) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#standardization",
    "href": "slides/12-logistic.html#standardization",
    "title": "Logistic Regression",
    "section": "Standardization",
    "text": "Standardization\n\n\nDoes this mean that PA500 is more important than I0?\nNot necessarily. They have different units and so the coefficients mean different things.\n“For every 1000 units of I0 higher, we predict 3.0 lower log-odds of cancer”\n“For every 0.1 unit of PA500 higher, we predict 1.1 higher log-odds of cancer.”\nWhat if we had standardized I0 and PA500?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#standardization-1",
    "href": "slides/12-logistic.html#standardization-1",
    "title": "Logistic Regression",
    "section": "Standardization",
    "text": "Standardization\n\n\nfrom sklearn.preprocessing import StandardScaler\n\npipeline2 = make_pipeline(\n  StandardScaler(),\n  LogisticRegression(penalty = None)\n  )\n\npipeline2 = pipeline2.fit(df[[\"I0\", \"PA500\"]], df['Cancerous'])\n\npd.DataFrame({\n  \"Coefficients\": pipeline2['logisticregression'].coef_[0],\n  \"Column\": [\"I0\", \"PA500\"]\n  })\n\n   Coefficients Column\n0     -2.309090     I0\n1      0.801477  PA500\n\n\n\n\n“For every standard deviation above the mean someone’s I0 is, we predict 2.3 lower log-odds of cancer”\n“For every standard deviation above the mean someone’s PA500 is, we predict 0.80 higher log-odds of cancer.”",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#standardization-do-you-need-it",
    "href": "slides/12-logistic.html#standardization-do-you-need-it",
    "title": "Logistic Regression",
    "section": "Standardization: Do you need it?",
    "text": "Standardization: Do you need it?\nBut - does this approach change our predictions?\n\nold_probs = pipeline.predict_proba(df[[\"I0\", \"PA500\"]])\nnew_probs = pipeline2.predict_proba(df[[\"I0\", \"PA500\"]])\n\npd.DataFrame({\n  \"without_stdize\": old_probs[:,1], \n  \"with_stdize\": new_probs[:,1]\n  }).head(10)\n\n   without_stdize  with_stdize\n0        0.736188     0.736226\n1        0.889803     0.889822\n2        0.813277     0.813319\n3        0.890781     0.890804\n4        0.842957     0.842978\n5        0.731660     0.731678\n6        0.775454     0.775463\n7        0.802116     0.802126\n8        0.816982     0.817014\n9        0.847675     0.847702",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#standardization-do-you-need-it-1",
    "href": "slides/12-logistic.html#standardization-do-you-need-it-1",
    "title": "Logistic Regression",
    "section": "Standardization: Do you need it?",
    "text": "Standardization: Do you need it?\n\n\n\nStandardizing will not change the predictions for Linear or Logistic Regression!\n\nThis is because the coefficients are chosen relative to the units of the predictors. (Unlike in KNN!)\n\nAdvantage of not standardizing: More interpretable coefficients\n\n“For each unit of…” instead of “For each sd above the mean…”\n\nAdvantage of standardizing: Compare relative importance of predictors\nIt’s up to you!\n\nDon’t use cross-validation to decide - you’ll get the same metrics for both!",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#activity-1",
    "href": "slides/12-logistic.html#activity-1",
    "title": "Logistic Regression",
    "section": "Activity",
    "text": "Activity\nFor your Logistic Regression using all predictors, which two were most important?\nHow would you interpret the coefficients?",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/12-logistic.html#takeaways-1",
    "href": "slides/12-logistic.html#takeaways-1",
    "title": "Logistic Regression",
    "section": "Takeaways",
    "text": "Takeaways\n\nTo fit a regression model (i.e., coefficients times predictors) to a categorical response, we use Logistic Regression.\nCoefficients are interpreted as “One unit increase in predictor is associated with a [something] increase in the log-odds of Category 1.”\nWe still use cross-validated metrics to decide between KNN and Logistic regression, and between different feature sets.\nWe still report confusion matrices and sometimes precision-recall curves of our final model.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 2 - Logistic Regression"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#what-to-expect",
    "href": "slides/07-text-data.html#what-to-expect",
    "title": "Bag-of-Words and TF-IDF",
    "section": "What to expect",
    "text": "What to expect\n\n\n\nContent\n\n\n\nEverything covered through the end of this week\n\nVisualizing numerical & categorical variables\nSummarizing numerical & categorical variables\nDistances between observations\nPreprocessing (scaling / one-hot-encoding) variables\n\n\n\n\n\n\n\n\nStructure\n\n\n\n80-minutes\n\nFirst part of class (8:10 - 9:30am; 12:10 - 1:30pm)\n\n\n\n\n\n\n\nGoogle Collab Notebook\n\n\n\n\n\n\nResources you can use:\n\nYour own Collab notebooks\nAny course materials\nOfficial Python documentation\n\n\n\n\n\n\n\nResources you can not use:\n\nOther humans\nGenerative AI for anything beyond text completion\nGoogle for anything except to reach Python documentation pages\n\n\n\n\n\n:::",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#data-for-exam-1",
    "href": "slides/07-text-data.html#data-for-exam-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Data for Exam 1",
    "text": "Data for Exam 1\n\nDataset on coffee attributes\n\nDerived from a Kaggle dataset (link to documentation)\nRaw data can be downloaded here (link to data)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#distances",
    "href": "slides/07-text-data.html#distances",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Distances",
    "text": "Distances\n\nWe measure similarity between observations by calculating distances.\nEuclidean distance: sum of squared differences, then square root\nManhattan distance: sum of absolute differences\nIn scikit-learn, use the pairwise_distances() function to get back a 2D numpy array of distances.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#scaling",
    "href": "slides/07-text-data.html#scaling",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Scaling",
    "text": "Scaling\n\nIt is important that all our features be on the same scale for distances to be meaningful.\nStandardize: Subtract the mean (of the column) and divide by the standard deviation (of the column).\nMinMax: Subtract the minimum value, divide by the range.\nIn scikit-learn, use the StandardScaler() or MinMaxScaler() functions.\nFollow the specify - fit - transform code structure.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-data",
    "href": "slides/07-text-data.html#text-data",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Data",
    "text": "Text Data\nA textual data set consists of multiple texts. Each text is called a document. The collection of texts is called a corpus.\nExample Corpus:\n\n\n\"I am Sam\\n\\nI am Sam\\nSam I...\"\n\"The sun did not shine.\\nIt was...\"\n\"Fox\\nSocks\\nBox\\nKnox\\n\\nKnox...\"\n\"Every Who\\nDown in Whoville\\n...\"\n\"UP PUP Pup is up.\\nCUP PUP...\"\n\"On the fifteenth of May, in the...\"\n\"Congratulations!\\nToday is your...\"\n\"One fish, two fish, red fish...\"",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#reading-text-data",
    "href": "slides/07-text-data.html#reading-text-data",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Reading Text Data",
    "text": "Reading Text Data\nDocuments are usually stored in different files.\n\nseuss_dir = \"http://dlsun.github.io/pods/data/drseuss/\"\nseuss_files = [\n    \"green_eggs_and_ham.txt\", \n    \"cat_in_the_hat.txt\",\n    \"fox_in_socks.txt\", \n    \"how_the_grinch_stole_christmas.txt\",\n    \"hop_on_pop.txt\", \n    \"horton_hears_a_who.txt\",\n    \"oh_the_places_youll_go.txt\", \n    \"one_fish_two_fish.txt\"]\n\n…so",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#we-have-to-read-them-in-one-by-one",
    "href": "slides/07-text-data.html#we-have-to-read-them-in-one-by-one",
    "title": "Bag-of-Words and TF-IDF",
    "section": "…we have to read them in one by one",
    "text": "…we have to read them in one by one\n\nimport requests\n\ndocs = {}\nfor filename in seuss_files:\n    response = requests.get(seuss_dir + filename)\n    docs[filename] = response.text\n\n\n\n\ndocs.keys()\n\ndict_keys(['green_eggs_and_ham.txt', 'cat_in_the_hat.txt', 'fox_in_socks.txt', 'how_the_grinch_stole_christmas.txt', 'hop_on_pop.txt', 'horton_hears_a_who.txt', 'oh_the_places_youll_go.txt', 'one_fish_two_fish.txt'])",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-data-1",
    "href": "slides/07-text-data.html#text-data-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Data",
    "text": "Text Data\n\ndocs.values()\n\ndict_values(['I am Sam\\n\\nI am Sam\\nSam I am\\n\\nThat Sam-I-am\\nThat Sam-I-am!\\nI do not like\\nthat Sam-I-am\\n\\nDo you like\\ngreen eggs and ham\\n\\nI do not like them,\\nSam-I-am.\\nI do not like\\ngreen eggs and ham.\\n\\nWould you like them\\nHere or there?\\n\\nI would not like them\\nhere or there.\\nI would not like them\\nanywhere.\\nI do not like\\ngreen eggs and ham.\\nI do not like them,\\nSam-I-am\\n\\nWould you like them\\nin a house?\\nWould you like them\\nwith a mouse?\\n\\nI do not like them\\nin a house.\\nI do not like them\\nwith a mouse.\\nI do not like them\\nhere or there.\\nI do not like them\\nanywhere.\\nI do not like green eggs and ham.\\nI do not like them, Sam-I-am.\\n\\n\\nWould you eat them\\nin a box?\\nWould you eat them\\nwith a fox?\\n\\nNot in a box.\\nNot with a fox.\\nNot in a house.\\nNot with a mouse.\\nI would not eat them here or there.\\nI would not eat them anywhere.\\nI would not eat green eggs and ham.\\nI do not like them, Sam-I-am.\\n\\nWould you? Could you?\\nin a car?\\nEat them! Eat them!\\nHere they are.\\n\\nI would not,\\ncould not,\\nin a car.\\n\\nYou may like them.\\nYou will see.\\nYou may like them\\nin a tree?\\n\\nI would not, could not in a tree.\\nNot in a car! You let me be.\\n\\nI do not like them in a box.\\nI do not like them with a fox\\nI do not like them in a house\\nI do mot like them with a mouse\\nI do not like them here or there.\\nI do not like them anywhere.\\nI do not like green eggs and ham.\\nI do not like them, Sam-I-am.\\n\\nA train! A train!\\nA train! A train!\\nCould you, would you\\non a train?\\n\\nNot on a train! Not in a tree!\\nNot in a car! Sam! Let me be!\\nI would not, could not, in a box.\\nI could not, would not, with a fox.\\nI will not eat them with a mouse\\nI will not eat them in a house.\\nI will not eat them here or there.\\nI will not eat them anywhere.\\nI do not like them, Sam-I-am.\\n\\n\\nSay!\\nIn the dark?\\nHere in the dark!\\nWould you, could you, in the dark?\\n\\nI would not, could not,\\nin the dark.\\n\\nWould you, could you,\\nin the rain?\\n\\nI would not, could not, in the rain.\\nNot in the dark. Not on a train,\\nNot in a car, Not in a tree.\\nI do not like them, Sam, you see.\\nNot in a house. Not in a box.\\nNot with a mouse. Not with a fox.\\nI will not eat them here or there.\\nI do not like them anywhere!\\n\\nYou do not like\\ngreen eggs and ham?\\n\\nI do not\\nlike them,\\nSam-I-am.\\n\\nCould you, would you,\\nwith a goat?\\n\\nI would not,\\ncould not.\\nwith a goat!\\n\\nWould you, could you,\\non a boat?\\n\\nI could not, would not, on a boat.\\nI will not, will not, with a goat.\\nI will not eat them in the rain.\\nI will not eat them on a train.\\nNot in the dark! Not in a tree!\\nNot in a car! You let me be!\\nI do not like them in a box.\\nI do not like them with a fox.\\nI will not eat them in a house.\\nI do not like them with a mouse.\\nI do not like them here or there.\\nI do not like them ANYWHERE!\\n\\nI do not like\\ngreen eggs\\nand ham!\\n\\nI do not like them,\\nSam-I-am.\\n\\nYou do not like them.\\nSO you say.\\nTry them! Try them!\\nAnd you may.\\nTry them and you may I say.\\n\\nSam!\\nIf you will let me be,\\nI will try them.\\nYou will see.\\n\\nSay!\\nI like green eggs and ham!\\nI do! I like them, Sam-I-am!\\nAnd I would eat them in a boat!\\nAnd I would eat them with a goat...\\nAnd I will eat them in the rain.\\nAnd in the dark. And on a train.\\nAnd in a car. And in a tree.\\nThey are so good so good you see!\\n\\nSo I will eat them in a box.\\nAnd I will eat them with a fox.\\nAnd I will eat them in a house.\\nAnd I will eat them with a mouse.\\nAnd I will eat them here and there.\\nSay! I will eat them ANYWHERE!\\n\\nI do so like\\ngreen eggs and ham!\\nThank you!\\nThank you,\\nSam-I-am\\n', 'The sun did not shine.\\nIt was too wet to play.\\nSo we sat in the house\\nAll that cold, cold, wet day.\\n\\nI sat there with Sally.\\nWe sat there, we two.\\nAnd I said, \"How I wish\\nWe had something to do!\"\\n\\nToo wet to go out\\nAnd too cold to play ball.\\nSo we sat in the house.\\nWe did nothing at all.\\n\\nSo all we could do was to\\n\\nSit!\\nSit!\\nSit!\\nSit!\\n\\nAnd we did not like it.\\nNot one little bit.\\n\\nBUMP!\\n\\nAnd then\\nsomething went BUMP!\\nHow that bump made us jump!\\n\\nWe looked!\\nThen we saw him step in on the mat!\\nWe looked!\\nAnd we saw him!\\nThe Cat in the Hat!\\nAnd he said to us,\\n\"Why do you sit there like that?\"\\n\"I know it is wet\\nAnd the sun is not sunny.\\nBut we can have\\nLots of good fun that is funny!\"\\n\\n\"I know some good games we could play,\"\\nSaid the cat.\\n\"I know some new tricks,\"\\nSaid the Cat in the Hat.\\n\"A lot of good tricks.\\nI will show them to you.\\nYour mother\\nWill not mind at all if I do.\"\\n\\nThen Sally and I\\nDid not know what to say.\\nOur mother was out of the house\\nFor the day.\\n\\nBut our fish said, \"No! No!\\nMake that cat go away!\\nTell that Cat in the Hat\\nYou do NOT want to play.\\nHe should not be here.\\nHe should not be about.\\nHe should not be here\\nWhen your mother is out!\"\\n\\n\"Now! Now! Have no fear.\\nHave no fear!\" said the cat.\\n\"My tricks are not bad,\"\\nSaid the Cat in the Hat.\\n\"Why, we can have\\nLots of good fun, if you wish,\\nwith a game that I call\\nUP-UP-UP with a fish!\"\\n\\n\"Put me down!\" said the fish.\\n\"This is no fun at all!\\nPut me down!\" said the fish.\\n\"I do NOT wish to fall!\"\\n\\n\"Have no fear!\" said the cat.\\n\"I will not let you fall.\\nI will hold you up high\\nAs I stand on a ball.\\nWith a book on one hand!\\nAnd a cup on my hat!\\nBut that is not ALL I can do!\"\\nSaid the cat...\\n\\n\"Look at me!\\nLook at me now!\" said the cat.\\n\"With a cup and a cake\\nOn the top of my hat!\\nI can hold up TWO books!\\nI can hold up the fish!\\nAnd a litte toy ship!\\nAnd some milk on a dish!\\nAnd look!\\nI can hop up and down on the ball!\\nBut that is not all!\\nOh, no.\\nThat is not all...\\n\\n\"Look at me!\\nLook at me!\\nLook at me NOW!\\nIt is fun to have fun\\nBut you have to know how.\\nI can hold up the cup\\nAnd the milk and the cake!\\nI can hold up these books!\\nAnd the fish on a rake!\\nI can hold the toy ship\\nAnd a little toy man!\\nAnd look! With my tail\\nI can hold a red fan!\\nI can fan with the fan\\nAs I hop on the ball!\\nBut that is not all.\\nOh, no.\\nThat is not all....\"\\n\\nThat is what the cat said...\\nThen he fell on his head!\\nHe came down with a bump\\nFrom up there on the ball.\\nAnd Sally and I,\\nWe saw ALL the things fall!\\n\\nAnd our fish came down, too.\\nHe fell into a pot!\\nHe said, \"Do I like this?\"\\nOh, no! I do not.\\nThis is not a good game,\"\\nSaid our fish as he lit.\\n\"No, I do not like it,\\nNot one little bit!\"\\n\\n\"Now look what you did!\"\\nSaid the fish to the cat.\\n\"Now look at this house!\\nLook at this! Look at that!\\nYou sank our toy ship,\\nSank it deep in the cake.\\nYou shook up our house\\nAnd you bent our new rake.\\nYou SHOULD NOT be here\\nWhen our mother is not.\\nYou get out of this house!\"\\nSaid the fish in the pot.\\n\\n\"But I like to be here.\\nOh, I like it a lot!\"\\nSaid the Cat in the Hat\\nTo the fish in the pot.\\n\"I will NOT go away.\\nI do NOT wish to go!\\nAnd so,\" said the Cat in the Hat,\\n\\n\"So\\nso\\nso...\\n\\nI will show you\\nAnother good game that I know!\"\\nAnd then he ran out.\\nAnd, then, fast as a fox,\\nThe Cat in the Hat\\nCame back in with a box.\\nA big red wood box.\\nIt was shut with a hook.\\n\"Now look at this trick,\"\\nSaid the cat.\\n\"Take a look!\"\\n\\nThen he got up on top\\nWith a tip of his hat.\\n\"I call this game FUN-IN-A-BOX,\"\\nSaid the cat.\\n\"In this box are two things\\nI will show to you now.\\nYou will like these two things,\"\\nSaid the cat with a bow.\\n\\n\"I will pick up the hook.\\nYou will see something new.\\nTwo things. And I call them\\nThing One and Thing Two.\\nThese Things will not bite you.\\nThey want to have fun.\"\\nThen, out of the box\\nCame Thing Two and Thing One!\\nAnd they ran to us fast.\\nThey said, \"How do you do?\\nWould you like to shake hands\\nWith Thing One and Thing Two?\"\\n\\nAnd Sally and I\\nDid not know what to do.\\nSo we had to shake hands\\nWith Thing One and Thing Two.\\nWe shook their two hands.\\nBut our fish said, \"No! No!\\nThose Things should not be\\nIn this house! Make them go!\\n\"They should not be here\\nWhen your mother is not!\\nPut them out! Put them out!\"\\nSaid the fish in the pot.\\n\\n\"Have no fear, little fish,\"\\nSaid the Cat in the Hat.\\n\"These Things are good Things.\"\\nAnd he gave them a pat.\\n\"They are tame. Oh, so tame!\\nThey have come here to play.\\nThey will give you some fun\\nOn this wet, wet, wet day.\"\\n\\n\"Now, here is a game that they like,\"\\nSaid the cat.\\n\"They like to fly kites,\"\\nSaid the Cat in the Hat\\n\\n\"No! Not in the house!\"\\nSaid the fish in the pot.\\n\"They should not fly kites\\nIn a house! They should not.\\nOh, the things they will bump!\\nOh, the things they will hit!\\nOh, I do not like it!\\nNot one little bit!\" Then Sally and I\\nSaw them run down the hall.\\nWe saw those two Things\\nBump their kites on the wall!\\nBump! Thump! Thump! Bump!\\nDown the wall in the hall.\\n\\nThing Two and Thing One!\\nThey ran up! They ran down!\\nOn the string of one kite\\nWe saw Mother\\'s new gown!\\nHer gown with the dots\\nThat are pink, white and red.\\nThen we saw one kite bump\\nOn the head of her bed!\\n\\nThen those Things ran about\\nWith big bumps, jumps and kicks\\nAnd with hops and big thumps\\nAnd all kinds of bad tricks.\\nAnd I said,\\n\"I do NOT like the way that they play\\nIf Mother could see this,\\nOh, what would she say!\"\\n\\nThen our fish said, \"Look! Look!\"\\nAnd our fish shook with fear.\\n\"Your mother is on her way home!\\nDo you hear?\\nOh, what will she do to us?\\nWhat will she say?\\nOh, she will not like it\\nTo find us this way!\"\\n\\n\"So, DO something! Fast!\" said the fish.\\n\"Do you hear!\\nI saw her. Your mother!\\nYour mother is near!\\nSo, as fast as you can,\\nThink of something to do!\\nYou will have to get rid of\\nThing One and Thing Two!\"\\n\\nSo, as fast as I could,\\nI went after my net.\\nAnd I said, \"With my net\\nI can get them I bet.\\nI bet, with my net,\\nI can get those Things yet!\"\\n\\nThen I let down my net.\\nIt came down with a PLOP!\\nAnd I had them! At last!\\nThoe two Things had to stop.\\nThen I said to the cat,\\n\"Now you do as I say.\\nYou pack up those Things\\nAnd you take them away!\"\\n\\n\"Oh dear!\" said the cat,\\n\"You did not like our game...\\nOh dear.\\n\\nWhat a shame!\\nWhat a shame!\\nWhat a shame!\"\\n\\nThen he shut up the Things\\nIn the box with the hook.\\nAnd the cat went away\\nWith a sad kind of look.\\n\\n\"That is good,\" said the fish.\\n\"He has gone away. Yes.\\nBut your mother will come.\\nShe will find this big mess!\\nAnd this mess is so big\\nAnd so deep and so tall,\\nWe ca not pick it up.\\nThere is no way at all!\"\\n\\nAnd THEN!\\nWho was back in the house?\\nWhy, the cat!\\n\"Have no fear of this mess,\"\\nSaid the Cat in the Hat.\\n\"I always pick up all my playthings\\nAnd so...\\nI will show you another\\nGood trick that I know!\"\\n\\nThen we saw him pick up\\nAll the things that were down.\\nHe picked up the cake,\\nAnd the rake, and the gown,\\nAnd the milk, and the strings,\\nAnd the books, and the dish,\\nAnd the fan, and the cup,\\nAnd the ship, and the fish.\\nAnd he put them away.\\nThen he said, \"That is that.\"\\nAnd then he was gone\\nWith a tip of his hat.\\n\\nThen our mother came in\\nAnd she said to us two,\\n\"Did you have any fun?\\nTell me. What did you do?\"\\n\\nAnd Sally and I did not know\\nWhat to say.\\nShould we tell her\\nThe things that went on there that day?\\n\\nShould we tell her about it?\\nNow, what SHOULD we do?\\nWell...\\nWhat would YOU do\\nIf your mother asked YOU?\\n', \"Fox\\nSocks\\nBox\\nKnox\\n\\nKnox in box.\\nFox in socks.\\n\\nKnox on fox in socks in box.\\n\\nSocks on Knox and Knox in box.\\n\\nFox in socks on box on Knox.\\n\\nChicks with bricks come.\\nChicks with blocks come.\\nChicks with bricks and blocks and clocks come.\\n\\nLook, sir.  Look, sir.  Mr. Knox, sir.\\nLet's do tricks with bricks and blocks, sir.\\nLet's do tricks with chicks and clocks, sir.\\n\\nFirst, I'll make a quick trick brick stack.\\nThen I'll make a quick trick block stack.\\n\\nYou can make a quick trick chick stack.\\nYou can make a quick trick clock stack.\\n\\nAnd here's a new trick, Mr. Knox....\\nSocks on chicks and chicks on fox.\\nFox on clocks on bricks and blocks.\\nBricks and blocks on Knox on box.\\n\\nNow we come to ticks and tocks, sir.\\nTry to say this Mr. Knox, sir....\\n\\nClocks on fox tick.\\nClocks on Knox tock.\\nSix sick bricks tick.\\nSix sick chicks tock.\\n\\nPlease, sir.  I don't like this trick, sir.\\nMy tongue isn't quick or slick, sir.\\nI get all those ticks and clocks, sir, \\nmixed up with the chicks and tocks, sir.\\nI can't do it, Mr. Fox, sir.\\n\\nI'm so sorry, Mr. Knox, sir.\\n\\nHere's an easy game to play.\\nHere's an easy thing to say....\\n\\nNew socks.\\nTwo socks.\\nWhose socks?\\nSue's socks.\\n\\nWho sews whose socks?\\nSue sews Sue's socks.\\n\\nWho sees who sew whose new socks, sir?\\nYou see Sue sew Sue's new socks, sir.\\n\\nThat's not easy, Mr. Fox, sir.\\n\\nWho comes? ...\\nCrow comes.\\nSlow Joe Crow comes.\\n\\nWho sews crow's clothes?\\nSue sews crow's clothes.\\nSlow Joe Crow sews whose clothes?\\nSue's clothes.\\n\\nSue sews socks of fox in socks now.\\n\\nSlow Joe Crow sews Knox in box now.\\n\\nSue sews rose on Slow Joe Crow's clothes.\\nFox sews hose on Slow Joe Crow's nose.\\n\\nHose goes.\\nRose grows.\\nNose hose goes some.\\nCrow's rose grows some.\\n\\nMr. Fox!\\nI hate this game, sir.\\nThis game makes my tongue quite lame, sir.\\n\\nMr. Knox, sir, what a shame, sir.\\n\\nWe'll find something new to do now.\\nHere is lots of new blue goo now.\\nNew goo.  Blue goo.\\nGooey.  Gooey.\\nBlue goo.  New goo.\\nGluey. Gluey.\\n\\nGooey goo for chewy chewing!\\nThat's what that Goo-Goose is doing.\\nDo you choose to chew goo, too, sir?\\nIf, sir, you, sir, choose to chew, sir, \\nwith the Goo-Goose, chew, sir.\\nDo, sir.\\n\\nMr. Fox, sir, \\nI won't do it.  \\nI can't say.  \\nI won't chew it.\\n\\nVery well, sir.\\nStep this way.\\nWe'll find another game to play.\\n\\nBim comes.\\nBen comes.\\nBim brings Ben broom.\\nBen brings Bim broom.\\n\\nBen bends Bim's broom.\\nBim bends Ben's broom.\\nBim's bends.\\nBen's bends.\\nBen's bent broom breaks.\\nBim's bent broom breaks.\\n\\nBen's band.  Bim's band.\\nBig bands.  Pig bands.\\n\\nBim and Ben lead bands with brooms.\\nBen's band bangs and Bim's band booms.\\n\\nPig band!  Boom band!\\nBig band!  Broom band!\\nMy poor mouth can't say that.  No, sir.\\nMy poor mouth is much too slow, sir.\\n\\nWell then... bring your mouth this way.\\nI'll find it something it can say.\\n\\nLuke Luck likes lakes.\\nLuke's duck likes lakes.\\nLuke Luck licks lakes.\\nLuck's duck licks lakes.\\n\\nDuck takes licks in lakes Luke Luck likes.\\nLuke Luck takes licks in lakes duck likes.\\n\\nI can't blab such blibber blubber!\\nMy tongue isn't make of rubber.\\n\\nMr. Knox.  Now come now.  Come now.\\nYou don't have to be so dumb now....\\n\\nTry to say this, Mr. Knox, please....\\n\\nThrough three cheese trees three free fleas flew.\\nWhile these fleas flew, freezy breeze blew.\\nFreezy breeze made these three trees freeze.\\nFreezy trees made these trees' cheese freeze.\\nThat's what made these three free fleas sneeze.\\n\\nStop it!  Stop it!\\nThat's enough, sir.\\nI can't say such silly stuff, sir.\\n\\nVery well, then, Mr. Knox, sir.\\n\\nLet's have a little talk about tweetle beetles....\\n\\nWhat do you know about tweetle beetles?  Well...\\n\\nWhen tweetle beetles fight, \\nit's called a tweetle beetle battle.\\n\\nAnd when they battle in a puddle, \\nit's a tweetle beetle puddle battle.\\n\\nAND when tweetle beetles battle with paddles in a puddle, \\nthey call it a tweetle beetle puddle paddle battle.\\n\\nAND...\\n\\nWhen beetles battle beetles in a puddle paddle battle \\nand the beetle battle puddle is a puddle in a bottle...\\n...they call this a tweetle beetle bottle puddle paddle battle muddle.\\n\\nAND...\\n\\nWhen beetles fight these battles in a bottle with their paddles \\nand the bottle's on a poodle and the poodle's eating noodles...\\n...they call this a muddle puddle tweetle poodle beetle noodle \\nbottle paddle battle.\\n\\nAND...\\n\\nNow wait a minute, Mr. Socks Fox!\\n\\nWhen a fox is in the bottle where the tweetle beetles battle \\nwith their paddles in a puddle on a noodle-eating poodle, \\nTHIS is what they call...\\n\\n...a tweetle beetle noodle poodle bottled paddled \\nmuddled duddled fuddled wuddled fox in socks, sir!\\n\\nFox in socks, our game is done, sir.\\nThank you for a lot of fun, sir.\\n\", 'Every Who\\nDown in Whoville\\nLiked Christmas a lot...\\n\\nBut the Grinch,\\nWho lived just north of Whoville,\\nDid NOT!\\n\\nThe Grinch hated Christmas! The whole Christmas season!\\nNow, please don\\'t ask why. No one quite knows the reason.\\nIt could be his head wasn\\'t screwed on just right.\\nIt could be, perhaps, that his shoes were too tight.\\nBut I think that the most likely reason of all,\\nMay have been that his heart was two sizes too small.\\n\\nWhatever the reason,\\nHis heart or his shoes,\\nHe stood there on Christmas Eve, hating the Whos,\\nStaring down from his cave with a sour, Grinchy frown,\\nAt the warm lighted windows below in their town.\\nFor he knew every Who down in Whoville beneath,\\nWas busy now, hanging a mistletoe wreath.\\n\\n\"And they\\'re hanging their stockings!\" he snarled with a sneer,\\n\"Tomorrow is Christmas! It\\'s practically here!\"\\nThen he growled, with his Grinch fingers nervously drumming,\\n\"I MUST find some way to stop Christmas from coming!\"\\nFor Tomorrow, he knew, all the Who girls and boys,\\nWould wake bright and early. They\\'d rush for their toys!\\nAnd then! Oh, the noise! Oh, the Noise!\\nNoise! Noise! Noise!\\nThat\\'s one thing he hated! The NOISE!\\nNOISE! NOISE! NOISE!\\n\\nThen the Whos, young and old, would sit down to a feast.\\nAnd they\\'d feast! And they\\'d feast! And they\\'d FEAST!\\nFEAST! FEAST! FEAST!\\nThey would feast on Who-pudding, and rare Who-roast beast.\\nWhich was something the Grinch couldn\\'t stand in the least!\\n\\nAnd THEN They\\'d do something He liked least of all!\\nEvery Who down in Whoville, the tall and the small,\\nWould stand close together, with Christmas bells ringing.\\nThey\\'d stand hand-in-hand. And the Whos would start singing!\\nThey\\'d sing! And they\\'d sing! And they\\'d SING!\\nSING! SING! SING!\\n\\nAnd the more the Grinch thought of this Who-Christmas-Sing,\\nThe more the Grinch thought, \"I must stop this whole thing!\"\\n\"Why, for fifty-three years I\\'ve put up with it now!\"\\n\"I MUST stop this Christmas from coming! But HOW?\"\\n\\nThen he got an idea! An awful idea!\\nTHE GRINCH GOT A WONDERFUL, AWFUL IDEA!\\n\\n\"I know just what to do!\" The Grinch laughed in his throat.\\nAnd he made a quick Santy Claus hat and a coat.\\nAnd he chuckled, and clucked, \"What a great Grinchy trick!\"\\n\"With this coat and this hat, I look just like Saint Nick!\"\\n\\n\"All I need is a reindeer...\"\\nThe Grinch looked around.\\nBut, since reindeer are scarce, there was none to be found.\\nDid that stop the old Grinch?\\nNo! The Grinch simply said,\\n\"If I can\\'t find a reindeer, I\\'ll make one instead!\"\\nSo he called his dog, Max. Then he took some red thread,\\nAnd he tied a big horn on the top of his head.\\n\\nTHEN\\nHe loaded some bags\\nAnd some old empty sacks,\\nOn a ramshackle sleigh\\nAnd he hitched up old Max.\\n\\nThen the Grinch said, \"Giddap!\"\\nAnd the sleigh started down,\\nToward the homes where the Whos\\nLay asnooze in their town.\\n\\nAll their windows were dark. Quiet snow filled the air.\\nAll the Whos were all dreaming sweet dreams without care.\\nWhen he came to the first little house on the square.\\n\"This is stop number one,\" the old Grinchy Claus hissed,\\nAnd he climbed to the roof, empty bags in his fist.\\n\\nThen he slid down the chimney. A rather tight pinch.\\nBut, if Santa could do it, then so could the Grinch.\\nHe got stuck only once, for a moment or two.\\nThen he stuck his head out of the fireplace flue.\\nWhere the little Who stockings all hung in a row.\\n\"These stockings,\" he grinned, \"are the first things to go!\"\\n\\nThen he slithered and slunk, with a smile most unpleasant,\\nAround the whole room, and he took every present!\\nPop guns! And bicycles! Roller skates! Drums!\\nCheckerboards! Tricycles! Popcorn! And plums!\\nAnd he stuffed them in bags. Then the Grinch, very nimbly,\\nStuffed all the bags, one by one, up the chimney!\\n\\nThen he slunk to the icebox. He took the Whos\\' feast!\\nHe took the Who-pudding! He took the roast beast!\\nHe cleaned out that icebox as quick as a flash.\\nWhy, that Grinch even took their last can of Who-hash!\\n\\nThen he stuffed all the food up the chimney with glee.\\n\"And NOW!\" grinned the Grinch, \"I will stuff up the tree!\"\\n\\nAnd the Grinch grabbed the tree, and he started to shove,\\nWhen he heard a small sound like the coo of a dove.\\nHe turned around fast, and he saw a small Who!\\nLittle Cindy-Lou Who, who was not more than two.\\n\\nThe Grinch had been caught by this tiny Who daughter,\\nWho\\'d got out of bed for a cup of cold water.\\nShe stared at the Grinch and said, \"Santy Claus, why,\\n\"Why are you taking our Christmas tree? WHY?\"\\n\\nBut, you know, that old Grinch was so smart and so slick,\\nHe thought up a lie, and he thought it up quick!\\n\"Why, my sweet little tot,\" the fake Santy Claus lied,\\n\"There\\'s a light on this tree that won\\'t light on one side.\"\\n\"So I\\'m taking it home to my workshop, my dear.\"\\n\"I\\'ll fix it up there. Then I\\'ll bring it back here.\"\\n\\nAnd his fib fooled the child. Then he patted her head,\\nAnd he got her a drink and he sent her to bed.\\nAnd when Cindy-Lou Who went to bed with her cup,\\nHE went to the chimney and stuffed the tree up!\\n\\nThen the last thing he took\\nWas the log for their fire!\\nThen he went up the chimney, himself, the old liar.\\nOn their walls he left nothing but hooks and some wire.\\n\\nAnd the one speck of food\\nThat he left in the house,\\nWas a crumb that was even too small for a mouse.\\n\\nThen\\nHe did the same thing To the other Whos\\' houses\\n\\nLeaving crumbs\\nMuch too small\\nFor the other Whos\\' mouses!\\n\\nIt was quarter past dawn...\\nAll the Whos, still a-bed,\\nAll the Whos, still asnooze\\nWhen he packed up his sled,\\nPacked it up with their presents! The ribbons! The wrappings!\\nThe tags! And the tinsel! The trimmings! The trappings!\\n\\nThree thousand feet up! Up the side of Mt. Crumpit,\\nHe rode with his load to the tiptop to dump it!\\n\"Pooh-pooh to the Whos!\" he was grinchishly humming.\\n\"They\\'re finding out now that no Christmas is coming!\"\\n\"They\\'re just waking up! I know just what they\\'ll do!\"\\n\"Their mouths will hang open a minute or two,\\nThen the Whos down in Whoville will all cry Boo-Hoo!\"\\n\\n\"That\\'s a noise,\" grinned the Grinch,\\n\"That I simply MUST hear!\"\\nSo he paused. And the Grinch put his hand to his ear.\\nAnd he did hear a sound rising over the snow.\\nIt started in low. Then it started to grow.\\n\\nBut the sound wasn\\'t sad! Why, this sound sounded merry!\\nIt couldn\\'t be so! But it WAS merry! VERY!\\nHe stared down at Whoville! The Grinch popped his eyes!\\nThen he shook! What he saw was a shocking surprise!\\n\\nEvery Who down in Whoville, the tall and the small,\\nWas singing! Without any presents at all!\\nHe HADN\\'T stopped Christmas from coming!\\nIT CAME!\\nSomehow or other, it came just the same!\\n\\nAnd the Grinch, with his grinch-feet ice-cold in the snow,\\nStood puzzling and puzzling: \"How could it be so?\"\\n\"It came with out ribbons! It came without tags!\"\\n\"It came without packages, boxes or bags!\"\\nAnd he puzzled three hours, till his puzzler was sore.\\nThen the Grinch thought of something he hadn\\'t before!\\n\"Maybe Christmas,\" he thought, \"doesn\\'t come from a store.\"\\n\"Maybe Christmas...perhaps...means a little bit more!\"\\n\\nAnd what happened then?\\nWell...in Whoville they say,\\nThat the Grinch\\'s small heart\\nGrew three sizes that day!\\nAnd the minute his heart didn\\'t feel quite so tight,\\nHe whizzed with his load through the bright morning light,\\nAnd he brought back the toys! And the food for the feast!\\nAnd he, HE HIMSELF! The Grinch carved the roast beast!\\n', 'UP PUP Pup is up.\\nCUP PUP Pup in cup.\\nPUP CUP Cup on pup.\\nMOUSE HOUSE Mouse on house.\\nHOUSE MOUSE House on mouse.\\nALL TALL We all are tall.\\nALL SMALL We all are small.\\nALL BALL We all play ball.\\nBALL WALL Up on a wall.\\nALL FALL Fall off the wall.\\nDAY PLAY We play all day.\\nNIGHT FIGHT We fight all night.HE ME He is after me.\\nHIM JIM Jim is after him.\\nSEE BEE We see a bee.\\nSEE BEE THREE Now we see three.\\nTHREE TREE Three fish in a tree.\\nFish in a tree? How can that be?\\nRED RED They call me Red.\\nRED BED I am in bed.\\nRED NED TED and ED in BED\\nPAT PAT they call him Pat.\\nPAT SAT Pat sat on hat.\\nPAT CAT Pat sat on cat.\\nPAT BAT Pat sat on bat.\\nNO PAT NO Don’t sit on that.\\nSAD DAD BAD HAD Dad is sad.\\nVery, very sad.\\nHe had a bad day. What a day Dad had!\\nTHING THING What is that thing?\\nTHING SING That thing can sing!\\nSONG LONG A long, long song.\\nGood-by, Thing. You sing too long.\\nWALK WALK We like to walk.\\nWALK TALK We like to talk.\\nHOP POP We like to hop.\\nWe like to hop on top of Pop.\\nSTOP You must not hop on Pop.\\nMr. BROWN Mrs. BROWN\\nMr. Brown upside down.\\nPup up. Brown down.\\nPup is down. Where is Brown?\\nWHERE IS BROWN? THERE IS BROWN!\\nMr. Brown is out of town.\\nBACK BLACK Brown came back.\\nBrown came back with Mr. Black.\\nSNACK SNACK Eat a snack.\\nEat a snack with Brown and Black.\\nJUMP BUMP He jumped. He bumped.\\nFAST PAST He went past fast.\\nWENT TENT SENT He went into the tent.\\nI sent him out of the tent.\\nWET GET Two dogs get wet.\\nHELP YELP They yelp for help.\\nHILL WILL Will went up hill.\\nWILL HILL STILL Will is up hill still.\\nFATHER MOTHER SISTER BROTHER\\nThat one is my other brother.\\nMy brothers read a little bit.\\nLittle words like If and it.\\nMy father can read big words, too.\\nLike CONSTANTINOPLE and TIMBUKTU\\nSAY SAY What does this say?\\nseehemewe\\npatpuppop\\nhethreetreebee\\ntophopstop\\nAsk me tomorrow but not today.\\n', 'On the fifteenth of May, in the jungle of Nool,\\nIn the heat of the day, in the cool of the pool,\\nHe was splashing...enjoying the jungle\\'s great joys...\\nWhen Horton the elephant heard a small noise.\\n\\nSo Horton stopped splashing. He looked towards the sound.\\n\"That\\'s funny,\" thought Horton. \"There\\'s no one around.\"\\nThen he heard it again! Just a very faint yelp\\nAs if some tiny person were calling for help.\\n\"I\\'ll help you,\" said Horton. \"But who are you? Where?\"\\nHe looked and he looked. He could see nothing there\\nBut a small speck of dust blowing past though the air.\\n\\n\"I say!\" murmured Horton. \"I\\'ve never heard tell\\nOf a small speck of dust that is able to yell.\\nSo you know what I think?...Why, I think that there must\\nBe someone on top of that small speck of dust!\\nSome sort of a creature of very small size,\\ntoo small to be seen by an elephant\\'s eyes...\\n\\n\"...some poor little person who\\'s shaking with fear\\nThat he\\'ll blow in the pool! He has no way to steer!\\nI\\'ll just have to save him. Because, after all,\\nA person\\'s a person, no matter how small.\"\\n\\nSo, gently, and using the greatest of care,\\nThe elephant stretched his great trunk through the air,\\nAnd he lifted the dust speck and carried it over\\nAnd placed it down, safe, on a very soft clover.\\n\\n\"Humpf!\" humpfed a voice. Twas a sour Kangaroo.\\nAnd the young kangaroo in he pouch said \"Humpf!\" too\\n\"Why, that speck is as small as the head of a pin.\\nA person on that?...why, there never has been!\"\\n\\n\"Believe me,\" said Horton. \"I tell you sincerely,\\nMy ears are quite keen and I heard him quite clearly.\\nI know there\\'s a person down there. And, what\\'s more,\\nQuite likely there\\'s two. Even three. Even four.\\nQuite likely...\\n\\n\"...a family, for all that we know!\\nA family with children just starting to grow.\\nSo, please,\" Horton said, \"as a favour to me,\\nTry not to disturb them. Just let them be.\"\\n\\n\"I think you\\'re a fool!\" laughed the sour kangaroo\\nAnd the young kangaroo in her pouch said, \"Me, too!\\nYou\\'re the biggest blame fool in the jungle of Nool!\"\\nAnd the kangaroos plunged in the cool of the pool.\\n\"What terrible splashing!\" the elephant frowned.\\n\"I can\\'t let my very small persons get drowned!\\nI\\'ve got to protect them. I\\'m bigger than they.\"\\nSo he plucked up the clover and hustled away.\\n\\nThrough the high jungle tree tops, the news quickly spread:\\n\"He talks to a dust speck! He\\'s out of his head!\\nJust look at him walk with that speck on the flower!\"\\nAnd Horton walked, worrying, almost an hour.\\n\"Should I put this speck down?...\" Horton though with alarm.\\n\"If I do, these small persons may come to great harm.\\nI can\\'t put it down. And I won\\'t! After all\\nA person\\'s a person. No matter how small.\"\\n\\nThen Horton stopped walking.\\nThe speck-voice was talking!\\nThe voice was so faint he could just barely hear it.\\n\"Speak up, please,\" Said Horton. He put his ear near it.\\n\"My friend,\" came the voice, \"you\\'re a very fine friend.\\nYou\\'ve helped all us folks on this dust speck no end.\\nYou\\'ve saved all our houses, our ceilings and floors.\\nYou\\'ve saved all our churches and grocery stores.\"\\n\\n\"You mean...\" Horton gasped, \"you have buildings there, too?\"\\n\"Oh, yes,\" piped the voice. \"We most certainly do...\\n\"I know,\" called the voice, \"I\\'m too small to be seen\\nBut I\\'m Mayor of a town that is friendly and clean.\\nOur buildings, to you, would seem terribly small\\nBut to us, who aren\\'t big, they are wonderfully tall.\\nMy town is called Who-ville, for I am a Who\\nAnd we Whos are all thankful and greatful to you\"\\n\\nAnd Horton called back to the Mayor of the town,\\n\"You\\'re safe now. Don\\'t worry. I won\\'t let you down.\"\\n\\nBut, Just as he spoke to the Mayor of the speck,\\nThree big jungle monkeys climbed up Horton\\'s neck!\\nThe Wickersham Brothers came shouting, \"What rot!\\nThis elephants talking to Whos who are not!\\nThere aren\\'t any Whos! And they don\\'t have a Mayor!\\nAnd we\\'re going to stop all this nonsense! So there!\"\\n\\nThey snatched Horton\\'s clover! They carried it off\\nTo a black-bottomed eagle named Valad Vlad-I-koff,\\nA mighty strong eagle, of very swift wing,\\nAnd they said, \"Will you kindly get rid of this thing?\"\\nAnd, before the poor elephant could even speak,\\nThat eagle flew off with the flower in his beak.\\n\\nAll that late afternoon and far into the night\\nThat black-bottomed bird flapped his wings in fast flight,\\nWhile Horton chased after, with groans, over stones\\nThat tattered his toenails and battered his bones,\\nAnd begged, \"Please don\\'t harm all my little folks, who\\nHave as much right to live as us bigger folk do!\"\\n\\nBut far, far beyond him, that eagle kept flapping\\nAnd over his shoulder called back, \"Quit your yapping.\\nI\\'ll fly the night through. I\\'m a bird. I don\\'t mind it.\\nAnd I\\'ll hide this, tomorrow, where you\\'ll never find it!\"\\n\\nAnd at 6:56 the next morning he did it.\\nIt sure was a terrible place that he hid it.\\nHe let that small clover drop somewhere inside\\nOf a great patch of clovers a hundred miles wide!\\n\"Find THAT!\" sneered the bird. \"But I think you will fail.\"\\nAnd he left\\nWith a flip\\nOf his black-bottomed tail.\\n\\n\"I\\'ll find it!\" cried Horton. \"I\\'ll find it or bust!\\nI SHALL find my friends on my small speck of dust!\"\\nAnd clover, by clover, by clover with care\\nHe picked up and searched the, and called, \"Are you there?\"\\nBut clover, by clover, by clover he found\\nThat the one that he sought for was just not around.\\nAnd by noon poor old Horton, more dead than alive,\\nHad picked, searched, and piled up, nine thousand and five.\\n\\nThen, on through the afternoon, hour after hour...\\nTill he found them at last! On the three millionth flower!\\n\"My friends!\" cried the elephant. \"Tell me! Do tell!\\nAre you safe? Are you sound? Are you whole? Are you well?\"\\n\\nFrom down on the speck came the voice of the Mayor:\\n\"We\\'ve really had trouble! Much more than our share.\\nWhen that black-bottomed birdie let go and we dropped,\\nWe landed so hard that our clocks have all stopped.\\nOur tea pots are broken. Our rocking-chairs are smashed.\\nAnd our bicycle tires all blew up when we crashed.\\nSo, Horton, Please!\" pleaded that voice of the Mayor\\'s,\\n\"Will you stick by us Whos while we\\'re making repairs?\"\\n\\n\"Of course,\" Horton answered. \"Of course I will stick.\\nI\\'ll stick by you small folks though thin and though thick!\"\\n\\n\"Humpf!\" humpfed a voice!\\n\"For almost two days you\\'ve run wild and insisted\\nOn chatting with persons who\\'ve never existed.\\nSuch carryings-on in our peaceable jungle!\\nWe\\'ve had quite enough of your bellowing bungle!\\nAnd I\\'m here to state,\" snapped the big kangaroo,\\n\"That your silly nonsensical game is all through!\"\\nAnd the young kangaroo in her pouch said, \"Me, too!\"\\n\\n\"With the help of the Wickersham Brothers and dozens\\nOf Wickersham Uncles and Wickershams Cousins\\nAnd Wickersham In-Laws, whose help I\\'ve engaged,\\nYou\\'re going to be roped! And you\\'re going to be caged!\\nAnd, as for your dust speck...hah!\\nThat we shall boil\\nIn a hot steaming kettle of Beezle-Nut Oil!\"\\n\"Boil it?...\" gasped Horton!\\n\"Oh, that you can\\'t do!\\nIt\\'s all full of persons!\\nThey\\'ll prove it to you!\"\\n\\n\"Mr. Mayor! Mr. Mayor!\" Horton called. \"Mr. Mayor!\\nYou\\'ve got to prove that you really are there!\\nSo call a big meeting. Get everyone out.\\nMake every Who holler! Make every Who shout!\\nMake every Who scream! If you don\\'t, every Who\\nIs going to end up in a Beezle-Nut stew!\"\\n\\nAnd, down on the dust speck, the scared little Mayor\\nQuick called a big meeting in Who-ville Town Square.\\nAnd his people cried loudly. They cried out in fear:\\n\"We are here! We are here! We are here!\"\\n\\nThe elephant smiled: \"That was clear as a bell.\\nYou Kangaroos surely heard that very well.\"\\n\"All I heard,\" snapped the big kangaroo, \"Was the breeze,\\nAnd the faint sound of wind through the far-distant trees.\\nI heard no small voices. And you didn\\'t either.\"\\nAnd the you kangaroo in her pouch said, \"Me, neither.\"\\n\\n\"Grab him!\" they shouted. \"And cage the big dope!\\nLasso his stomach with ten miles of rope!\\nTie the knots tight so he\\'ll never shake lose!\\nThen dunk that dumb speck in the Beezle-Nut juice!\"\\n\\n\\n\\nHorton fought back with great vigor and vim\\nBut the Wickersham gang was too many for him.\\nThey beat him! They mauled him! They started to haul\\nHim into his cage! But he managed to call\\nTo the Mayor: \"Don\\'t give up! I believe in you all\\nA person\\'s a person, no matter how small!\\nAnd you very small persons will not have to die\\nIf you make yourselves heard! So come on, now, and TRY!\"\\n\\nThe Mayor grabbed a tom-tom. He started to smack it.\\nAnd, all over Who-ville, they whooped up a racked.\\nThey rattled tie kettles! They beat on brass pans,\\nOn garbage pail tops and old cranberry cans!\\nThey blew on bazooka and blasted great toots\\nOn clarinets, oom-pahs and boom-pahs and flutes!\\n\\nGreat gusts of loud racket rang high through the air.\\nThey rattled and shook the whole sky! And the Mayor\\nCalled up through the howling mad hullabaloo:\\n\"Hey Horton! Hows this? Is our sound coming through?\"\\n\\nAnd Horton called back, \"I can hear you just fine.\\nBut the kangaroos\\' ears aren\\'t as strong, quite, as mine.\\nThey don\\'t hear a thing! Are you sure all you boys\\nAre doing their best? Are they ALL making noise?\\nAre you sure every Who down in Who-ville is working?\\nQuick! Look through your town! Is there anyone shirking?\"\\n\\nThrough the town rushed the Mayor, From the east to the west.\\nBut everyone seemed to be doing his best.\\nEveryone seemed to be yapping or yipping!\\nEveryone seemed to be beeping or bipping!\\nBut it wasn\\'t enough, all this ruckus and roar!\\nHe HAD to find someone to help him make more.\\nHe raced through each building! He searched floor-to-floor!\\n\\nAnd, just as he felt he was getting nowhere,\\nAnd almost about to give up in despair,\\nHe suddenly burst through a door and that Mayor\\nDiscovered one shirker! Quite hidden away\\nIn the Fairfax Apartments (Apartment 12-J)\\nA very small, very small shirker named Jo-Jo\\nwas standing, just standing, and bouncing a Yo-Yo!\\nNot making a sound! Not a yipp! Not a chirp!\\nAnd the Mayor rushed inside and he grabbed the young twerp!\\n\\nAnd he climbed with the lad up the Eiffelberg Tower.\\n\"This,\" cried the Mayor, \"is your towns darkest hour!\\nThe time for all Whos who have blood that is red\\nTo come to the aid of their country!\" he said.\\n\"We\\'ve GOT to make noises in greater amounts!\\nSo, open your mouth, lad! For every voice counts!\"\\n\\nThus he spoke as he climbed. When they got to the top,\\nThe lad cleared his throat and he shouted out, \"YOPP!\"\\n\\nAnd that Yopp...\\nThat one small, extra Yopp put it over!\\nFinally, at last! From that speck on that clover\\nTheir voices were heard! They rang out clear and clean.\\nAnd the elephant smiled. \"Do you see what I mean?...\\nThey\\'ve proved they ARE persons, no matter how small.\\nAnd their whole world was saved by the smallest of All!\"\\n\\n\"How true! Yes, how true,\" said the big kangaroo.\\n\"And, from now on, you know what I\\'m planning to do?...\\nFrom now on, I\\'m going to protect them with you!\"\\nAnd the young kangaroo in her pouch said...\\n\"...ME, TOO!\"\\n\"From the sun in the summer. From rain when it\\'s fall-ish,\\nI\\'m going to protect them. No matter how small-ish!\"\\n', 'Congratulations!\\nToday is your day.\\nYou\\'re off to Great Places!\\nYou\\'re off and away!\\n\\nYou have brains in your head.\\nYou have feet in your shoes\\nYou can steer yourself\\nAny direction you choose.\\nYou\\'re on your own. And you know what you know.\\nAnd YOU are the guy who\\'ll decide where to go.\\n\\nYou\\'ll look up and down streets. Look \\'em over with care.\\nAbout some you will say, \"I don\\'t choose to go there.\"\\nWith your head full of brains and your shoes full of feet,\\nYou\\'re too smart to go down any not-so-good street.\\n\\nAnd you may not find any\\nYou\\'ll want to go down.\\nIn that case, of course,\\nYou\\'ll head straight out of town.\\n\\nIt\\'s opener there\\nIn the wide open air.\\n\\nOut there things can happen\\nAnd frequently do\\nTo people as brainy\\nAnd footsy as you.\\n\\nAnd when things start to happen,\\nDon\\'t worry. Don\\'t stew.\\nJust go right along.\\nYou\\'ll start happening too.\\n\\nOH!\\nTHE PLACES YOU\\'LL GO!\\n\\nYou\\'ll be on your way up!\\nYou\\'ll be seeing great sights!\\nYou\\'ll join the high fliers\\nWho soar to high heights.\\n\\nYou won\\'t lag behind, because you\\'ll have the speed.\\nYou\\'ll pass the whole gang and you\\'ll soon take the lead.\\nWherever you fly, you\\'ll be the best of the best.\\nWherever you go, you will top all the rest.\\n\\nExcept when you don\\' t\\nBecause, sometimes, you won\\'t.\\n\\nI\\'m sorry to say so\\nbut, sadly, it\\'s true\\nthat Bang-ups\\nand Hang-ups\\ncan happen to you.\\n\\nYou can get all hung up\\nin a prickle-ly perch.\\nAnd your gang will fly on.\\nYou\\'ll be left in a Lurch.\\n\\nYou\\'ll come down from the Lurch\\nwith an unpleasant bump.\\nAnd the chances are, then,\\nthat you\\'ll be in a Slump.\\n\\nAnd when you\\'re in a Slump,\\nyou\\'re not in for much fun.\\nUn-slumping yourself\\nis not easily done.\\n\\nYou will come to a place where the streets are not marked.\\nSome windows are lighted. But mostly they\\'re darked.\\nA place you could sprain both your elbow and chin!\\nDo you dare to stay out? Do you dare to go in?\\nHow much can you lose? How much can you win?\\n\\nAnd IF you go in, should you turn left or right...\\nor right-and-three-quarters? Or, maybe, not quite?\\nOr go around back and sneak in from behind?\\nSimple it\\'s not, I\\'m afraid you will find,\\nfor a mind-maker-upper to make up his mind.\\n\\nYou can get so confused\\nthat you\\'ll start in to race\\ndown long wiggled roads at a break-necking pace\\nand grind on for miles cross weirdish wild space,\\nheaded, I fear, toward a most useless place.\\nThe Waiting Place...\\n\\n...for people just waiting.\\nWaiting for a train to go\\nor a bus to come, or a plane to go\\nor the mail to come, or the rain to go\\nor the phone to ring, or the snow to snow\\nor the waiting around for a Yes or No\\nor waiting for their hair to grow.\\nEveryone is just waiting.\\n\\nWaiting for the fish to bite\\nor waiting for the wind to fly a kite\\nor waiting around for Friday night\\nor waiting, perhaps, for their Uncle Jake\\nor a pot to boil, or a Better Break\\nor a string of pearls, or a pair of pants\\nor a wig with curls, or Another Chance.\\nEveryone is just waiting.\\n\\nNO!\\nThat\\'s not for you!\\n\\nSomehow you\\'ll escape\\nall that waiting and staying\\nYou\\'ll find the bright places\\nwhere Boom Bands are playing.\\n\\nWith banner flip-flapping,\\nonce more you\\'ll ride high!\\nReady for anything under the sky.\\nReady because you\\'re that kind of a guy!\\n\\nOh, the places you\\'ll go! There is fun to be done!\\nThere are points to be scored. There are games to be won.\\nAnd the magical things you can do with that ball\\nwill make you the winning-est winner of all.\\nFame! You\\'ll be as famous as famous can be,\\nwith the whole wide world watching you win on TV.\\n\\nExcept when they don\\'t\\nBecause, sometimes they won\\'t.\\n\\nI\\'m afraid that some times\\nyou\\'ll play lonely games too.\\nGames you can\\'t win\\n\\'cause you\\'ll play against you.\\n\\nAll Alone!\\nWhether you like it or not,\\nAlone will be something\\nyou\\'ll be quite a lot.\\n\\nAnd when you\\'re alone, there\\'s a very good chance\\nyou\\'ll meet things that scare you right out of your pants.\\nThere are some, down the road between hither and yon,\\nthat can scare you so much you won\\'t want to go on.\\n\\nBut on you will go\\nthough the weather be foul.\\nOn you will go\\nthough your enemies prowl.\\nOn you will go\\nthough the Hakken-Kraks howl.\\nOnward up many\\na frightening creek,\\nthough your arms may get sore\\nand your sneakers may leak.\\n\\nOn and on you will hike,\\nAnd I know you\\'ll hike far\\nand face up to your problems\\nwhatever they are.\\n\\nYou\\'ll get mixed up, of course,\\nAs you already know.\\nYou\\'ll get mixed up\\nWith many strange birds as you go.\\nSo be sure when you step.\\nStep with care and great tact\\nAnd remember that Life\\'s\\nA Great Balancing Act.\\nJust never forget to be dexterous and deft.\\nAnd never mix up your right foot with your left.\\n\\nAnd will you succeed?\\nYes! You will, indeed!\\n(98 and 3/4 percent guaranteed.)\\n\\nKID, YOU\\'LL MOVE MOUNTAINS!\\n\\nSo...\\nBe your name Buxbaum or Bixby or Bray\\nOr Mordecai Ali Van Allen O\\'Shea,\\nYou\\'re off to Great Places!\\nToday is your day!\\nYour mountain is waiting.\\nSo...get on your way!\\n', 'One fish, two fish, red fish, blue fish,\\nBlack fish, blue fish, old fish, new fish.\\nThis one has a little car.\\nThis one has a little star.\\nSay! What a lot of fish there are.\\nYes. Some are red, and some are blue.\\nSome are old. And some are new.\\nSome are sad. And some are glad.\\nAnd some are very, very bad.\\nWhy are they sad and glad and bad?\\nI do not know, go ask your dad.\\nSome are thin. And some are fat.\\nThe fat one has a yellow hat.\\nFrom there to here,\\nfrom here to there,\\nfunny things are everywhere.\\n\\nHere are some who like to run.\\nThey run for fun in the hot, hot sun.\\nOh me! Oh my! Oh me! oh my!\\nWhat a lot of funny things go by.\\nSome have two feet and some have four.\\nSome have six feet and some have more.\\nWhere do they come from? I can\\'t say.\\nBut I bet they have come a long, long way.\\nWe see them come, we see them go.\\nSome are fast. And some are slow.\\nSome are high. And some are low.\\nNot one of them is like another.\\nDon\\'t ask us why, go ask your mother.\\n\\nSay! Look at his fingers!\\nOne, two, three...\\nHow many fingers do I see?\\nOne, two, three, four,\\nfive, six, seven, eight, nine, ten.\\nHe has eleven!\\nEleven! This is something new.\\nI wish I had eleven too!\\n\\nBump! Bump! Bump!\\nDid you ever ride a Wump?\\nWe have a Wump with just one hump.\\nBut we know a man called Mr. Gump.\\nMr. Gump has a seven hump Wump. So...\\nIf you like to go Bump! Bump!\\nJust jump on the hump of the Wump of Gump.\\n\\nWho am I? My name is Ned\\nI do not like my little bed.\\nThis is no good. This is not right.\\nMy feet stick out of bed all night.\\nAnd when I pull them in,\\nOh, dear!\\nMy head sticks out of bed up here!\\n\\nWe like our bike. It is made for three.\\nOur Mike sits up in back, you see.\\nWe like our Mike, and this is why:\\nMike does all the work when the hills get high.\\n\\nHello there, Ned. How do you do?\\nTell me, tell me what is new?\\nHow are things in your little bed?\\nWhat is new? Please tell me Ned.\\n\\nI do not like this bed at all.\\na lot of things have come to call.\\nA cow, a dog, a cat, a mouse.\\nOh! what a bed! Oh! what a house!\\n\\nOh, dear! Oh, dear! I cannot hear.\\nWill you please come over near?\\nWill you please look in my ear?\\nThere must be something there, I fear.\\nSay look! A bird was in your ear.\\nBut he is out. So have no fear.\\nAgain your ear can hear, my dear.\\n\\nMy hat is old. My teeth are gold.\\nI have a bird I like to hold.\\nMy shoe is off. My foot is cold.\\nMy shoe is off. My foot is cold.\\nI have a bird I like to hold.\\nMy hat is old. My teeth are gold.\\nAnd now my story is all told.\\n\\nWe took a look. We saw a Nook.\\nOn his head he had a hook.\\nOn his hook he had a book.\\nOn his book was \"How to Cook.\"\\nWe saw him sit and try to cook.\\nHe took a look at the book on the hook.\\nBut a Nook can\\'t read, so a Nook can\\'t cook.\\nSO...\\nWhat good to a Nook is a hook cook book?\\n\\nThe moon was out and we saw some sheep.\\nWe saw some sheep take a walk in their sleep.\\nBy the light of the moon, by the light of a star,\\nthey walked all night from near to far.\\nI would never walk. I would take a car.\\n\\nI do not like this one so well.\\nAll he does is yell, yell, yell.\\nI will not have this one about.\\nWhen he comes in I put him out.\\nThis one is quiet as a mouse.\\nI like to have him in the house.\\n\\nAt our house we open cans.\\nWe have to open many cans.\\nand that is why we have a Zans.\\nA Zans for cans is very good.\\nHave you a Zans for cans? You should.\\n\\nI like to box. How I like to box.\\nSo, every day, I box a Gox.\\nIn yellow socks I box my Gox.\\nI box in yellow Gox box socks.\\n\\nIt is fun to sing if you sing with a Ying.\\nMy Ying can sing like anything.\\nI sing high and my Ying sings low.\\nAnd we are not too bad, you know.\\n\\nThis one, I think, is called a Yink.\\nHe likes to wink, he likes to drink.\\nHe likes to drink, and drink, and drink.\\nThe thing he likes to drink is ink.\\nThe ink he likes to drink is pink.\\nHe likes to wink and drink pink ink.\\nSO...\\nIf you have a lot of ink,\\nthen you should get a Yink, I think.\\n\\nHop! Hop! Hop! I am a Yop\\nAll I like to do is hop,\\nFrom finger top to finger top.\\nI hop from left to right and then...\\nHop! Hop! I hop right back again.\\nI like to hop all day and night.\\nFrom right to left and left to right.\\nWhy do I like to hop, hop, hop?\\nI do not know. Go ask your Pop.\\n\\nBrush! Brush! Brush! Brush!\\nComb! Comb! Comb! Comb!\\nBlue hair is fun to brush and comb.\\nAll girls who like to brush and comb,\\nShould have a pet like this at home.\\n\\nWho is this pet? Say! He is wet.\\nYou never yet met a pet, I bet,\\nas wet as they let this wet pet get.\\n\\nDid you ever fly a kite in bed?\\nDid you ever walk with ten cats on your head?\\nDid you ever milk this kind of cow?\\nWell, we can do it. We know how.\\nIf you never did, you should.\\nThese things are fun, and fun is good.\\n\\nHello! Hello! Are you there?\\nHello! I called you up to say hello.\\nI said hello.\\nCan you hear me, Joe?\\nOh no. I cannot hear your call.\\nI cannot hear your call at all.\\nThis is not good and I know why.\\nA mouse has cut the wire. Good-by!\\n\\nFrom near to far, from here to there,\\nFunny things are everywhere.\\nThese yellow pets are called the Zeds.\\nThey have one hair upon their heads.\\nTheir hair grows fast. So fast they say,\\nThey need a haircut every day.\\n\\nWho am I? My name is Ish\\nOn my hand I have a dish.\\nI have this dish to help me wish.\\nWhen I wish to make a wish\\nI wave my hand with a big swish swish.\\nThen I say, \"I wish for fish!\"\\nAnd I get fish right on my dish.\\nSo...\\nIf you wish to make a wish,\\nyou may swish for fish with my Ish wish dish.\\n\\nAt our house we play out back.\\nWe play a game called Ring the Gack.\\nWould you like to play this game?\\nCome down! We have the only Gack in town.\\n\\nLook what we found in the park in the dark.\\nWe will take him home, we will call him Clark.\\nHe will live at our house, he will grow and grow.\\nWill our mother like this? We don\\'t know.\\n\\nAnd now good night.\\nIt is time to sleep\\nSo we will sleep with our pet Zeep.\\nToday is gone. Today was fun.\\nTomorrow is another one.\\nEvery day, from here to there.\\nfunny things are everywhere.\\n'])",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#bag-of-words-representation",
    "href": "slides/07-text-data.html#bag-of-words-representation",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Bag-of-Words Representation",
    "text": "Bag-of-Words Representation\nIn the bag-of-words representation in this data, each column represents a word, and the values in the column are the word counts for that document.\n\nFirst, we need to split each document into individual words.\n\n\ndocs[\"hop_on_pop.txt\"].split()\n\n['UP', 'PUP', 'Pup', 'is', 'up.', 'CUP', 'PUP', 'Pup', 'in', 'cup.', 'PUP', 'CUP', 'Cup', 'on', 'pup.', 'MOUSE', 'HOUSE', 'Mouse', 'on', 'house.', 'HOUSE', 'MOUSE', 'House', 'on', 'mouse.', 'ALL', 'TALL', 'We', 'all', 'are', 'tall.', 'ALL', 'SMALL', 'We', 'all', 'are', 'small.', 'ALL', 'BALL', 'We', 'all', 'play', 'ball.', 'BALL', 'WALL', 'Up', 'on', 'a', 'wall.', 'ALL', 'FALL', 'Fall', 'off', 'the', 'wall.', 'DAY', 'PLAY', 'We', 'play', 'all', 'day.', 'NIGHT', 'FIGHT', 'We', 'fight', 'all', 'night.HE', 'ME', 'He', 'is', 'after', 'me.', 'HIM', 'JIM', 'Jim', 'is', 'after', 'him.', 'SEE', 'BEE', 'We', 'see', 'a', 'bee.', 'SEE', 'BEE', 'THREE', 'Now', 'we', 'see', 'three.', 'THREE', 'TREE', 'Three', 'fish', 'in', 'a', 'tree.', 'Fish', 'in', 'a', 'tree?', 'How', 'can', 'that', 'be?', 'RED', 'RED', 'They', 'call', 'me', 'Red.', 'RED', 'BED', 'I', 'am', 'in', 'bed.', 'RED', 'NED', 'TED', 'and', 'ED', 'in', 'BED', 'PAT', 'PAT', 'they', 'call', 'him', 'Pat.', 'PAT', 'SAT', 'Pat', 'sat', 'on', 'hat.', 'PAT', 'CAT', 'Pat', 'sat', 'on', 'cat.', 'PAT', 'BAT', 'Pat', 'sat', 'on', 'bat.', 'NO', 'PAT', 'NO', 'Don’t', 'sit', 'on', 'that.', 'SAD', 'DAD', 'BAD', 'HAD', 'Dad', 'is', 'sad.', 'Very,', 'very', 'sad.', 'He', 'had', 'a', 'bad', 'day.', 'What', 'a', 'day', 'Dad', 'had!', 'THING', 'THING', 'What', 'is', 'that', 'thing?', 'THING', 'SING', 'That', 'thing', 'can', 'sing!', 'SONG', 'LONG', 'A', 'long,', 'long', 'song.', 'Good-by,', 'Thing.', 'You', 'sing', 'too', 'long.', 'WALK', 'WALK', 'We', 'like', 'to', 'walk.', 'WALK', 'TALK', 'We', 'like', 'to', 'talk.', 'HOP', 'POP', 'We', 'like', 'to', 'hop.', 'We', 'like', 'to', 'hop', 'on', 'top', 'of', 'Pop.', 'STOP', 'You', 'must', 'not', 'hop', 'on', 'Pop.', 'Mr.', 'BROWN', 'Mrs.', 'BROWN', 'Mr.', 'Brown', 'upside', 'down.', 'Pup', 'up.', 'Brown', 'down.', 'Pup', 'is', 'down.', 'Where', 'is', 'Brown?', 'WHERE', 'IS', 'BROWN?', 'THERE', 'IS', 'BROWN!', 'Mr.', 'Brown', 'is', 'out', 'of', 'town.', 'BACK', 'BLACK', 'Brown', 'came', 'back.', 'Brown', 'came', 'back', 'with', 'Mr.', 'Black.', 'SNACK', 'SNACK', 'Eat', 'a', 'snack.', 'Eat', 'a', 'snack', 'with', 'Brown', 'and', 'Black.', 'JUMP', 'BUMP', 'He', 'jumped.', 'He', 'bumped.', 'FAST', 'PAST', 'He', 'went', 'past', 'fast.', 'WENT', 'TENT', 'SENT', 'He', 'went', 'into', 'the', 'tent.', 'I', 'sent', 'him', 'out', 'of', 'the', 'tent.', 'WET', 'GET', 'Two', 'dogs', 'get', 'wet.', 'HELP', 'YELP', 'They', 'yelp', 'for', 'help.', 'HILL', 'WILL', 'Will', 'went', 'up', 'hill.', 'WILL', 'HILL', 'STILL', 'Will', 'is', 'up', 'hill', 'still.', 'FATHER', 'MOTHER', 'SISTER', 'BROTHER', 'That', 'one', 'is', 'my', 'other', 'brother.', 'My', 'brothers', 'read', 'a', 'little', 'bit.', 'Little', 'words', 'like', 'If', 'and', 'it.', 'My', 'father', 'can', 'read', 'big', 'words,', 'too.', 'Like', 'CONSTANTINOPLE', 'and', 'TIMBUKTU', 'SAY', 'SAY', 'What', 'does', 'this', 'say?', 'seehemewe', 'patpuppop', 'hethreetreebee', 'tophopstop', 'Ask', 'me', 'tomorrow', 'but', 'not', 'today.']",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#then-count-the-words",
    "href": "slides/07-text-data.html#then-count-the-words",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Then Count the Words",
    "text": "Then Count the Words\n\n\nfrom collections import Counter\nCounter(\n  docs[\"hop_on_pop.txt\"]\n  .split()\n  )\n\nCounter({'is': 10, 'on': 10, 'We': 10, 'a': 9, 'He': 6, 'PAT': 6, 'Brown': 6, 'in': 5, 'all': 5, 'like': 5, 'Pup': 4, 'ALL': 4, 'RED': 4, 'and': 4, 'to': 4, 'Mr.': 4, 'PUP': 3, 'the': 3, 'can': 3, 'Pat': 3, 'sat': 3, 'What': 3, 'THING': 3, 'WALK': 3, 'of': 3, 'down.': 3, 'went': 3, 'up.': 2, 'CUP': 2, 'MOUSE': 2, 'HOUSE': 2, 'are': 2, 'BALL': 2, 'play': 2, 'wall.': 2, 'day.': 2, 'after': 2, 'SEE': 2, 'BEE': 2, 'see': 2, 'THREE': 2, 'that': 2, 'They': 2, 'call': 2, 'me': 2, 'BED': 2, 'I': 2, 'him': 2, 'NO': 2, 'Dad': 2, 'sad.': 2, 'That': 2, 'You': 2, 'hop': 2, 'Pop.': 2, 'not': 2, 'BROWN': 2, 'IS': 2, 'out': 2, 'came': 2, 'with': 2, 'Black.': 2, 'SNACK': 2, 'Eat': 2, 'tent.': 2, 'HILL': 2, 'WILL': 2, 'Will': 2, 'up': 2, 'My': 2, 'read': 2, 'SAY': 2, 'UP': 1, 'cup.': 1, 'Cup': 1, 'pup.': 1, 'Mouse': 1, 'house.': 1, 'House': 1, 'mouse.': 1, 'TALL': 1, 'tall.': 1, 'SMALL': 1, 'small.': 1, 'ball.': 1, 'WALL': 1, 'Up': 1, 'FALL': 1, 'Fall': 1, 'off': 1, 'DAY': 1, 'PLAY': 1, 'NIGHT': 1, 'FIGHT': 1, 'fight': 1, 'night.HE': 1, 'ME': 1, 'me.': 1, 'HIM': 1, 'JIM': 1, 'Jim': 1, 'him.': 1, 'bee.': 1, 'Now': 1, 'we': 1, 'three.': 1, 'TREE': 1, 'Three': 1, 'fish': 1, 'tree.': 1, 'Fish': 1, 'tree?': 1, 'How': 1, 'be?': 1, 'Red.': 1, 'am': 1, 'bed.': 1, 'NED': 1, 'TED': 1, 'ED': 1, 'they': 1, 'Pat.': 1, 'SAT': 1, 'hat.': 1, 'CAT': 1, 'cat.': 1, 'BAT': 1, 'bat.': 1, 'Don’t': 1, 'sit': 1, 'that.': 1, 'SAD': 1, 'DAD': 1, 'BAD': 1, 'HAD': 1, 'Very,': 1, 'very': 1, 'had': 1, 'bad': 1, 'day': 1, 'had!': 1, 'thing?': 1, 'SING': 1, 'thing': 1, 'sing!': 1, 'SONG': 1, 'LONG': 1, 'A': 1, 'long,': 1, 'long': 1, 'song.': 1, 'Good-by,': 1, 'Thing.': 1, 'sing': 1, 'too': 1, 'long.': 1, 'walk.': 1, 'TALK': 1, 'talk.': 1, 'HOP': 1, 'POP': 1, 'hop.': 1, 'top': 1, 'STOP': 1, 'must': 1, 'Mrs.': 1, 'upside': 1, 'Where': 1, 'Brown?': 1, 'WHERE': 1, 'BROWN?': 1, 'THERE': 1, 'BROWN!': 1, 'town.': 1, 'BACK': 1, 'BLACK': 1, 'back.': 1, 'back': 1, 'snack.': 1, 'snack': 1, 'JUMP': 1, 'BUMP': 1, 'jumped.': 1, 'bumped.': 1, 'FAST': 1, 'PAST': 1, 'past': 1, 'fast.': 1, 'WENT': 1, 'TENT': 1, 'SENT': 1, 'into': 1, 'sent': 1, 'WET': 1, 'GET': 1, 'Two': 1, 'dogs': 1, 'get': 1, 'wet.': 1, 'HELP': 1, 'YELP': 1, 'yelp': 1, 'for': 1, 'help.': 1, 'hill.': 1, 'STILL': 1, 'hill': 1, 'still.': 1, 'FATHER': 1, 'MOTHER': 1, 'SISTER': 1, 'BROTHER': 1, 'one': 1, 'my': 1, 'other': 1, 'brother.': 1, 'brothers': 1, 'little': 1, 'bit.': 1, 'Little': 1, 'words': 1, 'If': 1, 'it.': 1, 'father': 1, 'big': 1, 'words,': 1, 'too.': 1, 'Like': 1, 'CONSTANTINOPLE': 1, 'TIMBUKTU': 1, 'does': 1, 'this': 1, 'say?': 1, 'seehemewe': 1, 'patpuppop': 1, 'hethreetreebee': 1, 'tophopstop': 1, 'Ask': 1, 'tomorrow': 1, 'but': 1, 'today.': 1})",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#bag-of-words-representation-1",
    "href": "slides/07-text-data.html#bag-of-words-representation-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Bag-of-Words Representation",
    "text": "Bag-of-Words Representation\n… then, we put these counts into a Series.\n\n\n[\n  pd.Series(\n    Counter(doc.split())\n      ) for doc in docs.values()\n  ]\n\n[I           71\nam           3\nSam          3\nThat         2\nSam-I-am     4\n            ..\ngood         2\nsee!         1\nSo           1\nThank        2\nyou!         1\nLength: 116, dtype: int64, The         4\nsun         2\ndid         6\nnot        27\nshine.      1\n           ..\nNow,        1\nWell...     1\nYOU         1\nasked       1\nYOU?        1\nLength: 503, dtype: int64, Fox       6\nSocks     4\nBox       1\nKnox      8\nin       19\n         ..\nour       1\ndone,     1\nThank     1\nlot       1\nfun,      1\nLength: 328, dtype: int64, Every        3\nWho          9\nDown         1\nin          15\nWhoville     4\n            ..\nlight,       1\nbrought      1\nhe,          1\nHIMSELF!     1\ncarved       1\nLength: 623, dtype: int64, UP             1\nPUP            3\nPup            4\nis            10\nup.            2\n              ..\ntophopstop     1\nAsk            1\ntomorrow       1\nbut            1\ntoday.         1\nLength: 241, dtype: int64, On              5\nthe            88\nfifteenth       1\nof             33\nMay,            1\n               ..\nsummer.         1\nrain            1\nit's            1\nfall-ish,       1\nsmall-ish!\"     1\nLength: 918, dtype: int64, Congratulations!     1\nToday                2\nis                   7\nyour                19\nday.                 1\n                    ..\nday!                 1\nYour                 1\nmountain             1\nSo...get             1\nway!                 1\nLength: 449, dtype: int64, One         1\nfish,       7\ntwo         2\nred         1\nblue        2\n           ..\nTomorrow    1\nanother     1\none.        1\nEvery       1\nthere.      1\nLength: 501, dtype: int64]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#create-a-dataframe",
    "href": "slides/07-text-data.html#create-a-dataframe",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Create a DataFrame",
    "text": "Create a DataFrame\n… finally, we stack the Series into a DataFrame. This is called bag of words data.\n\n\npd.DataFrame(\n    [pd.Series(\n      Counter(doc.split())\n      ) for doc in docs.values()],\n    index = docs.keys()\n    )\n\n                                       I   am  Sam  ...  gone.  Tomorrow  one.\ngreen_eggs_and_ham.txt              71.0  3.0  3.0  ...    NaN       NaN   NaN\ncat_in_the_hat.txt                  48.0  NaN  NaN  ...    NaN       NaN   NaN\nfox_in_socks.txt                     9.0  NaN  NaN  ...    NaN       NaN   NaN\nhow_the_grinch_stole_christmas.txt   6.0  NaN  NaN  ...    NaN       NaN   NaN\nhop_on_pop.txt                       2.0  1.0  NaN  ...    NaN       NaN   NaN\nhorton_hears_a_who.txt              18.0  1.0  NaN  ...    NaN       NaN   NaN\noh_the_places_youll_go.txt           2.0  NaN  NaN  ...    NaN       NaN   NaN\none_fish_two_fish.txt               48.0  3.0  NaN  ...    1.0       1.0   1.0\n\n[8 rows x 2562 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#bag-of-words-in-scikit-learn",
    "href": "slides/07-text-data.html#bag-of-words-in-scikit-learn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Bag-of-Words in Scikit-Learn",
    "text": "Bag-of-Words in Scikit-Learn\n\nAlternatively, we can use CountVectorizer() in scikit-learn to produce a bag-of-words matrix.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\nSpecify\n\n\nvec = CountVectorizer()\n\n\n\n\nFit\n\n\n\nvec.fit(docs.values())\n\n\nCountVectorizer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer() \n\n\n\n\n\nTransform\n\n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 2308 stored elements and shape (8, 1344)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#entire-vocabulary",
    "href": "slides/07-text-data.html#entire-vocabulary",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Entire Vocabulary",
    "text": "Entire Vocabulary\n\nThe set of words across a corpus is called the vocabulary. We can view the vocabulary in a fitted CountVectorizer() as follows:\n\nvec.vocabulary_\n\n{'am': 23, 'sam': 935, 'that': 1138, 'do': 287, 'not': 767, 'like': 644, 'you': 1336, 'green': 471, 'eggs': 326, 'and': 26, 'ham': 495, 'them': 1141, 'would': 1316, 'here': 526, 'or': 786, 'there': 1143, 'anywhere': 32, 'in': 576, 'house': 558, 'with': 1303, 'mouse': 722, 'eat': 323, 'box': 132, 'fox': 419, 'could': 242, 'car': 179, 'they': 1145, 'are': 35, 'may': 688, 'will': 1292, 'see': 953, 'tree': 1204, 'let': 635, 'me': 691, 'be': 62, 'mot': 718, 'train': 1202, 'on': 778, 'say': 944, 'the': 1139, 'dark': 265, 'rain': 884, 'goat': 453, 'boat': 118, 'so': 1035, 'try': 1213, 'if': 575, 'good': 459, 'thank': 1136, 'sun': 1107, 'did': 279, 'shine': 972, 'it': 586, 'was': 1255, 'too': 1188, 'wet': 1268, 'to': 1178, 'play': 836, 'we': 1261, 'sat': 940, 'all': 16, 'cold': 231, 'day': 270, 'sally': 934, 'two': 1220, 'said': 932, 'how': 560, 'wish': 1302, 'had': 488, 'something': 1042, 'go': 452, 'out': 789, 'ball': 50, 'nothing': 768, 'at': 43, 'sit': 1001, 'one': 780, 'little': 650, 'bit': 102, 'bump': 157, 'then': 1142, 'went': 1265, 'made': 673, 'us': 1231, 'jump': 594, 'looked': 660, 'saw': 943, 'him': 536, 'step': 1077, 'mat': 684, 'cat': 185, 'hat': 507, 'he': 513, 'why': 1285, 'know': 614, 'is': 583, 'sunny': 1108, 'but': 165, 'can': 176, 'have': 512, 'lots': 663, 'of': 773, 'fun': 434, 'funny': 435, 'some': 1039, 'games': 438, 'new': 747, 'tricks': 1207, 'lot': 662, 'show': 986, 'your': 1338, 'mother': 719, 'mind': 704, 'what': 1269, 'our': 788, 'for': 413, 'fish': 388, 'no': 754, 'make': 676, 'away': 44, 'tell': 1130, 'want': 1253, 'should': 980, 'about': 4, 'when': 1271, 'now': 769, 'fear': 368, 'my': 735, 'bad': 47, 'game': 437, 'call': 172, 'up': 1226, 'put': 869, 'down': 299, 'this': 1151, 'fall': 358, 'hold': 543, 'high': 532, 'as': 39, 'stand': 1064, 'book': 122, 'hand': 496, 'cup': 260, 'look': 659, 'cake': 171, 'top': 1191, 'books': 123, 'litte': 649, 'toy': 1200, 'ship': 973, 'milk': 702, 'dish': 284, 'hop': 550, 'oh': 775, 'these': 1144, 'rake': 885, 'man': 680, 'tail': 1117, 'red': 897, 'fan': 362, 'fell': 372, 'his': 538, 'head': 514, 'came': 175, 'from': 429, 'things': 1149, 'into': 582, 'pot': 853, 'lit': 648, 'sank': 937, 'deep': 275, 'shook': 979, 'bent': 85, 'get': 444, 'another': 27, 'ran': 887, 'fast': 364, 'back': 46, 'big': 93, 'wood': 1308, 'shut': 987, 'hook': 548, 'trick': 1206, 'take': 1118, 'got': 462, 'tip': 1175, 'bow': 131, 'pick': 823, 'thing': 1148, 'bite': 103, 'shake': 964, 'hands': 497, 'their': 1140, 'those': 1153, 'gave': 442, 'pat': 807, 'tame': 1125, 'come': 233, 'give': 448, 'fly': 405, 'kites': 611, 'hit': 540, 'run': 925, 'hall': 494, 'wall': 1251, 'thump': 1161, 'string': 1096, 'kite': 610, 'gown': 463, 'her': 525, 'dots': 297, 'pink': 829, 'white': 1277, 'bed': 67, 'bumps': 159, 'jumps': 596, 'kicks': 605, 'hops': 551, 'thumps': 1162, 'kinds': 609, 'way': 1260, 'she': 969, 'home': 545, 'hear': 517, 'find': 380, 'near': 738, 'think': 1150, 'rid': 903, 'after': 7, 'net': 745, 'bet': 87, 'yet': 1327, 'plop': 841, 'last': 625, 'thoe': 1152, 'stop': 1086, 'pack': 793, 'dear': 273, 'shame': 967, 'sad': 929, 'kind': 607, 'has': 505, 'gone': 457, 'yes': 1326, 'mess': 697, 'tall': 1124, 'ca': 168, 'who': 1279, 'always': 22, 'playthings': 838, 'were': 1266, 'picked': 824, 'strings': 1097, 'any': 29, 'well': 1264, 'asked': 41, 'socks': 1037, 'knox': 616, 'chicks': 203, 'bricks': 143, 'blocks': 112, 'clocks': 224, 'sir': 999, 'mr': 727, 'first': 387, 'll': 653, 'quick': 875, 'brick': 142, 'stack': 1063, 'block': 111, 'chick': 202, 'clock': 223, 'ticks': 1165, 'tocks': 1180, 'tick': 1164, 'tock': 1179, 'six': 1003, 'sick': 988, 'please': 840, 'don': 293, 'tongue': 1187, 'isn': 585, 'slick': 1011, 'mixed': 709, 'sorry': 1048, 'an': 25, 'easy': 322, 'whose': 1283, 'sue': 1105, 'sews': 963, 'sees': 959, 'sew': 962, 'comes': 234, 'crow': 255, 'slow': 1014, 'joe': 590, 'clothes': 226, 'rose': 920, 'hose': 554, 'nose': 766, 'goes': 454, 'grows': 482, 'hate': 508, 'makes': 678, 'quite': 879, 'lame': 622, 'blue': 117, 'goo': 458, 'gooey': 460, 'gluey': 451, 'chewy': 201, 'chewing': 200, 'goose': 461, 'doing': 292, 'choose': 209, 'chew': 199, 'won': 1305, 'very': 1237, 'bim': 97, 'ben': 82, 'brings': 146, 'broom': 148, 'bends': 83, 'breaks': 140, 'band': 51, 'bands': 52, 'pig': 825, 'lead': 630, 'brooms': 149, 'bangs': 54, 'booms': 125, 'boom': 124, 'poor': 849, 'mouth': 724, 'much': 730, 'bring': 145, 'luke': 669, 'luck': 668, 'likes': 647, 'lakes': 621, 'duck': 309, 'licks': 637, 'takes': 1119, 'blab': 105, 'such': 1103, 'blibber': 110, 'blubber': 116, 'rubber': 923, 'dumb': 311, 'through': 1160, 'three': 1158, 'cheese': 198, 'trees': 1205, 'free': 420, 'fleas': 395, 'flew': 396, 'while': 1276, 'freezy': 422, 'breeze': 141, 'blew': 109, 'freeze': 421, 'sneeze': 1033, 'enough': 340, 'silly': 991, 'stuff': 1100, 'talk': 1121, 'tweetle': 1218, 'beetles': 72, 'fight': 377, 'called': 173, 'beetle': 71, 'battle': 59, 'puddle': 866, 'paddles': 798, 'paddle': 796, 'bottle': 127, 'muddle': 731, 'battles': 60, 'poodle': 846, 'eating': 324, 'noodles': 761, 'noodle': 760, 'wait': 1244, 'minute': 706, 'where': 1272, 'bottled': 128, 'paddled': 797, 'muddled': 732, 'duddled': 310, 'fuddled': 432, 'wuddled': 1319, 'done': 294, 'every': 346, 'whoville': 1284, 'liked': 645, 'christmas': 210, 'grinch': 473, 'lived': 652, 'just': 598, 'north': 765, 'hated': 509, 'whole': 1280, 'season': 952, 'ask': 40, 'knows': 615, 'reason': 896, 'wasn': 1256, 'screwed': 950, 'right': 905, 'perhaps': 817, 'shoes': 978, 'tight': 1168, 'most': 716, 'likely': 646, 'been': 69, 'heart': 519, 'sizes': 1005, 'small': 1019, 'whatever': 1270, 'stood': 1085, 'eve': 343, 'hating': 510, 'whos': 1282, 'staring': 1068, 'cave': 189, 'sour': 1053, 'grinchy': 475, 'frown': 430, 'warm': 1254, 'lighted': 643, 'windows': 1295, 'below': 81, 'town': 1198, 'knew': 612, 'beneath': 84, 'busy': 164, 'hanging': 499, 'mistletoe': 707, 'wreath': 1318, 're': 892, 'stockings': 1082, 'snarled': 1027, 'sneer': 1031, 'tomorrow': 1186, 'practically': 856, 'growled': 481, 'fingers': 384, 'nervously': 744, 'drumming': 307, 'must': 734, 'coming': 235, 'girls': 447, 'boys': 134, 'wake': 1246, 'bright': 144, 'early': 318, 'rush': 926, 'toys': 1201, 'noise': 755, 'young': 1337, 'old': 777, 'feast': 369, 'pudding': 865, 'rare': 889, 'roast': 912, 'beast': 64, 'which': 1275, 'couldn': 243, 'least': 632, 'close': 225, 'together': 1183, 'bells': 80, 'ringing': 907, 'start': 1069, 'singing': 997, 'sing': 996, 'more': 714, 'thought': 1155, 'fifty': 376, 'years': 1322, 've': 1236, 'idea': 574, 'awful': 45, 'wonderful': 1306, 'laughed': 627, 'throat': 1159, 'santy': 939, 'claus': 216, 'coat': 230, 'chuckled': 211, 'clucked': 229, 'great': 467, 'saint': 933, 'nick': 750, 'need': 742, 'reindeer': 898, 'around': 38, 'since': 994, 'scarce': 945, 'none': 757, 'found': 417, 'simply': 993, 'instead': 581, 'dog': 290, 'max': 687, 'took': 1189, 'thread': 1157, 'tied': 1167, 'horn': 552, 'loaded': 655, 'bags': 48, 'empty': 335, 'sacks': 928, 'ramshackle': 886, 'sleigh': 1010, 'hitched': 541, 'giddap': 446, 'started': 1070, 'toward': 1195, 'homes': 546, 'lay': 629, 'asnooze': 42, 'quiet': 877, 'snow': 1034, 'filled': 378, 'air': 12, 'dreaming': 301, 'sweet': 1112, 'dreams': 302, 'without': 1304, 'care': 180, 'square': 1062, 'number': 771, 'hissed': 539, 'climbed': 222, 'roof': 916, 'fist': 389, 'slid': 1012, 'chimney': 206, 'rather': 890, 'pinch': 828, 'santa': 938, 'stuck': 1099, 'only': 781, 'once': 779, 'moment': 710, 'fireplace': 386, 'flue': 403, 'hung': 570, 'row': 922, 'grinned': 477, 'slithered': 1013, 'slunk': 1017, 'smile': 1023, 'unpleasant': 1225, 'room': 917, 'present': 857, 'pop': 850, 'guns': 485, 'bicycles': 92, 'roller': 915, 'skates': 1006, 'drums': 308, 'checkerboards': 197, 'tricycles': 1208, 'popcorn': 851, 'plums': 843, 'stuffed': 1101, 'nimbly': 752, 'by': 167, 'icebox': 573, 'cleaned': 218, 'flash': 394, 'even': 344, 'hash': 506, 'food': 408, 'glee': 450, 'grabbed': 466, 'shove': 985, 'heard': 518, 'sound': 1051, 'coo': 239, 'dove': 298, 'turned': 1215, 'cindy': 213, 'lou': 664, 'than': 1135, 'caught': 187, 'tiny': 1174, 'daughter': 268, 'water': 1258, 'stared': 1067, 'taking': 1120, 'smart': 1021, 'lie': 638, 'tot': 1194, 'fake': 357, 'lied': 639, 'light': 642, 'side': 989, 'workshop': 1312, 'fix': 391, 'fib': 374, 'fooled': 410, 'child': 204, 'patted': 810, 'drink': 303, 'sent': 960, 'log': 656, 'fire': 385, 'himself': 537, 'liar': 636, 'walls': 1252, 'left': 634, 'hooks': 549, 'wire': 1301, 'speck': 1056, 'crumb': 256, 'same': 936, 'other': 787, 'houses': 559, 'leaving': 633, 'crumbs': 257, 'mouses': 723, 'quarter': 873, 'past': 806, 'dawn': 269, 'still': 1081, 'packed': 795, 'sled': 1008, 'presents': 858, 'ribbons': 902, 'wrappings': 1317, 'tags': 1116, 'tinsel': 1173, 'trimmings': 1209, 'trappings': 1203, 'thousand': 1156, 'feet': 371, 'mt': 729, 'crumpit': 258, 'rode': 914, 'load': 654, 'tiptop': 1176, 'dump': 312, 'pooh': 847, 'grinchishly': 474, 'humming': 565, 'finding': 381, 'waking': 1247, 'mouths': 725, 'hang': 498, 'open': 784, 'cry': 259, 'boo': 121, 'hoo': 547, 'paused': 811, 'ear': 317, 'rising': 908, 'over': 790, 'low': 667, 'grow': 480, 'sounded': 1052, 'merry': 696, 'popped': 852, 'eyes': 352, 'shocking': 976, 'surprise': 1111, 'hadn': 489, 'stopped': 1087, 'somehow': 1040, 'ice': 572, 'puzzling': 872, 'packages': 794, 'boxes': 133, 'puzzled': 870, 'hours': 557, 'till': 1169, 'puzzler': 871, 'sore': 1047, 'before': 74, 'maybe': 689, 'doesn': 289, 'store': 1088, 'means': 693, 'happened': 501, 'grew': 472, 'didn': 280, 'feel': 370, 'whizzed': 1278, 'morning': 715, 'brought': 152, 'carved': 183, 'pup': 868, 'off': 774, 'night': 751, 'jim': 588, 'bee': 68, 'ned': 741, 'ted': 1128, 'ed': 325, 'bat': 57, 'dad': 263, 'song': 1045, 'long': 658, 'walk': 1248, 'brown': 153, 'mrs': 728, 'upside': 1230, 'black': 106, 'snack': 1025, 'jumped': 595, 'bumped': 158, 'tent': 1132, 'dogs': 291, 'help': 523, 'yelp': 1325, 'hill': 534, 'father': 366, 'sister': 1000, 'brother': 150, 'brothers': 151, 'read': 893, 'words': 1309, 'constantinople': 238, 'timbuktu': 1170, 'does': 288, 'seehemewe': 954, 'patpuppop': 809, 'hethreetreebee': 527, 'tophopstop': 1192, 'today': 1181, 'fifteenth': 375, 'jungle': 597, 'nool': 763, 'heat': 520, 'cool': 241, 'pool': 848, 'splashing': 1058, 'enjoying': 339, 'joys': 592, 'horton': 553, 'elephant': 331, 'towards': 1196, 'again': 9, 'faint': 355, 'person': 818, 'calling': 174, 'dust': 314, 'blowing': 115, 'though': 1154, 'murmured': 733, 'never': 746, 'able': 3, 'yell': 1323, 'someone': 1041, 'sort': 1049, 'creature': 251, 'size': 1004, 'seen': 958, 'shaking': 965, 'blow': 114, 'steer': 1076, 'save': 941, 'because': 66, 'matter': 685, 'gently': 443, 'using': 1233, 'greatest': 469, 'stretched': 1095, 'trunk': 1212, 'lifted': 641, 'carried': 181, 'placed': 832, 'safe': 931, 'soft': 1038, 'clover': 227, 'humpf': 567, 'humpfed': 568, 'voice': 1242, 'twas': 1217, 'kangaroo': 599, 'pouch': 855, 'pin': 827, 'believe': 77, 'sincerely': 995, 'ears': 319, 'keen': 601, 'clearly': 221, 'four': 418, 'family': 360, 'children': 205, 'starting': 1071, 'favour': 367, 'disturb': 286, 'fool': 409, 'biggest': 95, 'blame': 107, 'kangaroos': 600, 'plunged': 844, 'terrible': 1133, 'frowned': 431, 'persons': 819, 'drowned': 306, 'protect': 861, 'bigger': 94, 'plucked': 842, 'hustled': 571, 'tops': 1193, 'news': 748, 'quickly': 876, 'spread': 1061, 'talks': 1123, 'flower': 402, 'walked': 1249, 'worrying': 1315, 'almost': 18, 'hour': 556, 'alarm': 13, 'harm': 504, 'walking': 1250, 'talking': 1122, 'barely': 56, 'speak': 1055, 'friend': 425, 'fine': 382, 'helped': 524, 'folks': 407, 'end': 336, 'saved': 942, 'ceilings': 190, 'floors': 401, 'churches': 212, 'grocery': 479, 'stores': 1089, 'mean': 692, 'gasped': 441, 'buildings': 156, 'piped': 830, 'certainly': 191, 'mayor': 690, 'friendly': 426, 'clean': 217, 'seem': 956, 'terribly': 1134, 'aren': 36, 'wonderfully': 1307, 'ville': 1239, 'thankful': 1137, 'greatful': 470, 'worry': 1314, 'spoke': 1059, 'monkeys': 711, 'neck': 739, 'wickersham': 1286, 'shouting': 984, 'rot': 921, 'elephants': 332, 'going': 455, 'nonsense': 758, 'snatched': 1028, 'bottomed': 129, 'eagle': 316, 'named': 737, 'valad': 1234, 'vlad': 1241, 'koff': 617, 'mighty': 699, 'strong': 1098, 'swift': 1113, 'wing': 1296, 'kindly': 608, 'beak': 63, 'late': 626, 'afternoon': 8, 'far': 363, 'bird': 99, 'flapped': 392, 'wings': 1297, 'flight': 398, 'chased': 195, 'groans': 478, 'stones': 1084, 'tattered': 1126, 'toenails': 1182, 'battered': 58, 'bones': 120, 'begged': 75, 'live': 651, 'folk': 406, 'beyond': 90, 'kept': 602, 'flapping': 393, 'shoulder': 981, 'quit': 878, 'yapping': 1321, 'hide': 531, '56': 1, 'next': 749, 'sure': 1109, 'place': 831, 'hid': 529, 'drop': 304, 'somewhere': 1044, 'inside': 579, 'patch': 808, 'clovers': 228, 'hundred': 569, 'miles': 701, 'wide': 1288, 'sneered': 1032, 'fail': 354, 'flip': 399, 'cried': 253, 'bust': 163, 'shall': 966, 'friends': 427, 'searched': 951, 'sought': 1050, 'noon': 764, 'dead': 272, 'alive': 15, 'piled': 826, 'nine': 753, 'five': 390, 'millionth': 703, 'really': 895, 'trouble': 1210, 'share': 968, 'birdie': 100, 'dropped': 305, 'landed': 623, 'hard': 503, 'tea': 1127, 'pots': 854, 'broken': 147, 'rocking': 913, 'chairs': 192, 'smashed': 1022, 'bicycle': 91, 'tires': 1177, 'crashed': 250, 'pleaded': 839, 'stick': 1079, 'making': 679, 'repairs': 900, 'course': 246, 'answered': 28, 'thin': 1147, 'thick': 1146, 'days': 271, 'wild': 1291, 'insisted': 580, 'chatting': 196, 'existed': 350, 'carryings': 182, 'peaceable': 812, 'bellowing': 79, 'bungle': 160, 'state': 1072, 'snapped': 1026, 'nonsensical': 759, 'dozens': 300, 'uncles': 1223, 'wickershams': 1287, 'cousins': 247, 'laws': 628, 'engaged': 338, 'roped': 919, 'caged': 170, 'hah': 490, 'boil': 119, 'hot': 555, 'steaming': 1075, 'kettle': 603, 'beezle': 73, 'nut': 772, 'oil': 776, 'full': 433, 'prove': 862, 'meeting': 695, 'everyone': 347, 'holler': 544, 'shout': 982, 'scream': 949, 'stew': 1078, 'scared': 947, 'people': 814, 'loudly': 666, 'smiled': 1024, 'clear': 219, 'bell': 78, 'surely': 1110, 'wind': 1294, 'distant': 285, 'voices': 1243, 'either': 329, 'neither': 743, 'grab': 465, 'shouted': 983, 'cage': 169, 'dope': 296, 'lasso': 624, 'stomach': 1083, 'ten': 1131, 'rope': 918, 'tie': 1166, 'knots': 613, 'lose': 661, 'dunk': 313, 'juice': 593, 'fought': 415, 'vigor': 1238, 'vim': 1240, 'gang': 439, 'many': 682, 'beat': 65, 'mauled': 686, 'haul': 511, 'managed': 681, 'die': 281, 'yourselves': 1340, 'tom': 1185, 'smack': 1018, 'whooped': 1281, 'racked': 882, 'rattled': 891, 'kettles': 604, 'brass': 137, 'pans': 802, 'garbage': 440, 'pail': 800, 'cranberry': 249, 'cans': 178, 'bazooka': 61, 'blasted': 108, 'toots': 1190, 'clarinets': 214, 'oom': 783, 'pahs': 799, 'flutes': 404, 'gusts': 486, 'loud': 665, 'racket': 883, 'rang': 888, 'sky': 1007, 'howling': 562, 'mad': 672, 'hullabaloo': 564, 'hey': 528, 'hows': 563, 'mine': 705, 'best': 86, 'working': 1311, 'anyone': 30, 'shirking': 975, 'rushed': 927, 'east': 321, 'west': 1267, 'seemed': 957, 'yipping': 1331, 'beeping': 70, 'bipping': 98, 'ruckus': 924, 'roar': 911, 'raced': 881, 'each': 315, 'building': 155, 'floor': 400, 'felt': 373, 'getting': 445, 'nowhere': 770, 'despair': 277, 'suddenly': 1104, 'burst': 161, 'door': 295, 'discovered': 283, 'shirker': 974, 'hidden': 530, 'fairfax': 356, 'apartments': 34, 'apartment': 33, '12': 0, 'jo': 589, 'standing': 1065, 'bouncing': 130, 'yo': 1332, 'yipp': 1330, 'chirp': 208, 'twerp': 1219, 'lad': 619, 'eiffelberg': 327, 'tower': 1197, 'towns': 1199, 'darkest': 267, 'time': 1171, 'blood': 113, 'aid': 11, 'country': 244, 'noises': 756, 'greater': 468, 'amounts': 24, 'counts': 245, 'thus': 1163, 'cleared': 220, 'yopp': 1335, 'extra': 351, 'finally': 379, 'proved': 863, 'world': 1313, 'smallest': 1020, 'true': 1211, 'planning': 835, 'summer': 1106, 'ish': 584, 'congratulations': 237, 'places': 833, 'brains': 135, 'yourself': 1339, 'direction': 282, 'own': 791, 'guy': 487, 'decide': 274, 'streets': 1094, 'em': 334, 'street': 1093, 'case': 184, 'straight': 1091, 'opener': 785, 'happen': 500, 'frequently': 423, 'brainy': 136, 'footsy': 412, 'along': 20, 'happening': 502, 'seeing': 955, 'sights': 990, 'join': 591, 'fliers': 397, 'soar': 1036, 'heights': 521, 'lag': 620, 'behind': 76, 'speed': 1057, 'pass': 805, 'soon': 1046, 'wherever': 1273, 'rest': 901, 'except': 349, 'sometimes': 1043, 'sadly': 930, 'bang': 53, 'ups': 1229, 'prickle': 859, 'ly': 671, 'perch': 816, 'lurch': 670, 'chances': 194, 'slump': 1015, 'un': 1221, 'slumping': 1016, 'easily': 320, 'marked': 683, 'mostly': 717, 'darked': 266, 'sprain': 1060, 'both': 126, 'elbow': 330, 'chin': 207, 'dare': 264, 'stay': 1073, 'win': 1293, 'turn': 1214, 'quarters': 874, 'sneak': 1029, 'simple': 992, 'afraid': 6, 'maker': 677, 'upper': 1228, 'confused': 236, 'race': 880, 'wiggled': 1290, 'roads': 910, 'break': 139, 'necking': 740, 'pace': 792, 'grind': 476, 'cross': 254, 'weirdish': 1263, 'space': 1054, 'headed': 515, 'useless': 1232, 'waiting': 1245, 'bus': 162, 'plane': 834, 'mail': 675, 'phone': 822, 'ring': 906, 'hair': 491, 'friday': 424, 'uncle': 1222, 'jake': 587, 'better': 88, 'pearls': 813, 'pair': 801, 'pants': 803, 'wig': 1289, 'curls': 261, 'chance': 193, 'escape': 341, 'staying': 1074, 'playing': 837, 'banner': 55, 'ride': 904, 'ready': 894, 'anything': 31, 'under': 1224, 'points': 845, 'scored': 948, 'magical': 674, 'winning': 1300, 'est': 342, 'winner': 1299, 'fame': 359, 'famous': 361, 'watching': 1257, 'tv': 1216, 'times': 1172, 'lonely': 657, 'cause': 188, 'against': 10, 'alone': 19, 'whether': 1274, 'meet': 694, 'scare': 946, 'road': 909, 'between': 89, 'hither': 542, 'yon': 1333, 'weather': 1262, 'foul': 416, 'enemies': 337, 'prowl': 864, 'hakken': 493, 'kraks': 618, 'howl': 561, 'onward': 782, 'frightening': 428, 'creek': 252, 'arms': 37, 'sneakers': 1030, 'leak': 631, 'hike': 533, 'face': 353, 'problems': 860, 'already': 21, 'strange': 1092, 'birds': 101, 'tact': 1115, 'remember': 899, 'life': 640, 'balancing': 49, 'act': 5, 'forget': 414, 'dexterous': 278, 'deft': 276, 'mix': 708, 'foot': 411, 'succeed': 1102, 'indeed': 577, '98': 2, 'percent': 815, 'guaranteed': 483, 'kid': 606, 'move': 726, 'mountains': 721, 'name': 736, 'buxbaum': 166, 'bixby': 104, 'bray': 138, 'mordecai': 713, 'ali': 14, 'van': 1235, 'allen': 17, 'shea': 970, 'mountain': 720, 'star': 1066, 'glad': 449, 'fat': 365, 'yellow': 1324, 'everywhere': 348, 'seven': 961, 'eight': 328, 'eleven': 333, 'ever': 345, 'wump': 1320, 'hump': 566, 'gump': 484, 'pull': 867, 'sticks': 1080, 'bike': 96, 'mike': 700, 'sits': 1002, 'work': 1310, 'hills': 535, 'hello': 522, 'cow': 248, 'cannot': 177, 'teeth': 1129, 'gold': 456, 'shoe': 977, 'story': 1090, 'told': 1184, 'nook': 762, 'cook': 240, 'moon': 712, 'sheep': 971, 'sleep': 1009, 'zans': 1341, 'gox': 464, 'ying': 1328, 'sings': 998, 'yink': 1329, 'wink': 1298, 'ink': 578, 'yop': 1334, 'finger': 383, 'brush': 154, 'comb': 232, 'pet': 820, 'met': 698, 'cats': 186, 'cut': 262, 'pets': 821, 'zeds': 1342, 'upon': 1227, 'heads': 516, 'haircut': 492, 'wave': 1259, 'swish': 1114, 'gack': 436, 'park': 804, 'clark': 215, 'zeep': 1343}",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#specific-words",
    "href": "slides/07-text-data.html#specific-words",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Specific Words",
    "text": "Specific Words\n\nWe can extract specific words from the vocabulary as follows:\n\nvec.vocabulary_[\"fish\"]\n\n388\n\nvec.vocabulary_[\"pop\"]\n\n850\n\nvec.vocabulary_[\"eggs\"]\n\n326",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-normalizing",
    "href": "slides/07-text-data.html#text-normalizing",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Normalizing",
    "text": "Text Normalizing\nWhat’s wrong with the way we counted words originally?\nCounter({'UP': 1, 'PUP': 3, 'Pup': 4, 'is': 10, 'up.': 2, ...})\n\n\nIt’s usually good to normalize for punctuation and capitalization.\nNormalization options are specified when you initialize the CountVectorizer().\nBy default, scikit-learn strips punctuation and converts all characters to lowercase.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-normalizing-in-sklearn",
    "href": "slides/07-text-data.html#text-normalizing-in-sklearn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Normalizing in sklearn",
    "text": "Text Normalizing in sklearn\n\nIf you don’t want scikit-learn to normalize for punctuation and capitalization, you can do the following:\n\n\n\nvec = CountVectorizer(lowercase = False, token_pattern = r\"[\\S]+\")\nvec.fit(docs.values())\n\nCountVectorizer(lowercase=False, token_pattern='[\\\\S]+')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(lowercase=False, token_pattern='[\\\\S]+') \n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 3679 stored elements and shape (8, 2562)&gt;\n\n\n\n\n\n\n\n\n\n\nCountVectorizer()\n\n\nSetting lowercase = False doesn’t automatically convert every word to lowercase. token_pattern = r\"[\\S]+\" declares a regular expression that treats every sequence of characters that is not whitespace ([\\S]) as a single token.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#the-shortcomings-of-bag-of-words",
    "href": "slides/07-text-data.html#the-shortcomings-of-bag-of-words",
    "title": "Bag-of-Words and TF-IDF",
    "section": "The Shortcomings of Bag-of-Words",
    "text": "The Shortcomings of Bag-of-Words\nBag-of-words is easy to understand and easy to implement. What are its disadvantages?\nConsider the following documents:\n\n“The dog bit her owner.”\n“Her dog bit the owner.”\n\nBoth documents have the same exact bag-of-words representation, but they mean something quite different!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#n-grams-1",
    "href": "slides/07-text-data.html#n-grams-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "N-grams",
    "text": "N-grams\n\n\nAn n-gram is a sequence of \\(n\\) words.\nN-grams allow us to capture more of the meaning.\nFor example, if we count bigrams (2-grams) instead of words, we can distinguish the two documents from before:\n\n\n“The dog bit her owner.”\n“Her dog bit the owner.”\n\n\n\n\\[\\begin{array}{l|ccccccc}\n& \\text{the, dog} & \\text{her, dog} & \\text{dog, bit} & \\text{bit, the} & \\text{bit, her} & \\text{the, owner} & \\text{her, owner} \\\\\n\\hline\n\\text{1} & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\\\\n\\text{2} & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\\\\n\\end{array}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#n-grams-in-scikit-learn",
    "href": "slides/07-text-data.html#n-grams-in-scikit-learn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "N-grams in scikit-learn",
    "text": "N-grams in scikit-learn\n\nWe can easily modify our previous approach by specifying ngram_range = in CountVectorizer(). To get bigrams, we set the range to (2, 2).\n\n\nSpecify\n\n\nvec = CountVectorizer(ngram_range = (2, 2))\n\n\n\n\nFit\n\n\n\nvec.fit(docs.values())\n\n\nCountVectorizer(ngram_range=(2, 2))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(ngram_range=(2, 2)) \n\n\n\n\n\nTransform\n\n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 6459 stored elements and shape (8, 5846)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#n-grams-in-scikit-learn-1",
    "href": "slides/07-text-data.html#n-grams-in-scikit-learn-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "N-grams in scikit-learn",
    "text": "N-grams in scikit-learn\n\n… or we can also get individual words (unigrams) alongside the bigrams:\n\nSpecify\n\n\nvec = CountVectorizer(ngram_range = (1, 2))\n\n\nFit\n\n\n\nvec.fit(docs.values())\n\n\nCountVectorizer(ngram_range=(1, 2))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(ngram_range=(1, 2)) \n\n\n\nTransform\n\n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 8767 stored elements and shape (8, 7190)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#similar-documents",
    "href": "slides/07-text-data.html#similar-documents",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Similar Documents",
    "text": "Similar Documents\nNow, we can use this bag-of-words data to measure similarities between documents!\n\n\nfrom sklearn.metrics import pairwise_distances\n\ndat = vec.transform(docs.values())\n\ndists = pairwise_distances(dat)\ndists\n\narray([[  0.        , 204.84628383, 178.52730884, 219.79535937,\n        168.42802617, 228.52570971, 183.5973856 , 178.83511959],\n       [204.84628383,   0.        , 189.45711916, 157.15597348,\n        186.2793601 , 152.32859219, 175.28262892, 156.12815249],\n       [178.52730884, 189.45711916,   0.        , 171.66828478,\n         95.21554495, 189.95262567, 141.52031656, 130.11533345],\n       [219.79535937, 157.15597348, 171.66828478,   0.        ,\n        163.84138671, 138.97481786, 174.56230979, 162.59766296],\n       [168.42802617, 186.2793601 ,  95.21554495, 163.84138671,\n          0.        , 188.92855793, 133.1990991 , 112.89818422],\n       [228.52570971, 152.32859219, 189.95262567, 138.97481786,\n        188.92855793,   0.        , 162.83120094, 164.95453919],\n       [183.5973856 , 175.28262892, 141.52031656, 174.56230979,\n        133.1990991 , 162.83120094,   0.        , 134.98888843],\n       [178.83511959, 156.12815249, 130.11533345, 162.59766296,\n        112.89818422, 164.95453919, 134.98888843,   0.        ]])",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#similar-documents-1",
    "href": "slides/07-text-data.html#similar-documents-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Similar Documents",
    "text": "Similar Documents\n\n\n\ndists[0].argsort()\n\narray([0, 4, 2, 7, 6, 1, 3, 5])\n\n\n\n\n\n\ndocs.keys()\n\ndict_keys(['green_eggs_and_ham.txt', 'cat_in_the_hat.txt', 'fox_in_socks.txt', 'how_the_grinch_stole_christmas.txt', 'hop_on_pop.txt', 'horton_hears_a_who.txt', 'oh_the_places_youll_go.txt', 'one_fish_two_fish.txt'])\n\n\n\n\n\n\n\n\n\nTip\n\n\nThis is how data scientists do authorship identification!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#issues-with-the-distance-approach",
    "href": "slides/07-text-data.html#issues-with-the-distance-approach",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Issues with the Distance Approach",
    "text": "Issues with the Distance Approach\nBUT WAIT!\n\nDon’t we care more about word choice than total words used?\nWouldn’t a longer document have more words, and thus be able to “match” other documents?\nWouldn’t more common words appear in more documents, and thus cause them to “match”?\nRecall: We have many options for scaling\nRecall: We have many options for distance metrics.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#example",
    "href": "slides/07-text-data.html#example",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Example",
    "text": "Example\n\nDocument A:\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\nDocument B:\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\nDocument C:\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\nDocument D:\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#example-with-code",
    "href": "slides/07-text-data.html#example-with-code",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Example with Code",
    "text": "Example with Code\n\n\n\nCode\ndocuments = [\n    \"whoever has hate for his brother is in the darkness and walks in the darkness\",\n    \"hello darkness my old friend\",\n    \"returning hate for hate multiplies hate adding deeper darkness to a night already devoid of stars darkness cannot drive out darkness only light can do that\",\n    \"happiness can be found in the darkest of times if only one remembers to turn on the light\"\n]\n\n\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer(token_pattern = r\"\\w+\")\nvec.fit(documents)\n\n\nCountVectorizer(token_pattern='\\\\w+')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(token_pattern='\\\\w+') \n\n\nCode\nbow_matrix = vec.transform(documents)\nbow_matrix\n\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 56 stored elements and shape (4, 45)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#example-output",
    "href": "slides/07-text-data.html#example-output",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Example Output",
    "text": "Example Output\n\nbow_dataframe = pd.DataFrame(\n  bow_matrix.todense(), \n  columns = vec.get_feature_names_out()\n  )\nbow_dataframe[[\"darkness\", \"light\"]]\n\n   darkness  light\n0         2      0\n1         1      0\n2         3      1\n3         0      1",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#measuring-similarity",
    "href": "slides/07-text-data.html#measuring-similarity",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Measuring Similarity",
    "text": "Measuring Similarity\n\n\nfrom sklearn.metrics import pairwise_distances\n\ndists = pairwise_distances(bow_matrix)\ndists[0].argsort()\n\narray([0, 1, 3, 2])\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#choosing-your-distance-metric",
    "href": "slides/07-text-data.html#choosing-your-distance-metric",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Choosing Your Distance Metric",
    "text": "Choosing Your Distance Metric\nIs euclidean distance really the best choice?!\n\n\nMy name is James Bond, James Bond is my name.\n\n\nMy name is James Bond.\n\n\nMy name is James.\n\n\nIf we count words the second two will be the most similar.\nThe first document is longer, so it has “double” counts.\nBut, it has the exact same words as the second document!\nSolution: cosine distance",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#what-is-cosine-distance",
    "href": "slides/07-text-data.html#what-is-cosine-distance",
    "title": "Bag-of-Words and TF-IDF",
    "section": "What is cosine distance?",
    "text": "What is cosine distance?\n\n\nCosine similarity is a metric that determines how two vectors (words, sentences, features) are similar to each other.\n\n\n\n\n\n\n\nCosine similarity is the dot product of the vectors divided by the product of their lengths.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#cosine-distance-1",
    "href": "slides/07-text-data.html#cosine-distance-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Cosine Distance",
    "text": "Cosine Distance\n\nAs a rule, cosine distance is a better choice for bag-of-words data!\n\nfrom sklearn.metrics.pairwise import cosine_distances\n\ndists = cosine_distances(bow_matrix)\ndists[0].argsort()\n\narray([0, 2, 3, 1])\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#measuring-similarity-1",
    "href": "slides/07-text-data.html#measuring-similarity-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Measuring Similarity",
    "text": "Measuring Similarity\nWhich of these seems most important for measuring similarity?\n\nDocument B, C, D all have the word “to”\nDocuments A, B, and C all have the word darkness.\nDocument A and Document C both have the word “hate”\nDocument C and Document D both have the word “light”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-scaling",
    "href": "slides/07-text-data.html#tf-idf-scaling",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF Scaling",
    "text": "TF-IDF Scaling\nWe would like to scale our word counts by the document length (TF).\n\nWe would also like to scale our word counts by the number of documents they appear in. (IDF)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#document-lengths",
    "href": "slides/07-text-data.html#document-lengths",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Document Lengths",
    "text": "Document Lengths\nIf a document is longer, it is more likely to share words with other documents.\n\nbow_totals = bow_dataframe.sum(axis = 1)\nbow_totals\n\n0    15\n1     5\n2    26\n3    18\ndtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#term-frequencies-tf",
    "href": "slides/07-text-data.html#term-frequencies-tf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Term Frequencies (TF)",
    "text": "Term Frequencies (TF)\nLet’s use frequencies instead of counts.\n\n\nbow_tf = bow_dataframe.divide(bow_totals, axis = 0)\nbow_tf\n\n          a    adding   already  ...      turn     walks   whoever\n0  0.000000  0.000000  0.000000  ...  0.000000  0.066667  0.066667\n1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n2  0.038462  0.038462  0.038462  ...  0.000000  0.000000  0.000000\n3  0.000000  0.000000  0.000000  ...  0.055556  0.000000  0.000000\n\n[4 rows x 45 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#distance-of-term-frequencies-tf",
    "href": "slides/07-text-data.html#distance-of-term-frequencies-tf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Distance of Term Frequencies (TF)",
    "text": "Distance of Term Frequencies (TF)\n\n\ndists = cosine_distances(bow_tf)\ndists[0].argsort()\n\narray([0, 2, 3, 1])\n\n\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#inverse-document-frequency-idf",
    "href": "slides/07-text-data.html#inverse-document-frequency-idf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Inverse Document Frequency (IDF)",
    "text": "Inverse Document Frequency (IDF)\n\nIn principle, if two documents share rarer words they are more similar.\nWhat matters is not overall word frequency but how many of the documents have that word.\n\n\n\nbow_dataframe\n\n   a  adding  already  and  be  brother  ...  the  times  to  turn  walks  whoever\n0  0       0        0    1   0        1  ...    2      0   0     0      1        1\n1  0       0        0    0   0        0  ...    0      0   0     0      0        0\n2  1       1        1    0   0        0  ...    0      0   1     0      0        0\n3  0       0        0    0   1        0  ...    2      1   1     1      0        0\n\n[4 rows x 45 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-1",
    "href": "slides/07-text-data.html#idf---step-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 1",
    "text": "IDF - Step 1\nFirst, isolate the words that occurred in each document.\n\nhas_word = (bow_dataframe &gt; 0)\nhas_word[[\"darkness\", \"light\", \"hate\"]]\n\n   darkness  light   hate\n0      True  False   True\n1      True  False  False\n2      True   True   True\n3     False   True  False",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-2",
    "href": "slides/07-text-data.html#idf---step-2",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 2",
    "text": "IDF - Step 2\nThen, let’s calculate how often the word occurred across the four documents.\n\n\n\nbow_df = (\n  has_word\n  .sum(axis = 0) / 4\n  )\nbow_df\n\na             0.25\nadding        0.25\nalready       0.25\nand           0.25\nbe            0.25\nbrother       0.25\ncan           0.50\ncannot        0.25\ndarkest       0.25\ndarkness      0.75\ndeeper        0.25\ndevoid        0.25\ndo            0.25\ndrive         0.25\nfor           0.50\nfound         0.25\nfriend        0.25\nhappiness     0.25\nhas           0.25\nhate          0.50\nhello         0.25\nhis           0.25\nif            0.25\nin            0.50\nis            0.25\nlight         0.50\nmultiplies    0.25\nmy            0.25\nnight         0.25\nof            0.50\nold           0.25\non            0.25\none           0.25\nonly          0.50\nout           0.25\nremembers     0.25\nreturning     0.25\nstars         0.25\nthat          0.25\nthe           0.50\ntimes         0.25\nto            0.50\nturn          0.25\nwalks         0.25\nwhoever       0.25\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\naxis = 0\n\n\nWhat values are we summing?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-3",
    "href": "slides/07-text-data.html#idf---step-3",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 3",
    "text": "IDF - Step 3\nFind the inverse document frequencies:\n\n\n\nbow_log_idf = np.log(1 / bow_df)\nbow_log_idf\n\na             1.386294\nadding        1.386294\nalready       1.386294\nand           1.386294\nbe            1.386294\nbrother       1.386294\ncan           0.693147\ncannot        1.386294\ndarkest       1.386294\ndarkness      0.287682\ndeeper        1.386294\ndevoid        1.386294\ndo            1.386294\ndrive         1.386294\nfor           0.693147\nfound         1.386294\nfriend        1.386294\nhappiness     1.386294\nhas           1.386294\nhate          0.693147\nhello         1.386294\nhis           1.386294\nif            1.386294\nin            0.693147\nis            1.386294\nlight         0.693147\nmultiplies    1.386294\nmy            1.386294\nnight         1.386294\nof            0.693147\nold           1.386294\non            1.386294\none           1.386294\nonly          0.693147\nout           1.386294\nremembers     1.386294\nreturning     1.386294\nstars         1.386294\nthat          1.386294\nthe           0.693147\ntimes         1.386294\nto            0.693147\nturn          1.386294\nwalks         1.386294\nwhoever       1.386294\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\nMore than just the inverse!\n\n\nNotice we are using \\(log(\\frac{1}{p_i})\\) to get the IDFs.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-4",
    "href": "slides/07-text-data.html#idf---step-4",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 4",
    "text": "IDF - Step 4\nAdjust for the inverse document frequencies:\n\n\n\n\nbow_tf[[\"darkness\", \"light\", \"hate\"]]\n\n   darkness     light      hate\n0  0.133333  0.000000  0.066667\n1  0.200000  0.000000  0.000000\n2  0.115385  0.038462  0.115385\n3  0.000000  0.055556  0.000000\n\n\n\n\n\n\n\n\nbow_log_idf[[\"darkness\", \"light\", \"hate\"]]\n\ndarkness    0.287682\nlight       0.693147\nhate        0.693147\ndtype: float64\n\n\n\n\n\n\nbow_tf_idf = bow_tf.multiply(bow_log_idf, axis = 1)\n\n\n\n\n\nbow_tf_idf[[\"darkness\", \"light\", \"hate\"]]\n\n   darkness     light      hate\n0  0.038358  0.000000  0.046210\n1  0.057536  0.000000  0.000000\n2  0.033194  0.026660  0.079979\n3  0.000000  0.038508  0.000000",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-distances",
    "href": "slides/07-text-data.html#tf-idf-distances",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF Distances",
    "text": "TF-IDF Distances\n\n\ndists = cosine_distances(bow_tf_idf).round(decimals = 2)\ndists[0].argsort()\n\narray([0, 3, 2, 1])\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-in-sklearn",
    "href": "slides/07-text-data.html#tf-idf-in-sklearn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF in sklearn",
    "text": "TF-IDF in sklearn\nSpecify\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# These options ensure that the numbers match our example above\nvec = TfidfVectorizer(smooth_idf = False)\n\n\nFit\n\n\n\nvec.fit(documents)\n\n\nTfidfVectorizer(smooth_idf=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.TfidfVectorizer?Documentation for TfidfVectorizeriFittedTfidfVectorizer(smooth_idf=False) \n\n\n\nTransform\n\n\ntfidf_matrix = vec.transform(documents)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-in-sklearn-1",
    "href": "slides/07-text-data.html#tf-idf-in-sklearn-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF in sklearn",
    "text": "TF-IDF in sklearn\n\n\ndists = cosine_distances(tfidf_matrix).round(decimals = 2)\ndists[0].argsort()\n\narray([0, 2, 3, 1])\n\n\n\n \n\n\n\n\n😕",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#what-happened",
    "href": "slides/07-text-data.html#what-happened",
    "title": "Bag-of-Words and TF-IDF",
    "section": "What happened?",
    "text": "What happened?\n\nThe formula that is used to compute the tf-idf for a term t of a document d in a document set is:\ntf-idf(t, d) = tf(t, d) * idf(t),\nand the idf is computed as:\nidf(t) = log [ n / df(t) ] + 1,\nwhere n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#sklearn-idf",
    "href": "slides/07-text-data.html#sklearn-idf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "sklearn IDF",
    "text": "sklearn IDF\n\nthe idf is computed as: idf(t) = log [ n / df(t) ] + 1\n\nWe used \\(\\text{log} \\Bigg ( \\frac{1}{\\frac{\\text{df(t)}}{n}} \\Bigg )\\) which is the same as \\(\\text{log} \\Big ( \\frac{n}{df(t)} \\Big )\\)\n\n\nBut, we never added 1 before taking the log…\n\nThe effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#sklearn-tf",
    "href": "slides/07-text-data.html#sklearn-tf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "sklearn TF",
    "text": "sklearn TF\nIn the documentation of TfidfVectorizer() it states that the function uses CountVectorizer() to obtain the TF matrix.\n\nCountVectorizer(): Transforms text into a sparse matrix of n-gram counts.\n\n\n\nThis means that sklearn is not dividing the counts of each term by the length of each document.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#what-should-you-do",
    "href": "slides/07-text-data.html#what-should-you-do",
    "title": "Bag-of-Words and TF-IDF",
    "section": "What should you do?",
    "text": "What should you do?\n\nHere’s what I know:\n\nsklearn is used by the majority of data scientists\nsklearn is focusing on “training” data not being overly specific with the documents in the training set\n\nBoth of these are points in favor of sklearn.\n\n\n\nThe fact that the authors aren’t scaling the counts by the length of each document is a major bummer, but maybe not enough to outweigh the benefits?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#takeaways-1",
    "href": "slides/07-text-data.html#takeaways-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe represent text data as a bag-of-words or bag-of-n-grams matrix.\nEach row is a document in the corpus.\nWe typically use cosine distance to measure similarity, because it captures patterns of word choice\nWe apply TF-IDF transformations to scale the bag-of-words data, so that words that appear in fewer documents are more important",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/11-classification.html#who-are-you-working-with",
    "href": "slides/11-classification.html#who-are-you-working-with",
    "title": "Classification",
    "section": "Who are you working with?",
    "text": "Who are you working with?\nPlease take 3-minutes and fill out this form:\nhttps://forms.gle/QMsNkkY1P7KQbzJq9",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/14-merge.html#data-analysis-the-whole-game",
    "href": "slides/14-merge.html#data-analysis-the-whole-game",
    "title": "Combining Datasets",
    "section": "Data analysis: The whole game",
    "text": "Data analysis: The whole game\n\n\nAcquire data and clean it by fixing variable types, dropping or replacing missing data, and looking for other issues.\nExplore the dataset by making summaries and plots of one variable.\nEstablish research questions to answer with this data.\nCreate visualizations of two or more variables that address simple questions.\nFit predictive models to address more complex questions, and/or to prepare for prediction on future data.\nFit unsupervised models to answer open-ended questions.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#example-planes-and-flights",
    "href": "slides/14-merge.html#example-planes-and-flights",
    "title": "Combining Datasets",
    "section": "Example: Planes and Flights",
    "text": "Example: Planes and Flights\nSometimes, information is spread across multiple data sets.\nFor example, suppose we want to know which manufacturer’s planes made the most flights in November 2013.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#example-planes-and-flights-1",
    "href": "slides/14-merge.html#example-planes-and-flights-1",
    "title": "Combining Datasets",
    "section": "Example: Planes and Flights",
    "text": "Example: Planes and Flights\n\nWhich manufacturer’s planes made the most flights in November 2013?\n\nIn order to answer this question we need to join these two data sets together!",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#keys",
    "href": "slides/14-merge.html#keys",
    "title": "Combining Datasets",
    "section": "Keys",
    "text": "Keys\n\n\nA primary key is a column (or a set of columns) that uniquely identifies observations in a data frame.\nThe primary key is the column(s) you would think of as the index.\nA foreign key is a column (or a set of columns) that points to the primary key of another data frame.\nPlanes are uniquely identified by their tail number (tailnum).",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joining-on-a-key-1",
    "href": "slides/14-merge.html#joining-on-a-key-1",
    "title": "Combining Datasets",
    "section": "Joining on a Key",
    "text": "Joining on a Key\nEach value of the primary key should only appear once, but it could appear many times in a foreign key.\n\n\n\n(\n  df_planes['tailnum']\n  .value_counts()\n)\n\ntailnum\nN999DN    1\nN10156    1\nN102UW    1\nN103US    1\nN104UW    1\n         ..\nN11192    1\nN11193    1\nN11194    1\nN11199    1\nN111US    1\nName: count, Length: 3322, dtype: int64\n\n\n\n\n\n\n(\n  df_flights['tailnum']\n  .value_counts()\n)\n\ntailnum\nN353JB    48\nN955UW    47\nN184JB    47\nN281JB    42\nN375JB    42\n          ..\nN8319F     1\nN8683B     1\nN919WN     1\nN119US     1\nN395HA     1\nName: count, Length: 2600, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joining-on-a-key-2",
    "href": "slides/14-merge.html#joining-on-a-key-2",
    "title": "Combining Datasets",
    "section": "Joining on a Key",
    "text": "Joining on a Key\nThe Pandas function .merge() can be used to join two DataFrames on a key.\n\n\ndf_joined = df_flights.merge(df_planes, on = \"tailnum\")\n\n\n\n       year_x  month  day  dep_time  ...  engines  seats  speed     engine\n0        2013     11    1    2108.0  ...        2     55    NaN  Turbo-fan\n1        2013     11    1    1154.0  ...        2    182    NaN  Turbo-fan\n2        2013     11    1     854.0  ...        2     55    NaN  Turbo-fan\n3        2013     11    1    1643.0  ...        2     55    NaN  Turbo-fan\n4        2013     11    1     603.0  ...        2     55    NaN  Turbo-fan\n...       ...    ...  ...       ...  ...      ...    ...    ...        ...\n23295    2013     11   30    1337.0  ...        2    142    NaN  Turbo-jet\n23296    2013     11   30     802.0  ...        2    142    NaN  Turbo-fan\n23297    2013     11   30    1544.0  ...        2    142    NaN  Turbo-fan\n23298    2013     11   30     850.0  ...        2    142    NaN  Turbo-fan\n23299    2013     11   30    1959.0  ...        2    142    NaN  Turbo-jet\n\n[23300 rows x 26 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#overlapping-column-names",
    "href": "slides/14-merge.html#overlapping-column-names",
    "title": "Combining Datasets",
    "section": "Overlapping Column Names",
    "text": "Overlapping Column Names\n\n\n\nJoining two data frames results in a wider data frame, with more columns.\nBy default, Pandas adds the suffixes _x and _y to overlapping column names. , but this can be customized.\n\n\n\n\n\n\ndf_joined.columns\n\nIndex(['year_x', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay',\n       'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight',\n       'origin', 'dest', 'air_time', 'distance', 'hour', 'minute', 'tailnum',\n       'year_y', 'type', 'manufacturer', 'model', 'engines', 'seats', 'speed',\n       'engine'],\n      dtype='object')",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#analyzing-the-joined-data",
    "href": "slides/14-merge.html#analyzing-the-joined-data",
    "title": "Combining Datasets",
    "section": "Analyzing the Joined Data",
    "text": "Analyzing the Joined Data\n\nWhich manufacturer’s planes made the most flights in November 2013?\n\n\ndf_joined[\"manufacturer\"].value_counts()\n\nmanufacturer\nBOEING                           6557\nEMBRAER                          5175\nAIRBUS                           3954\nAIRBUS INDUSTRIE                 3456\nBOMBARDIER INC                   2632\nMCDONNELL DOUGLAS AIRCRAFT CO     811\nMCDONNELL DOUGLAS                 330\nCANADAIR                          122\nMCDONNELL DOUGLAS CORPORATION     121\nCESSNA                             36\nCIRRUS DESIGN CORP                 21\nROBINSON HELICOPTER CO             19\nBARKER JACK L                      15\nPIPER                               9\nFRIEDEMANN JON                      8\nSIKORSKY                            6\nBELL                                5\nDEHAVILLAND                         4\nAMERICAN AIRCRAFT INC               3\nAGUSTA SPA                          3\nLEARJET INC                         2\nLAMBERT RICHARD                     2\nDOUGLAS                             2\nKILDALL GARY                        2\nAVIAT AIRCRAFT INC                  1\nMARZ BARRY                          1\nPAIR MIKE E                         1\nLEBLANC GLENN T                     1\nSTEWART MACO                        1\nName: count, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#example-weather-and-flights",
    "href": "slides/14-merge.html#example-weather-and-flights",
    "title": "Combining Datasets",
    "section": "Example: Weather and Flights",
    "text": "Example: Weather and Flights\n\nWhat weather factors are related to flight delays?\n\n\nHere is a data set containing hourly weather data at each airport in 2013:\n\n\n\nCode\ndf_weather = pd.read_csv(f\"{data_dir}/weather.csv\")\n\n\n\n\n      airport  year  month  day  ...  wind_gust  precip  pressure  visib\n0         EWR  2013      1    1  ...        NaN     0.0    1012.0   10.0\n1         EWR  2013      1    1  ...        NaN     0.0    1012.3   10.0\n2         EWR  2013      1    1  ...        NaN     0.0    1012.5   10.0\n3         EWR  2013      1    1  ...        NaN     0.0    1012.2   10.0\n4         EWR  2013      1    1  ...        NaN     0.0    1011.9   10.0\n...       ...   ...    ...  ...  ...        ...     ...       ...    ...\n26110     LGA  2013     12   30  ...   21.86482     0.0    1017.1   10.0\n26111     LGA  2013     12   30  ...   21.86482     0.0    1018.8   10.0\n26112     LGA  2013     12   30  ...   23.01560     0.0    1019.5   10.0\n26113     LGA  2013     12   30  ...        NaN     0.0    1019.9   10.0\n26114     LGA  2013     12   30  ...        NaN     0.0    1020.9   10.0\n\n[26115 rows x 14 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#a-key-with-multiple-columns",
    "href": "slides/14-merge.html#a-key-with-multiple-columns",
    "title": "Combining Datasets",
    "section": "A Key with Multiple Columns",
    "text": "A Key with Multiple Columns\nLet’s start by looking at flights out of JFK only, for simplicity.\n\n\ndf_flights_jfk = df_flights[df_flights[\"origin\"] == \"JFK\"]\ndf_weather_jfk = df_weather[df_weather[\"airport\"] == \"JFK\"]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#lets-see-how-rain-affects-departure-delays.",
    "href": "slides/14-merge.html#lets-see-how-rain-affects-departure-delays.",
    "title": "Combining Datasets",
    "section": "Let’s see how rain affects departure delays.",
    "text": "Let’s see how rain affects departure delays.\n\n\nCode\nfrom plotnine import *\n\n(ggplot(df_jfk, aes(x=\"precip\", y=\"dep_delay\")) +\ngeom_point(alpha=0.2) +\ntheme_classic())\n\n\n&lt;plotnine.ggplot.ggplot object at 0x10ccbfd90&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joining-on-keys-with-different-names",
    "href": "slides/14-merge.html#joining-on-keys-with-different-names",
    "title": "Combining Datasets",
    "section": "Joining on Keys with Different Names",
    "text": "Joining on Keys with Different Names\n\n\n\nSometimes, the join keys have different names in the two data sets.\nThis frequently happens if the data sets come from different sources.\nFor example, if we want to join the (entire) flights data to the weather data, we would need to include the airport in the key.\nBut the airport is called origin in df_flights and airport in df_weather.\nThe .merge() function provides left_on = and right_on = arguments for specifying different column names in the left (first) and right (second) data sets.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joining-on-keys-with-different-names-1",
    "href": "slides/14-merge.html#joining-on-keys-with-different-names-1",
    "title": "Combining Datasets",
    "section": "Joining on Keys with Different Names",
    "text": "Joining on Keys with Different Names\n\ndf_flights_weather = df_flights.merge(\n    df_weather,\n    left_on = (\"origin\", \"year\", \"month\", \"day\", \"hour\"),\n    right_on = (\"airport\", \"year\", \"month\", \"day\", \"hour\")\n    )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#rain-and-delays-by-airport",
    "href": "slides/14-merge.html#rain-and-delays-by-airport",
    "title": "Combining Datasets",
    "section": "Rain and delays by airport",
    "text": "Rain and delays by airport\nNow we can visualize how rain impacts delays at each airport!\n\nBut first, let’s find the mean amount of rain for each departure delay to clean up our plot.\n\ndf_rain = (\n  df_flights_weather\n  .groupby([\"airport\", \"precip\"])[\"dep_delay\"]\n  .mean()\n  .reset_index()\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#joins",
    "href": "slides/14-merge.html#joins",
    "title": "Combining Datasets",
    "section": "Joins",
    "text": "Joins\nWe can merge these two data sets on a primary key…\n\ndf_joined = (\n  df_1920\n  .merge(df_2020, \n         on = [\"Name\", \"Sex\"], \n         suffixes = (\"_1920\", \"_2020\")\n         )\n  )\n\n\n\n          Name Sex  Count_1920  Count_2020\n0         Mary   F       70975        2210\n1      Dorothy   F       36645         562\n2        Helen   F       35098         721\n3     Margaret   F       27997        2190\n4         Ruth   F       26100        1323\n...        ...  ..         ...         ...\n4473     Whitt   M           5          23\n4474     Wyley   M           5           6\n4475    Xavier   M           5        3876\n4476      York   M           5          14\n4477      Zeke   M           5         382\n\n[4478 rows x 4 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#missing-keys",
    "href": "slides/14-merge.html#missing-keys",
    "title": "Combining Datasets",
    "section": "Missing Keys?",
    "text": "Missing Keys?\n… but what happened to some of the names?\n\ndf_joined[df_joined[\"Name\"] == \"Maya\"]\n\nEmpty DataFrame\nColumns: [Name, Sex, Count_1920, Count_2020]\nIndex: []",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#missing-keys-1",
    "href": "slides/14-merge.html#missing-keys-1",
    "title": "Combining Datasets",
    "section": "Missing Keys?",
    "text": "Missing Keys?\n\nWhy isn’t Maya in the joined data? It’s in the 2020 data…\n\ndf_2020[df_2020[\"Name\"] == \"Maya\"]\n\n       Name Sex  Count\n60     Maya   F   3724\n28914  Maya   M      6\n\n\n\n\n\n…but it’s not in the 1920 data.\n\ndf_1920[df_1920[\"Name\"] == \"Maya\"]\n\nEmpty DataFrame\nColumns: [Name, Sex, Count]\nIndex: []",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#missing-keys-2",
    "href": "slides/14-merge.html#missing-keys-2",
    "title": "Combining Datasets",
    "section": "Missing keys",
    "text": "Missing keys\n\nHow does the merge() function determine which keys get kept?\n\nBy default, in order to appear in the joined data, a key must be present in both tables.\n\n\nThis is actually what is called an *inner join, but there are other types of joins!",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#types-of-joins",
    "href": "slides/14-merge.html#types-of-joins",
    "title": "Combining Datasets",
    "section": "Types of Joins",
    "text": "Types of Joins\n\n\n\nBy default, Pandas does an inner join, which only keeps keys that are present in both tables.\nAn outer join keeps any key that is present in either table.\nA left join keeps all keys in the left table, even if they are not in the right table. But any keys that are only in the right table are dropped.\nA right join keeps all keys in the right table, even if they are not in the left table. But any keys that are only in the left table are dropped.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#types-of-joins-1",
    "href": "slides/14-merge.html#types-of-joins-1",
    "title": "Combining Datasets",
    "section": "Types of Joins",
    "text": "Types of Joins",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#types-of-joins-2",
    "href": "slides/14-merge.html#types-of-joins-2",
    "title": "Combining Datasets",
    "section": "Types of Joins",
    "text": "Types of Joins\n\nWe can customize the type of join using the how = parameter of .merge(). By default, how = \"inner\".\n\ndf_joined_outer = (\n  df_1920\n  .merge(df_2020, \n         on = [\"Name\", \"Sex\"],\n         suffixes = (\"_1920\", \"_2020\"), \n         how = \"outer\")\n    )\n\n\n\n\n\n\ndf_joined_outer[df_joined_outer[\"Name\"] == \"Maya\"]\n\n       Name Sex  Count_1920  Count_2020\n24999  Maya   F         NaN      3724.0\n25000  Maya   M         NaN         6.0",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#types-of-joins-3",
    "href": "slides/14-merge.html#types-of-joins-3",
    "title": "Combining Datasets",
    "section": "Types of Joins",
    "text": "Types of Joins\n\n\nNote the missing values for other columns, like Count_1920!\nWhat other type of join would have produced this output in the Maya row?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#quick-quiz",
    "href": "slides/14-merge.html#quick-quiz",
    "title": "Combining Datasets",
    "section": "Quick Quiz",
    "text": "Quick Quiz\nWhich type of join would be best suited for each case?\n\nWe want to determine the names that have increased in popularity the most between 1920 and 2020.\n\n\n\nWe want to graph the popularity of names over time.\n\n\n\nWe want to determine the names that have decreased in popularity the most between 1920 and 2020.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#many-to-many-relationships",
    "href": "slides/14-merge.html#many-to-many-relationships",
    "title": "Combining Datasets",
    "section": "Many-to-Many Relationships",
    "text": "Many-to-Many Relationships\n\nSo far, the keys we’ve joined on have been the primary key of (at least) one table.\nIf we join to the primary key of another table, then the relationship is one-to-one (since primary keys uniquely identify rows).\nIf we join to the foreign key of another table, then the relationship is one-to-many.\nWhat if we join on a key that is not a primary key?\nThat is, what if the key does not uniquely identify rows in either table so that each value of the key might appear multiple times?\nThis relaionship is called many-to-many.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#many-to-many-example",
    "href": "slides/14-merge.html#many-to-many-example",
    "title": "Combining Datasets",
    "section": "Many-to-Many Example",
    "text": "Many-to-Many Example\nWhat if we only joined on the name?\n\ndf_1920.merge(df_2020, on=\"Name\")\n\n          Name Sex_x  Count_x Sex_y  Count_y\n0         Mary     F    70975     F     2210\n1         Mary     F    70975     M        5\n2      Dorothy     F    36645     F      562\n3        Helen     F    35098     F      721\n4     Margaret     F    27997     F     2190\n...        ...   ...      ...   ...      ...\n6158    Xavier     M        5     M     3876\n6159      York     M        5     F        6\n6160      York     M        5     M       14\n6161      Zeke     M        5     M      382\n6162      Zera     M        5     F       11\n\n[6163 rows x 5 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#preventing-bugs",
    "href": "slides/14-merge.html#preventing-bugs",
    "title": "Combining Datasets",
    "section": "Preventing Bugs",
    "text": "Preventing Bugs\n\nMost of the time, many-to-many joins are a bug, caused by a misunderstanding about the primary key.\nPandas allows us to specify the relationship we are expecting. It will fail with an error if the relationship is a different kind.\nFor example, suppose we thought that “name” was the primary key of the baby name tables.\n\n\ndf_1920.merge(df_2020, on=\"Name\",\n              validate=\"one_to_one\")\n\nMergeError: Merge keys are not unique in either left or right dataset; not a one-to-one merge\nErrors are (sometimes) your friend. They can prevent you from making even bigger mistakes!",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#filtering-joins-1",
    "href": "slides/14-merge.html#filtering-joins-1",
    "title": "Combining Datasets",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nInner, outer, left, and right are known as mutating joins, because they create new combined data sets.\n\n\nThere are two other types of joins that we use for filtering to get rid of some rows:\nA semi-join tells us which keys in the left are present in the right.\nAn anti-join tells us which keys in the left are not present in the right.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#filtering-joins-2",
    "href": "slides/14-merge.html#filtering-joins-2",
    "title": "Combining Datasets",
    "section": "Filtering Joins",
    "text": "Filtering Joins",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#your-turn",
    "href": "slides/14-merge.html#your-turn",
    "title": "Combining Datasets",
    "section": "Your turn",
    "text": "Your turn\n\nDid your name exist in 2020 but not 1920?\nIf so, how has the popularity changed?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#takeaways-1",
    "href": "slides/14-merge.html#takeaways-1",
    "title": "Combining Datasets",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nA primary key is one or more columns that uniquely identify the rows.\nWe can join (a.k.a. merge) data sets if they share a primary key, or if one has a foreign key.\nThe default of .merge() is an inner join: only keys in both data sets are kept.\nWe can instead specify a left join, right join, or outer join; think about which rows we want to keep.\nFiltering joins like anti-join and semi-join can help you answer questions about the data.\nUse .isin() to see which keys in one dataset exist in the other.",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#example-information-on-flights",
    "href": "slides/14-merge.html#example-information-on-flights",
    "title": "Combining Datasets",
    "section": "Example: Information on Flights",
    "text": "Example: Information on Flights\nOne data set contains information about flights in Nov. 2013…\n\n\n\nCode\nimport pandas as pd\ndata_dir = \"https://datasci112.stanford.edu/data/nycflights13/\"\ndf_flights = pd.read_csv(f\"{data_dir}/flights11.csv\")\n\n\n\n\n       year  month  day  dep_time  ...  distance  hour  minute  tailnum\n0      2013     11    1    2108.0  ...      1167    20      56   N10156\n1      2013     11    1    1154.0  ...       541    12       0   N102UW\n2      2013     11    1     854.0  ...       946     8      29   N10575\n3      2013     11    1    1643.0  ...       594    15       5   N10575\n4      2013     11    1     603.0  ...       282     6       0   N11109\n...     ...    ...  ...       ...  ...       ...   ...     ...      ...\n23295  2013     11   30    1337.0  ...      1076    13      40   N994DL\n23296  2013     11   30     802.0  ...      1069     8       7   N995DL\n23297  2013     11   30    1544.0  ...      1069    15      50   N995DL\n23298  2013     11   30     850.0  ...       762     9       0   N996DL\n23299  2013     11   30    1959.0  ...       762    20       0   N999DN\n\n[23300 rows x 18 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#example-information-on-planes",
    "href": "slides/14-merge.html#example-information-on-planes",
    "title": "Combining Datasets",
    "section": "Example: Information on Planes",
    "text": "Example: Information on Planes\n…while another contains information about planes.\n\n\n\nCode\ndf_planes = pd.read_csv(f\"{data_dir}/planes.csv\")\n\n\n\n\n     tailnum    year                     type  ... seats speed     engine\n0     N10156  2004.0  Fixed wing multi engine  ...    55   NaN  Turbo-fan\n1     N102UW  1998.0  Fixed wing multi engine  ...   182   NaN  Turbo-fan\n2     N103US  1999.0  Fixed wing multi engine  ...   182   NaN  Turbo-fan\n3     N104UW  1999.0  Fixed wing multi engine  ...   182   NaN  Turbo-fan\n4     N10575  2002.0  Fixed wing multi engine  ...    55   NaN  Turbo-fan\n...      ...     ...                      ...  ...   ...   ...        ...\n3317  N997AT  2002.0  Fixed wing multi engine  ...   100   NaN  Turbo-fan\n3318  N997DL  1992.0  Fixed wing multi engine  ...   142   NaN  Turbo-fan\n3319  N998AT  2002.0  Fixed wing multi engine  ...   100   NaN  Turbo-fan\n3320  N998DL  1992.0  Fixed wing multi engine  ...   142   NaN  Turbo-jet\n3321  N999DN  1992.0  Fixed wing multi engine  ...   142   NaN  Turbo-jet\n\n[3322 rows x 9 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#overlapping-column-names-1",
    "href": "slides/14-merge.html#overlapping-column-names-1",
    "title": "Combining Datasets",
    "section": "Overlapping Column Names",
    "text": "Overlapping Column Names\nBut this can be customized!\n\n\ndf_joined = df_flights.merge(df_planes, on = \"tailnum\",\n                             suffixes = (\"_flight\", \"_plane\")\n                             )\n\n\n\n\n\n\nIndex(['year_flight', 'month', 'day', 'dep_time', 'sched_dep_time',\n       'dep_delay', 'arr_time', 'sched_arr_time', 'arr_delay', 'carrier',\n       'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute',\n       'tailnum', 'year_plane', 'type', 'manufacturer', 'model', 'engines',\n       'seats', 'speed', 'engine'],\n      dtype='object')",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#identifying-the-primary-key",
    "href": "slides/14-merge.html#identifying-the-primary-key",
    "title": "Combining Datasets",
    "section": "Identifying the Primary Key",
    "text": "Identifying the Primary Key\nWhat is / are the primary key(s) of this dataset?\n\n\n\n      airport  year  month  day  hour  wind_gust  precip  pressure  visib\n0         EWR  2013      1    1     1        NaN     0.0    1012.0   10.0\n1         EWR  2013      1    1     2        NaN     0.0    1012.3   10.0\n2         EWR  2013      1    1     3        NaN     0.0    1012.5   10.0\n3         EWR  2013      1    1     4        NaN     0.0    1012.2   10.0\n4         EWR  2013      1    1     5        NaN     0.0    1011.9   10.0\n...       ...   ...    ...  ...   ...        ...     ...       ...    ...\n26110     LGA  2013     12   30    14   21.86482     0.0    1017.1   10.0\n26111     LGA  2013     12   30    15   21.86482     0.0    1018.8   10.0\n26112     LGA  2013     12   30    16   23.01560     0.0    1019.5   10.0\n26113     LGA  2013     12   30    17        NaN     0.0    1019.9   10.0\n26114     LGA  2013     12   30    18        NaN     0.0    1020.9   10.0\n\n[26115 rows x 9 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#verifying-primary-key",
    "href": "slides/14-merge.html#verifying-primary-key",
    "title": "Combining Datasets",
    "section": "Verifying Primary Key",
    "text": "Verifying Primary Key\n\n(\n  df_weather\n  .groupby([\"airport\", \"year\", \"month\", \"day\", \"hour\"])\n  .size()\n)\n\nairport  year  month  day  hour\nEWR      2013  1      1    1       1\n                           2       1\n                           3       1\n                           4       1\n                           5       1\n                                  ..\nLGA      2013  12     30   14      1\n                           15      1\n                           16      1\n                           17      1\n                           18      1\nLength: 26112, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#a-key-with-multiple-columns-1",
    "href": "slides/14-merge.html#a-key-with-multiple-columns-1",
    "title": "Combining Datasets",
    "section": "A Key with Multiple Columns",
    "text": "A Key with Multiple Columns\nWe need to join to the weather data on year, month, day, and hour.\n\n\ndf_jfk = df_flights_jfk.merge(df_weather_jfk, \n                              on = (\"year\", \"month\", \"day\", \"hour\")\n                              )\n\n\n\n      year  month  day  dep_time  ...  wind_gust  precip  pressure  visib\n0     2013     11    1    1154.0  ...        NaN    0.00    1002.8   10.0\n1     2013     11    1    2055.0  ...        NaN    0.00    1005.0   10.0\n2     2013     11    1    1814.0  ...        NaN    0.00    1003.9   10.0\n3     2013     11    1    1014.0  ...        NaN    0.07       NaN   10.0\n4     2013     11    1    1852.0  ...        NaN    0.00    1003.9   10.0\n...    ...    ...  ...       ...  ...        ...     ...       ...    ...\n7396  2013     11   30    2009.0  ...        NaN    0.00       NaN   10.0\n7397  2013     11   30    2047.0  ...        NaN    0.00    1031.9   10.0\n7398  2013     11   30    1841.0  ...        NaN    0.00    1033.6   10.0\n7399  2013     11   30     802.0  ...        NaN    0.00    1042.1   10.0\n7400  2013     11   30    1544.0  ...        NaN    0.00    1036.1   10.0\n\n[7401 rows x 28 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#how-does-rain-affects-departure-delays",
    "href": "slides/14-merge.html#how-does-rain-affects-departure-delays",
    "title": "Combining Datasets",
    "section": "How does rain affects departure delays?",
    "text": "How does rain affects departure delays?\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(data = df_jfk, \n         mapping = aes(x = \"precip\", y = \"dep_delay\")) +\n  geom_point(alpha = 0.2) +\n  labs(x = \"Precipitation (feet)\", \n       y = \"Departure Delay (minutes)\") +\n  theme_bw()\n)\n\n\n\nHmmmm….where did all the data go?",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#how-does-rain-affects-departure-delays-1",
    "href": "slides/14-merge.html#how-does-rain-affects-departure-delays-1",
    "title": "Combining Datasets",
    "section": "How does rain affects departure delays?",
    "text": "How does rain affects departure delays?\n\n\nCode\n(\n  ggplot(data = df_jfk, \n         mapping = aes(x = \"precip\", y = \"dep_delay\")) +\n  geom_jitter(alpha = 0.2, width = 0.1) +\n  labs(x = \"Precipitation (feet)\", \n       y = \"Departure Delay (minutes)\", \n       color = \"New York City Airport\") +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#rain-and-delays-by-airport-1",
    "href": "slides/14-merge.html#rain-and-delays-by-airport-1",
    "title": "Combining Datasets",
    "section": "Rain and delays by airport",
    "text": "Rain and delays by airport\n\n\nCode\n(\n  ggplot(data = df_rain, \n         mapping = aes(x = \"precip\", y = \"dep_delay\", color = \"airport\")) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Precipitation (feet)\", \n       y = \"Departure Delay (minutes)\") +\n  theme_bw() +\n  theme(legend_position = \"top\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/14-merge.html#example-baby-names",
    "href": "slides/14-merge.html#example-baby-names",
    "title": "Combining Datasets",
    "section": "Example: Baby names",
    "text": "Example: Baby names\n\nThe data below contains counts of names for babies born in 1920 and 2020:\n\n\n\n\nCode\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\n\ndf_1920 = pd.read_csv(data_dir + \"yob1920.txt\", header = None,\n                      names = [\"Name\", \"Sex\", \"Count\"]\n                      )\ndf_2020 = pd.read_csv(data_dir + \"yob2020.txt\", header = None,\n                      names = [\"Name\", \"Sex\", \"Count\"]\n                      )\n\n\n\n\n\n\n1920\n\n\n           Name Sex  Count\n0          Mary   F  70975\n1       Dorothy   F  36645\n2         Helen   F  35098\n3      Margaret   F  27997\n4          Ruth   F  26100\n...         ...  ..    ...\n10751     Zearl   M      5\n10752  Zeferino   M      5\n10753      Zeke   M      5\n10754      Zera   M      5\n10755   Zygmont   M      5\n\n[10756 rows x 3 columns]\n\n\n\n\n\n2020\n\n\n            Name Sex  Count\n0         Olivia   F  17641\n1           Emma   F  15656\n2            Ava   F  13160\n3      Charlotte   F  13065\n4         Sophia   F  13036\n...          ...  ..    ...\n31448     Zykell   M      5\n31449      Zylus   M      5\n31450     Zymari   M      5\n31451        Zyn   M      5\n31452      Zyran   M      5\n\n[31453 rows x 3 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  },
  {
    "objectID": "slides/11-classification.html#project-proposal---due-sunday-february-23",
    "href": "slides/11-classification.html#project-proposal---due-sunday-february-23",
    "title": "Classification",
    "section": "Project Proposal - Due Sunday, February 23",
    "text": "Project Proposal - Due Sunday, February 23\n\n\nYour group member names.\nInformation about the dataset(s) you intend to analyze:\n\nWhere are the data located?\nWho collected the data and why?\nWhat information (variables) are in the dataset?\n\nResearch Questions: You should have one primary research question and a few secondary questions\nPreliminary exploration of your dataset(s): A few simple plots or summary statistics that relate to the variables you plan to study.",
    "crumbs": [
      "Lecture Slides",
      "Week 7, Part 1 - Classification"
    ]
  },
  {
    "objectID": "slides/14-merge.html#filtering-joins-3",
    "href": "slides/14-merge.html#filtering-joins-3",
    "title": "Combining Datasets",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nWhich names existed in 1920 but don’t in 2020?\n\n\nIn Pandas, we can’t do these using .merge(), but we can use the isin() function!\n\nin_both = df_1920['Name'].isin(df_2020['Name'])\ndf_1920.loc[~in_both, 'Name']\n\n67          Myrtle\n245            Sue\n257         Nannie\n284         Virgie\n300      Bernadine\n           ...    \n10748         Ynes\n10750     Zaragoza\n10751        Zearl\n10752     Zeferino\n10755      Zygmont\nName: Name, Length: 5638, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 8, Part 2 - Combining Datasets"
    ]
  }
]