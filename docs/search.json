[
  {
    "objectID": "slides/04-group-by.html#the-importance-of-axis-labels",
    "href": "slides/04-group-by.html#the-importance-of-axis-labels",
    "title": "Multivariate Summaries",
    "section": "The Importance of Axis Labels",
    "text": "The Importance of Axis Labels\n\n\n\n\n\n\n\n\n\nDoes your plot communicate the context of the data you are plotting?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#last-week",
    "href": "slides/04-group-by.html#last-week",
    "title": "Multivariate Summaries",
    "section": "Last Week",
    "text": "Last Week\n\nReading in data and cleaning / prepping it.\nSummarizing one categorical variable with a distribution.\nSummarizing two categorical variables with joint and conditional distributions.\nUsing plotnine and the grammar of graphics to make bar plots and column plots.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#quantitative-variables-so-far",
    "href": "slides/04-group-by.html#quantitative-variables-so-far",
    "title": "Multivariate Summaries",
    "section": "Quantitative Variables So Far",
    "text": "Quantitative Variables So Far\n\nVisualizing by converting to categorical.\nVisualizing with histograms or densities.\nEstimating probabilities from histograms and densities.\nDescribing the skew.\nCalculating and explaining the mean and the median.\nCalculating and explaining the standard deviation and variance.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#new-dataset-airplane-flights",
    "href": "slides/04-group-by.html#new-dataset-airplane-flights",
    "title": "Multivariate Summaries",
    "section": "New dataset: Airplane Flights",
    "text": "New dataset: Airplane Flights\n\nWhich airline carriers are most likely to be delayed?\n\n\nLet’s look at a data set of all domestic flights that departed from one of New York City’s airports (JFK, LaGuardia, and Newark) on November 16, 2013.\n\n\ndata_dir = \"https://datasci112.stanford.edu/data/\"\ndf = pd.read_csv(data_dir + \"flights_nyc_20131116.csv\")\ndf\n\n    carrier  flight origin dest  dep_delay\n0        US    1895    EWR  CLT       -5.0\n1        UA    1014    LGA  IAH       -3.0\n2        AA    2243    JFK  MIA        2.0\n3        UA     303    JFK  SFO       -8.0\n4        US     795    LGA  PHL       -8.0\n..      ...     ...    ...  ...        ...\n573      B6     745    JFK  PSE       -3.0\n574      B6     839    JFK  BQN        0.0\n575      UA     360    EWR  PBI        NaN\n576      US    1946    EWR  CLT        NaN\n577      US    2142    LGA  BOS        NaN\n\n[578 rows x 5 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays",
    "href": "slides/04-group-by.html#delays",
    "title": "Multivariate Summaries",
    "section": "Delays",
    "text": "Delays\nWe already know how to summarize the flight delays:\n\n\n\n\n\n\nCheck-in 2.2: Interpret these numbers!\n\n\n\n\n\n\n\ndf['dep_delay'].median()\n\nnp.float64(-4.0)\n\ndf['dep_delay'].mean()\n\nnp.float64(2.0469565217391303)\n\ndf['dep_delay'].std()\n\nnp.float64(23.52882923523891)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-1",
    "href": "slides/04-group-by.html#delays-1",
    "title": "Multivariate Summaries",
    "section": "Delays",
    "text": "Delays\nWe already know how to visualize the flight delays:\n\n\n\n\n\n\nCheck-in 2.2: How would you describe this distribution?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin",
    "href": "slides/04-group-by.html#delays-by-origin",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\n\nDo the three origin airports (JFK, LGA, EWR) have different delay patterns?\n\n\n\n\n\n\n\n\nCheck-in 2.2: What could you change in this code to include the origin variable?\n\n\n\n\n\n\n\n(\n  ggplot(df, aes(x = 'dep_delay')) + \n  geom_histogram() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin-1",
    "href": "slides/04-group-by.html#delays-by-origin-1",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\nOverlapping histograms can be really hard to read…\n\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay', fill = 'origin')) + \n  geom_histogram() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin-2",
    "href": "slides/04-group-by.html#delays-by-origin-2",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\n… but overlapping densities often look nicer…\n\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay', fill = 'origin')) + \n  geom_density() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin-3",
    "href": "slides/04-group-by.html#delays-by-origin-3",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin",
    "text": "Delays by Origin\n… especially if we make them a little see-through!\n\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay', fill = 'origin')) + \n  geom_density(alpha = 0.5) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#variable-transformations",
    "href": "slides/04-group-by.html#variable-transformations",
    "title": "Multivariate Summaries",
    "section": "Variable Transformations",
    "text": "Variable Transformations\n\nThat last plot was okay, but it was hard to see the details, because the distribution is so skewed right.\nSometimes, for easier visualization, it is worth transforming a variable.\nFor skewed data, we often use a log transformation.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#log-transformation",
    "href": "slides/04-group-by.html#log-transformation",
    "title": "Multivariate Summaries",
    "section": "Log Transformation",
    "text": "Log Transformation\nExample: Salaries of $10,000, and $100,000, and $10,000,000:\n\n\ndat = pd.DataFrame({\"salary\": [10000, 100000, 10000000]})\ndat[\"log_salary\"] = np.log(dat[\"salary\"])\n\n\n\n\n\n\n\nCode\n(\n  ggplot(data = dat, mapping = aes(x = \"salary\")) + \n  geom_histogram(bins = 100) + \n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(data = dat, mapping = aes(x = \"log_salary\")) + \n  geom_histogram(bins = 100) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#log-transformations",
    "href": "slides/04-group-by.html#log-transformations",
    "title": "Multivariate Summaries",
    "section": "Log transformations",
    "text": "Log transformations\n\nUsually, we use the natural log, just for convenience.\n\n\n\nPros:\nSkewed data looks less skewed, so it is easier to see patterns.\n\n\n\nCons:\nThe variable is now on a different scale so it is not as interpretable.\n\n\n\n\n\n\n\n\nRemember, log transformations need positive numbers!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#delays-by-origin---transformed",
    "href": "slides/04-group-by.html#delays-by-origin---transformed",
    "title": "Multivariate Summaries",
    "section": "Delays by Origin - Transformed",
    "text": "Delays by Origin - Transformed\n\n\n\nCode\n# Shift delays to be above zero\ndf['delay_shifted'] = df['dep_delay'] - df['dep_delay'].min() + 1\n\n# Log transform\ndf['log_delay'] = np.log(df['delay_shifted'])\n\n(\n  ggplot(df, aes(x = 'log_delay', fill = 'origin')) + \n  geom_density(alpha = 0.5) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#another-option-boxplots",
    "href": "slides/04-group-by.html#another-option-boxplots",
    "title": "Multivariate Summaries",
    "section": "Another option: Boxplots",
    "text": "Another option: Boxplots\n\n\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = 'log_delay', x = 'origin')) + \n  geom_boxplot() + \n  labs(x = \"\", \n       y = \"Log Delay (minutes)\", \n       title = \"Comparing Departure Delays for NYC Airports\") +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = 'log_delay', x = 'origin')) + \n  geom_boxplot() +\n  labs(x = \"\", \n       y = \"Log Delay (minutes)\", \n       title = \"Comparing Departure Delays for NYC Airports\") +\n  coord_flip() +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#facetting-1",
    "href": "slides/04-group-by.html#facetting-1",
    "title": "Multivariate Summaries",
    "section": "Facetting",
    "text": "Facetting\n\nThis plot still was a little hard to read.\nWhat if we just made separate plots for each origin?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#separate-plots-for-each-origin",
    "href": "slides/04-group-by.html#separate-plots-for-each-origin",
    "title": "Multivariate Summaries",
    "section": "Separate Plots for Each Origin",
    "text": "Separate Plots for Each Origin\nOne option would be to create separate data frames for each origin.\n\n\nis_jfk = (df['origin'] == \"JFK\")\ndf_jfk = df[is_jfk]\ndf_jfk\n\n    carrier  flight origin dest  dep_delay  delay_shifted  log_delay\n2        AA    2243    JFK  MIA        2.0           22.0   3.091042\n3        UA     303    JFK  SFO       -8.0           12.0   2.484907\n11       EV    5716    JFK  IAD       -4.0           16.0   2.772589\n12       B6     583    JFK  MCO       -3.0           17.0   2.833213\n14       B6    1403    JFK  SJU       -2.0           18.0   2.890372\n..      ...     ...    ...  ...        ...            ...        ...\n570      B6     718    JFK  BOS        1.0           21.0   3.044522\n571      B6    1816    JFK  SYR       20.0           40.0   3.688879\n572      B6    1503    JFK  SJU      -10.0           10.0   2.302585\n573      B6     745    JFK  PSE       -3.0           17.0   2.833213\n574      B6     839    JFK  BQN        0.0           20.0   2.995732\n\n[208 rows x 7 columns]\n\n\n\n\nThis seems kind of annoying…",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#fyi-boolean-masking",
    "href": "slides/04-group-by.html#fyi-boolean-masking",
    "title": "Multivariate Summaries",
    "section": "FYI: Boolean Masking",
    "text": "FYI: Boolean Masking\nHow did we filter the previous df to only include \"JFK\" origins?\n\n\n\nStep 1\n\n\nis_jfk = (df['origin'] == \"JFK\")\nis_jfk\n\n0      False\n1      False\n2       True\n3       True\n4      False\n       ...  \n573     True\n574     True\n575    False\n576    False\n577    False\nName: origin, Length: 578, dtype: bool\n\n\n\n\n\n\n\n\nStep 2\n\n\ndf_jfk = df[is_jfk]\ndf_jfk[\"origin\"]\n\n2      JFK\n3      JFK\n11     JFK\n12     JFK\n14     JFK\n      ... \n570    JFK\n571    JFK\n572    JFK\n573    JFK\n574    JFK\nName: origin, Length: 208, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#facetting-2",
    "href": "slides/04-group-by.html#facetting-2",
    "title": "Multivariate Summaries",
    "section": "Facetting",
    "text": "Facetting\nFortunately, plotnine (and other plotting packages) has a trick for you!\n\n\n(\n  ggplot(df, aes(x = 'dep_delay')) + \n  geom_density() + \n  facet_wrap('origin')\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#freeing-the-scales",
    "href": "slides/04-group-by.html#freeing-the-scales",
    "title": "Multivariate Summaries",
    "section": "Freeing the Scales",
    "text": "Freeing the Scales\n\n\nCode\n(\n  ggplot(df, aes(x = 'dep_delay')) + \n  geom_density() + \n  facet_wrap('origin', scales = \"free_y\") +\n  labs(x = \"Departure Delay (minutes)\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#split-apply-combine",
    "href": "slides/04-group-by.html#split-apply-combine",
    "title": "Multivariate Summaries",
    "section": "Split-apply-combine",
    "text": "Split-apply-combine\n\n\n\nOur visualizations told us some of the story, but can we use numeric summaries as well?\nTo do this, we want to calculate the mean or median delay time for each origin airport.\nWe call this split-apply-combine:\n\nsplit the dataset up by a categorical variable origin\napply a calculation like mean\ncombine the results back into one dataset\n\nIn pandas, we use the groupby() function to take care of the split and combine steps!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#group-by",
    "href": "slides/04-group-by.html#group-by",
    "title": "Multivariate Summaries",
    "section": "Group-by",
    "text": "Group-by\n\n\n\n\n(\n  df\n  .groupby(by = \"origin\")[\"dep_delay\"]\n  .mean()\n)\n\norigin\nEWR    4.064935\nJFK    1.461538\nLGA   -0.485294\nName: dep_delay, dtype: float64\n\n\n\n\n\n\n\n\n(\n  df\n  .groupby(by = \"origin\")[\"dep_delay\"]\n  .median()\n)\n\norigin\nEWR   -3.0\nJFK   -4.0\nLGA   -6.0\nName: dep_delay, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#group-by-check-in",
    "href": "slides/04-group-by.html#group-by-check-in",
    "title": "Multivariate Summaries",
    "section": "Group-by Check-in",
    "text": "Group-by Check-in\n\n\n\n\n\n\nCheck-in 2.2\n\n\n\nWhich code is causing “split by origin”?\nWhich code is causing “calculate the mean of delays”?\nWhich code is causing the re-combining of the data?\n\n\n\n\n\n(\n  df\n  .groupby(by = \"origin\")[\"dep_delay\"]\n  .mean()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#simple-example-exam-scores",
    "href": "slides/04-group-by.html#simple-example-exam-scores",
    "title": "Multivariate Summaries",
    "section": "Simple Example: Exam Scores",
    "text": "Simple Example: Exam Scores\nHermione’s exam scores are is:\n\nPotions class: 77/100\nCharms class: 95/100\nHerbology class: 90/100\n\nIn which class did she do best?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#but-wait",
    "href": "slides/04-group-by.html#but-wait",
    "title": "Multivariate Summaries",
    "section": "But wait!",
    "text": "But wait!\nThe class means are:\n\nPotions class: 75/100\nCharms class: 85/100\nHerbology class: 85/100\n\nIn which class did she do best?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#but-wait-1",
    "href": "slides/04-group-by.html#but-wait-1",
    "title": "Multivariate Summaries",
    "section": "But wait!",
    "text": "But wait!\nThe class standard deviations are:\n\nPotions class: 2 points\nCharms class: 5 points\nHerbology class: 1 point\n\nIn which class did she do best?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#different-variabilities-by-origin",
    "href": "slides/04-group-by.html#different-variabilities-by-origin",
    "title": "Multivariate Summaries",
    "section": "Different variabilities by origin",
    "text": "Different variabilities by origin\n\nIn addition to having different centers, the three origins also have different spreads.\n\n(\n  df\n  .groupby(\"origin\")[\"dep_delay\"]\n  .std()\n)\n\norigin\nEWR    25.646258\nJFK    18.713927\nLGA    26.121365\nName: dep_delay, dtype: float64\n\n\n\n\n\n\nIn general flights from \"LGA\" have departure delays that are the furthest from the mean.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#standardized-values-1",
    "href": "slides/04-group-by.html#standardized-values-1",
    "title": "Multivariate Summaries",
    "section": "Standardized values",
    "text": "Standardized values\n\nWe standardize values by subtracting the mean and dividing by the standard deviation.\nThis tells us how much better/worse than typical values our target value is.\nThis is also called the z-score. \\[z_i = \\frac{x_i - \\bar{x}}{s_x}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#standardized-values-2",
    "href": "slides/04-group-by.html#standardized-values-2",
    "title": "Multivariate Summaries",
    "section": "Standardized values",
    "text": "Standardized values\n\nSuppose you fly from LGA and your flight is 40 minutes late. Your friend flies from JFK and their flight is 30 minutes late.\nWho got “unluckier”?\n\n\n\n\nYou?\n\n(40 - -0.48) / 26.12\n\n1.5497702909647777\n\n\n\n\n\nYour friend?\n\n(30 - 1.46) / 18.71\n\n1.5253874933190805",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#activity-2.2",
    "href": "slides/04-group-by.html#activity-2.2",
    "title": "Multivariate Summaries",
    "section": "Activity 2.2",
    "text": "Activity 2.2\n\nDo the different airlines have different patterns of flight delays?\n\n\nMake a plot to answer the question.\nCalculate values to answer the question.\nThe first row is a flight from EWR to CLT on US Airways. The second row is a flight from LGA to IAH on United Airlines. Which one was a “more extreme” delay?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#did-older-passengers-pay-a-higher-fare-on-the-titanic",
    "href": "slides/04-group-by.html#did-older-passengers-pay-a-higher-fare-on-the-titanic",
    "title": "Multivariate Summaries",
    "section": "Did older passengers pay a higher fare on the Titanic?",
    "text": "Did older passengers pay a higher fare on the Titanic?\n\nTo visualize two quantitative variables, we make a scatterplot (or point geometry).\n\n\n\n\n\nCode\n(\n  ggplot(data = df_titanic, mapping = aes(x = 'age', y = 'fare')) + \n  geom_point() +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#scatterplots",
    "href": "slides/04-group-by.html#scatterplots",
    "title": "Multivariate Summaries",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\n\n\nNotice\n\n\n\nThe explanatory variable was on the x-axis.\nThe response variable was on the y-axis.\n“If you are older, you pay more” not “If you pay more, you get older”.\n\n\n\n\n\n(\n  ggplot(data = df_titanic, \n         mapping = aes(x = 'age', y = 'fare')\n         ) + \n  geom_point() +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#making-a-clearer-plot",
    "href": "slides/04-group-by.html#making-a-clearer-plot",
    "title": "Multivariate Summaries",
    "section": "Making a Clearer Plot",
    "text": "Making a Clearer Plot\nDid you notice how difficult it was to pick out each point?\n\n\n\nPoint Size\n\n\n\nCode\n(\n  ggplot(data = df_titanic, \n         mapping = aes(x = 'age', y = 'fare')\n         ) + \n  geom_jitter(size = 0.5) +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparency\n\n\n\nCode\n(\n  ggplot(data = df_titanic, \n         mapping = aes(x = 'age', y = 'fare')\n         ) + \n  geom_point(alpha = 0.5) +\n  labs(x = \"Age of Passenger\", \n       y = \"Fare Paid on Titanic\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#spicing-things-up",
    "href": "slides/04-group-by.html#spicing-things-up",
    "title": "Multivariate Summaries",
    "section": "Spicing Things Up",
    "text": "Spicing Things Up\nHow could we make this more interesting?\n\nUse a log-transformation for fare because it is very skewed.\nAdd in a third variable, pclass. How might you do this?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#challenge",
    "href": "slides/04-group-by.html#challenge",
    "title": "Multivariate Summaries",
    "section": "Challenge",
    "text": "Challenge\nCan you re-create this plot?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#describing-a-scatterplot",
    "href": "slides/04-group-by.html#describing-a-scatterplot",
    "title": "Multivariate Summaries",
    "section": "Describing a Scatterplot",
    "text": "Describing a Scatterplot\nLet’s look at just third class:\n\n\n\nCode\nis_third= df_titanic['pclass'] == 3\ndf_third = df_titanic[is_third]\n\n(\n  ggplot(df_third, aes(x = 'age', y = 'fare')) + \n  geom_jitter(alpha = 0.8) + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#describing-the-relationship",
    "href": "slides/04-group-by.html#describing-the-relationship",
    "title": "Multivariate Summaries",
    "section": "Describing the Relationship",
    "text": "Describing the Relationship\n\n\nStrength\nNot very strong: the points don’t follow a clear pattern.\n\n\n\n\nDirection\nSlightly negative: When age was higher, fare was a little lower.\n\n\n\n\n\nShape\nNot very linear: the points don’t form a straight line.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#correlation",
    "href": "slides/04-group-by.html#correlation",
    "title": "Multivariate Summaries",
    "section": "Correlation",
    "text": "Correlation\nWhat if we want a numerical summary of the relationship between variables?\n\nDo “older than average” people pay “higher than average” fares?\n\nIn other words, when the z-score of age was high, was the z-score of fare also high?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#age-fare-correlation",
    "href": "slides/04-group-by.html#age-fare-correlation",
    "title": "Multivariate Summaries",
    "section": "Age & Fare Correlation",
    "text": "Age & Fare Correlation\n\n\n\nCode\nmean_age = df_third['age'].mean()\nmean_fare = df_third['fare'].mean()\n\n(\n  ggplot(data = df_third, mapping = aes(x = 'age', y = 'fare')) + \n  geom_jitter(alpha = 0.8) + \n  geom_vline(xintercept = mean_age, color = \"red\", linetype = \"dashed\") + \n  geom_hline(yintercept = mean_fare, color = \"red\", linetype = \"dashed\") + \n  labs(x = \"Age of Passenger\", \n       y = \"Titanic Fare Paid\")\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#correlation-1",
    "href": "slides/04-group-by.html#correlation-1",
    "title": "Multivariate Summaries",
    "section": "Correlation",
    "text": "Correlation\nInterpret this result:\n\ndf_third[['age', 'fare']].corr()\n\n           age      fare\nage   1.000000 -0.238137\nfare -0.238137  1.000000\n\n\n\n\nAge and fare are slightly negatively correlated.\nCan you think of an explanation for this?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#correlation-is-not-the-relationship",
    "href": "slides/04-group-by.html#correlation-is-not-the-relationship",
    "title": "Multivariate Summaries",
    "section": "Correlation is not the Relationship",
    "text": "Correlation is not the Relationship\n\nJust for fun: Guess the Correlation Game",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/04-group-by.html#takeaways-1",
    "href": "slides/04-group-by.html#takeaways-1",
    "title": "Multivariate Summaries",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nPlot quantitative variables across groups with overlapping density plots, boxplots, or by facetting.\nSummarize quantitative variables across groups by using groupby() and then calculating summary statistics.\nKnow what split-apply-combine means.\nPlot relationships between quantitative variables with a scatterplot.\nDescribe the strength, direction, and shape of the relationship displayed in a scatterplot.\nSummarize relationships between quantitative variables with the correlation",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 2 - Multivariate Summaries"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#getting-and-prepping-data",
    "href": "slides/02-conditional-distributions.html#getting-and-prepping-data",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Getting and Prepping Data",
    "text": "Getting and Prepping Data\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\n\ndf[\"pclass\"] = df[\"pclass\"].astype(\"category\")\ndf[\"survived\"] = df[\"survived\"].astype(\"category\")",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#thinking-about-variable-types",
    "href": "slides/02-conditional-distributions.html#thinking-about-variable-types",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Thinking About Variable Types",
    "text": "Thinking About Variable Types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\npclass\nsurvived\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\nAllen, Miss. Elisabeth Walton\n1\n1\nfemale\n29.0000\n0\n0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\nAllison, Master. Hudson Trevor\n1\n1\nmale\n0.9167\n1\n2\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\nAllison, Miss. Helen Loraine\n1\n0\nfemale\n2.0000\n1\n2\n113781\n151.5500\nC22 C26\nS\nNA\nNaN\nMontreal, PQ / Chesterville, ON\n\n\nAllison, Mr. Hudson Joshua Creighton\n1\n0\nmale\n30.0000\n1\n2\n113781\n151.5500\nC22 C26\nS\nNA\n135\nMontreal, PQ / Chesterville, ON\n\n\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\n1\n0\nfemale\n25.0000\n1\n2\n113781\n151.5500\nC22 C26\nS\nNA\nNaN\nMontreal, PQ / Chesterville, ON\n\n\nAnderson, Mr. Harry\n1\n1\nmale\n48.0000\n0\n0\n19952\n26.5500\nE12\nS\n3\nNaN\nNew York, NY",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#accessing-rows-and-columns",
    "href": "slides/02-conditional-distributions.html#accessing-rows-and-columns",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Accessing Rows and Columns",
    "text": "Accessing Rows and Columns\n\n\n\ndf.iloc[5,]\n\nname         Anderson, Mr. Harry\npclass                         1\nsurvived                       1\nsex                         male\nage                         48.0\nsibsp                          0\nparch                          0\nticket                     19952\nfare                       26.55\ncabin                        E12\nembarked                       S\nboat                           3\nbody                         NaN\nhome.dest           New York, NY\nName: 5, dtype: object\n\n\n\n\ndf[\"name\"].head()\n\n0                      Allen, Miss. Elisabeth Walton\n1                     Allison, Master. Hudson Trevor\n2                       Allison, Miss. Helen Loraine\n3               Allison, Mr. Hudson Joshua Creighton\n4    Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\nName: name, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#quick-summary-of-quantitative-variables",
    "href": "slides/02-conditional-distributions.html#quick-summary-of-quantitative-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Quick Summary of Quantitative Variables",
    "text": "Quick Summary of Quantitative Variables\n\n\ndf.describe()\n\n               age        sibsp        parch         fare        body\ncount  1046.000000  1309.000000  1309.000000  1308.000000  121.000000\nmean     29.881135     0.498854     0.385027    33.295479  160.809917\nstd      14.413500     1.041658     0.865560    51.758668   97.696922\nmin       0.166700     0.000000     0.000000     0.000000    1.000000\n25%      21.000000     0.000000     0.000000     7.895800   72.000000\n50%      28.000000     0.000000     0.000000    14.454200  155.000000\n75%      39.000000     1.000000     0.000000    31.275000  256.000000\nmax      80.000000     8.000000     9.000000   512.329200  328.000000",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#summarizing-categorical-variables",
    "href": "slides/02-conditional-distributions.html#summarizing-categorical-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Summarizing Categorical Variables",
    "text": "Summarizing Categorical Variables\nThe list of percents for each category is called the distribution of the variable.\n\ndf[\"pclass\"].value_counts()\n\npclass\n3    709\n1    323\n2    277\nName: count, dtype: int64\n\ndf[\"pclass\"].value_counts(normalize = True)\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#the-grammar-of-graphics",
    "href": "slides/02-conditional-distributions.html#the-grammar-of-graphics",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe grammar of graphics is a framework for creating data visualizations.\n\n\n\nA visualization consists of:\n\nThe aesthetic: Which variables are dictating which plot elements.\nThe geometry: What shape of plot you are making.\nThe theme: Other choices about the appearance.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#example",
    "href": "slides/02-conditional-distributions.html#example",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Example",
    "text": "Example\n\n\n\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\nfrom plotnine import ggplot, geom_point, aes, geom_boxplot\n\npenguins = load_penguins()\n\n(\n  ggplot(data = penguins, mapping = aes(x = \"species\", \n                                        y = \"bill_length_mm\", \n                                        fill = \"sex\")\n        ) +\n  geom_boxplot()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAesthetics\nWhere are variables mapped to aspects of the plot?\n\n\nGeometry\nWhat shape(s) are used to represent the data / observations?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#plotnine",
    "href": "slides/02-conditional-distributions.html#plotnine",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "plotnine",
    "text": "plotnine\nThe plotnine library implements the grammar of graphics in Python.\n\nThe aes() function is the place to map variables to plot aesthetics.\n\nx, y, and fill are three possible aesthetics that can be specified\n\nA variety of geom_XXX() functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)\n\nAesthetics can differ based on the geom you choose!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#themes",
    "href": "slides/02-conditional-distributions.html#themes",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Themes",
    "text": "Themes\n\n\n\n\n\nCode\n(\n  ggplot(data = penguins, mapping = aes(x = \"species\", \n                                        y = \"bill_length_mm\", \n                                        fill = \"sex\")\n         ) + \n  geom_boxplot()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom plotnine import theme_bw\n\n(\n  ggplot(penguins, aes(x = \"species\", \n                       y = \"bill_length_mm\", \n                       fill = \"sex\")\n                       ) + \n  geom_boxplot() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#check-in",
    "href": "slides/02-conditional-distributions.html#check-in",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Check-In",
    "text": "Check-In\nWhat are the aesthetics and geometry in the cartoon plot below?\n\nAn XKCD comic",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#bar-plots",
    "href": "slides/02-conditional-distributions.html#bar-plots",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo visualize the distribution of a categorical variable, we should use a bar plot.\n\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\")) + \n  geom_bar() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#calculating-percents",
    "href": "slides/02-conditional-distributions.html#calculating-percents",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Calculating Percents",
    "text": "Calculating Percents\n\npclass_dist = (\n  df['pclass']\n  .value_counts(normalize = True)\n  .reset_index()\n  )\n  \npclass_dist\n\n  pclass  proportion\n0      3    0.541635\n1      1    0.246753\n2      2    0.211612",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#percents-on-plots",
    "href": "slides/02-conditional-distributions.html#percents-on-plots",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Percents on Plots",
    "text": "Percents on Plots\n\n\n\nCode\n(\n  ggplot(data = pclass_dist, \n         mapping = aes(x = \"pclass\", y = \"proportion\")) + \n  geom_col() + ### notice this change to a column plot!\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nTechnically, you could still use geom_bar(), but you would need to specify that you didn’t want it to use stat = \"count\" (the default). You’ve already calculated the proportions, so you would use geom_bar(stat = \"identity\").",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot",
    "href": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 1: Stacked Bar Plot",
    "text": "Option 1: Stacked Bar Plot\n\n\n\nCode\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"stack\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot-1",
    "href": "slides/02-conditional-distributions.html#option-1-stacked-bar-plot-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 1: Stacked Bar Plot",
    "text": "Option 1: Stacked Bar Plot\n\nWhat are some pros and cons of the stacked bar plot?\n\n\n\n\nPros\n\n\nWe can still see the total counts in each class\nWe can easily compare the male counts in each class, since those bars are on the bottom.\n\n\n\n\n\n\n\nCons\n\n\nIt is hard to compare the female counts, since those bars are stacked on top.\nIt is hard to estimate the distributions.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot",
    "href": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 2: Side-by-Side Bar Plot",
    "text": "Option 2: Side-by-Side Bar Plot\n\n\n\nCode\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"dodge\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot-1",
    "href": "slides/02-conditional-distributions.html#option-2-side-by-side-bar-plot-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 2: Side-by-side Bar Plot",
    "text": "Option 2: Side-by-side Bar Plot\n\nWhat are some pros and cons of the side-by-side bar plot?\n\n\n\n\nPros\n\n\nWe can easily compare the female counts in each class.\nWe can easily compare the male counts in each class.\nWe can easily see counts of each within each class.\n\n\n\n\n\n\n\nCons\n\n\nIt is hard to see total counts in each class.\nIt is hard to estimate the distributions.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot",
    "href": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 3: Stacked Percentage Bar Plot",
    "text": "Option 3: Stacked Percentage Bar Plot\n\n\n\nCode\n(\n  ggplot(data = df, mapping = aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot-1",
    "href": "slides/02-conditional-distributions.html#option-3-stacked-percentage-bar-plot-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Option 3: Stacked Percentage Bar Plot",
    "text": "Option 3: Stacked Percentage Bar Plot\n\nWhat are some pros and cons of the stacked percentage bar plot?\n\n\n\n\nPros\n\n\nThis is the best way to compare sex balance across classes!\nThis is the option I use the most, because it can answer “Are you more likely to find ______ in ______ ?” type questions.\n\n\n\n\n\n\n\nCons\n\n\nWe can no longer see any counts!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#activity-1.2",
    "href": "slides/02-conditional-distributions.html#activity-1.2",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Activity 1.2",
    "text": "Activity 1.2\n\nChoose one of the plots from lecture so far and “upgrade” it.\n\n\nYou can do this by:\n\nFinding and using a different theme\nUsing labs() to change the axis labels\nTrying different variables\nTrying a different geometries\nUsing + scale_fill_manual() to change the colors being used\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou will need to use documentation of plotnine and online resources!\nCheck out https://www.data-to-viz.com/ for ideas and example code.\nAsk GenAI questions like, “What do I add to a plotnine bar plot to change the colors?” (But of course, make sure you understand the code you use!)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#two-categorical-variables",
    "href": "slides/02-conditional-distributions.html#two-categorical-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Two Categorical Variables",
    "text": "Two Categorical Variables\n\ndf[[\"pclass\", \"sex\"]].value_counts()\n\npclass  sex   \n3       male      493\n        female    216\n1       male      179\n2       male      171\n1       female    144\n2       female    106\nName: count, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#two-way-table",
    "href": "slides/02-conditional-distributions.html#two-way-table",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Two-way Table",
    "text": "Two-way Table\n\n\n(\n  df[[\"pclass\", \"sex\"]]\n  .value_counts()\n  .unstack()\n  )\n\n\nsex     female  male\npclass              \n1          144   179\n2          106   171\n3          216   493\n\n\n\nThis is sometimes called a cross-tab or cross-tabulation.\n\n\n\n\n\n\nPivot Table\n\n\nEssentially unstack() has pivoted the sex column from long format (where the values are included in one column) to wide format where each value has its own column.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#two-way-table---percents",
    "href": "slides/02-conditional-distributions.html#two-way-table---percents",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Two-way Table - Percents",
    "text": "Two-way Table - Percents\n\n(\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623\n\n\n\nAll of these values should sum to 1, aka, 100%!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#switching-variable-order",
    "href": "slides/02-conditional-distributions.html#switching-variable-order",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Switching Variable Order",
    "text": "Switching Variable Order\nWhat cross-tabulation would you expect if we changed the order of the variables? In other words, what would happen if \"sex\" came first and \"pclass\" came second?\n\n\n\n(\n  df[[\"sex\", \"pclass\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n\npclass         1         2         3\nsex                                 \nfemale  0.110008  0.080978  0.165011\nmale    0.136746  0.130634  0.376623",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#interpretation",
    "href": "slides/02-conditional-distributions.html#interpretation",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\nWe call this the joint distribution of the two variables.\n\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623\n\n\n\n\nOf all the passengers on the Titanic, 11% were female passengers riding in first class.\n\n\n\nNOT “11% of all females on Titanic…”\nNOT “11% of all first class passengers…”",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#conditional-distribution-from-counts",
    "href": "slides/02-conditional-distributions.html#conditional-distribution-from-counts",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Conditional Distribution from Counts",
    "text": "Conditional Distribution from Counts\nWe know that:\n\n466 passengers identified as female\nOf those 466 passengers, 144 rode in first class\n\n\nSo:\n\n144 / 466 = 31% of female identifying passengers rode in first class\n\nHere we conditioned on the passenger being female, and then looked at the conditional distribution of pclass.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#conditional-distribution-from-percentages",
    "href": "slides/02-conditional-distributions.html#conditional-distribution-from-percentages",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Conditional Distribution from Percentages",
    "text": "Conditional Distribution from Percentages\nWe know that:\n\n35.5% of all passengers identified as female\nOf those 35.5% of passengers, 11% rode in first class\n\nSo:\n\n0.11 / 0.355 = 31% of female identifying passengers rode in first class\n\nHere we conditioned on the passenger being female, and then looked at the conditional distribution of pclass.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#swapping-variables",
    "href": "slides/02-conditional-distributions.html#swapping-variables",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Swapping Variables",
    "text": "Swapping Variables\nWe know that:\n\n323 passengers rode in first class\nOf those 323 passengers, 144 identified as female\n\nSo:\n\n144 / 323 = 44.6% of first class passengers identified as female\n\nHere we conditioned on the passenger being in first class, and then looked at the conditional distribution of sex.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#which-one-to-condition-on",
    "href": "slides/02-conditional-distributions.html#which-one-to-condition-on",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Which one to condition on?",
    "text": "Which one to condition on?\nThis depends on the research question you are trying to answer.\n\n\n“What class did most female identifying passengers ride in?”\n\n-&gt; Of all female passengers, what is the conditional distribution of class?\n\n\n\n“What was the gender breakdown of first class?”\n\n-&gt; Of all first class passengers, what is the conditional distribution of sex?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#calculating-in-python",
    "href": "slides/02-conditional-distributions.html#calculating-in-python",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Calculating in Python",
    "text": "Calculating in Python\n\nWhen we study two variables, we call the individual one-variable distributions the marginal distribution of that variable.\n\n\n\n\nmarginal_class = (\n  df['pclass']\n  .value_counts(normalize = True)\n  )\n\n\n\nmarginal_class\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64\n\n\n\n\n\n\nmarginal_sex = (\n  df['sex']\n  .value_counts(normalize = True)\n  )\n\n\n\nmarginal_sex\n\nsex\nmale      0.644003\nfemale    0.355997\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#calculating-in-python-1",
    "href": "slides/02-conditional-distributions.html#calculating-in-python-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Calculating in Python",
    "text": "Calculating in Python\n\nWe need to divide the joint distribution (e.g. “11% of passengers were first class female”) by the marginal distribution of the variable we want to condition on (e.g. 35.5% of passengers were female).\n\n\njoint_class_sex = (\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n  \njoint_class_sex.divide(marginal_sex)\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#check-in-1",
    "href": "slides/02-conditional-distributions.html#check-in-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Check-In",
    "text": "Check-In\n\n\n\nmarginal_sex\n\nsex\nmale      0.644003\nfemale    0.355997\nName: proportion, dtype: float64\n\n\n\n\n\n\njoint_class_sex\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623\n\n\n\n\n\n\n\njoint_class_sex.divide(marginal_sex)\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816\n\n\n\n\n\nHow do you think divide() works?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#check-in-2",
    "href": "slides/02-conditional-distributions.html#check-in-2",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Check-In",
    "text": "Check-In\nShould the rows or columns add up to 100%? Why?\n\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#conditional-on-class",
    "href": "slides/02-conditional-distributions.html#conditional-on-class",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Conditional on Class",
    "text": "Conditional on Class\n\njoint_class_sex = (\n  df[[\"sex\", \"pclass\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n  \njoint_class_sex.divide(marginal_class)\n\npclass        1         2         3\nsex                                \nfemale  0.44582  0.382671  0.304654\nmale    0.55418  0.617329  0.695346",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#what-if-you-get-it-backwards",
    "href": "slides/02-conditional-distributions.html#what-if-you-get-it-backwards",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "What if you get it backwards?",
    "text": "What if you get it backwards?\n\njoint_class_sex = (\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize = True)\n  .unstack()\n  )\n  \njoint_class_sex.divide(marginal_class)\n\n         1   2   3  female  male\npclass                          \n1      NaN NaN NaN     NaN   NaN\n2      NaN NaN NaN     NaN   NaN\n3      NaN NaN NaN     NaN   NaN",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#which-plot-better-answers-the-question",
    "href": "slides/02-conditional-distributions.html#which-plot-better-answers-the-question",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Which plot better answers the question:",
    "text": "Which plot better answers the question:\n\n“Did women tend to ride in first class more than men?”\n\n\n\n\n\nCode\n(\n  ggplot(df, aes(x = \"pclass\", fill = \"sex\")) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n  ggplot(df, aes(x = \"sex\", fill = \"pclass)) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/02-conditional-distributions.html#takeaways-1",
    "href": "slides/02-conditional-distributions.html#takeaways-1",
    "title": "Visualizing and Comparing Categorical Variables",
    "section": "Takeaways",
    "text": "Takeaways\n\n\n\nWe use plotnine and the grammar of graphics to make visuals.\nFor two categorical variables, we might use a stacked bar plot, a side-by-side bar plot, or a stacked percentage bar plot - depending on what we are trying to show.\nThe joint distribution of two variables gives the percents in each subcategory.\nThe marginal distribution of a variable is its individual distribution.\nThe conditional distribution of a variable is its distribution among only one category of a different variable.\nWe calculate the conditional distribution by dividing the joint by the marginal.",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 2 - Visualizing and Comparing Categorical Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#changes-from-week-1",
    "href": "slides/03-quantitative-variables.html#changes-from-week-1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Changes from Week 1",
    "text": "Changes from Week 1\n\nWeek 1 taught me that I need to make some adjustments!\n\n\n\n\n\nLab Attendance\n\n\nIs not required. However, if you do not attend lab and come to student hours or post questions on Discord about the lab, I will be displeased.\n\n\n\n\n\n\n\nDeadlines\n\n\n\n\nLabs are due the day after lab (like lecture quizzes / activities).\n\nTuesday’s lab is due on Wednesday at 11:59pm.\nThursday’s lab is due on Friday at 11:59pm.\n\nEnd of week assignments are due on Sundays at 11:59pm (not Saturdays).\n\n\n\n\n\n\n\n\n\nLab Submissions\n\n\n\nPDFs will be required for every Collab submission.\n\nYour code cannot be more than 80 characters—where the grey line appears in Collab.\nUse returns!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#getting-prepping-and-summarizing-data",
    "href": "slides/03-quantitative-variables.html#getting-prepping-and-summarizing-data",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Getting, prepping, and summarizing data",
    "text": "Getting, prepping, and summarizing data\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\ndf[\"pclass\"] = df[\"pclass\"].astype(\"category\")\ndf[\"survived\"] = df[\"survived\"].astype(\"category\")",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#marginal-distributions",
    "href": "slides/03-quantitative-variables.html#marginal-distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Marginal Distributions",
    "text": "Marginal Distributions\nIf I choose a passenger at random, what is the probability they rode in 1st class?\n\nmarginal_class = (\n  df['pclass']\n  .value_counts(normalize = True)\n  )\nmarginal_class\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#joint-distributions",
    "href": "slides/03-quantitative-variables.html#joint-distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Joint Distributions",
    "text": "Joint Distributions\nIf I choose a passenger at random, what is the probability they are a woman who rode in first class?\n\njoint_class_sex = (\n  df[[\"pclass\", \"sex\"]]\n  .value_counts(normalize=True)\n  .unstack()\n  )\n  \njoint_class_sex\n\nsex       female      male\npclass                    \n1       0.110008  0.136746\n2       0.080978  0.130634\n3       0.165011  0.376623",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#conditional-distributions",
    "href": "slides/03-quantitative-variables.html#conditional-distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nIf I choose a woman at random, what is the probability they rode in first class?\n\nmarginal_sex = (\n  df['sex']\n  .value_counts(normalize = True)\n  )\n  \njoint_class_sex.divide(marginal_sex)\n\nsex       female      male\npclass                    \n1       0.309013  0.212337\n2       0.227468  0.202847\n3       0.463519  0.584816",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#visualizing-with-plotnine",
    "href": "slides/03-quantitative-variables.html#visualizing-with-plotnine",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Visualizing with plotnine",
    "text": "Visualizing with plotnine\n\n\n(\n  ggplot(df, aes(x = \"sex\", fill = \"pclass\")) + \n  geom_bar(position = \"fill\") + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#quantitative-variables-1",
    "href": "slides/03-quantitative-variables.html#quantitative-variables-1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\nWe have analyzed a quantitative variable already. Where?\nIn the Colombia COVID data!\n\n\ndf_CO = pd.read_csv(\"http://dlsun.github.io/pods/data/covid/colombia_2020-05-28.csv\")\ndf_CO\n\n            Departamento  Edad  ... Fecha de diagnóstico Fecha recuperado\n0            Bogotá D.C.    19  ...           2020-03-06       2020-03-13\n1        Valle del Cauca    34  ...           2020-03-09       2020-03-19\n2              Antioquia    50  ...           2020-03-09       2020-03-15\n3              Antioquia    55  ...           2020-03-11       2020-03-26\n4              Antioquia    25  ...           2020-03-11       2020-03-23\n...                  ...   ...  ...                  ...              ...\n25361  Buenaventura D.E.    48  ...           2020-05-28              NaN\n25362    Valle del Cauca    55  ...           2020-05-28              NaN\n25363  Buenaventura D.E.    39  ...           2020-05-28              NaN\n25364    Valle del Cauca    13  ...           2020-05-28              NaN\n25365            Córdoba     0  ...           2020-05-28              NaN\n\n[25366 rows x 10 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#option-1-convert-it-to-categorical",
    "href": "slides/03-quantitative-variables.html#option-1-convert-it-to-categorical",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Option 1: Convert it to categorical",
    "text": "Option 1: Convert it to categorical\nTo visualize the age variable, we did the following:\n\ndf_CO[\"age\"] = pd.cut(\n    df_CO[\"Edad\"],\n    bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 120],\n    labels = [\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80+\"],\n    right = False, \n    ordered = True)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#option-1-then-make-a-barplot",
    "href": "slides/03-quantitative-variables.html#option-1-then-make-a-barplot",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Option 1: Then make a barplot",
    "text": "Option 1: Then make a barplot\nThen, we could treat age as categorical and make a barplot:\n\n\n\nCode\n(\n  ggplot(df_CO, aes(x = \"age\")) + \n  geom_bar() + \n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#option-2-treat-it-as-a-quantitative-variable",
    "href": "slides/03-quantitative-variables.html#option-2-treat-it-as-a-quantitative-variable",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Option 2: Treat it as a quantitative variable!",
    "text": "Option 2: Treat it as a quantitative variable!\nA histogram uses equal sized bins to summarize a quantitative variable.\n\n\n\nCode\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram() + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#changing-binwidth",
    "href": "slides/03-quantitative-variables.html#changing-binwidth",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Changing Binwidth",
    "text": "Changing Binwidth\nTo tweak your histogram, you can change the binwith:\n\n\n\n\n\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram(binwidth = 1) + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram(binwidth = 10) + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#adding-color-outline",
    "href": "slides/03-quantitative-variables.html#adding-color-outline",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Adding Color & Outline",
    "text": "Adding Color & Outline\n\n\n\nCode\n(\n  ggplot(df_CO, aes(x = \"Edad\")) + \n  geom_histogram(binwidth = 10, \n                 color = \"white\", \n                 fill = \"gray\") + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#using-percents-instead-of-counts",
    "href": "slides/03-quantitative-variables.html#using-percents-instead-of-counts",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Using Percents Instead of Counts",
    "text": "Using Percents Instead of Counts\n\n\n\nCode\n(\n  ggplot(df_CO, mapping = aes(x = \"Edad\")) + \n  geom_histogram(mapping = aes(y = '..density..'), \n                 binwidth = 10, \n                 color = \"white\", \n                 fill = \"gray\") + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Age Demographics of Columbia's Population (2020)\"\n       ) +\n  theme_bw()\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#distributions",
    "href": "slides/03-quantitative-variables.html#distributions",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Distributions",
    "text": "Distributions\n\nRecall the distribution of a categorical variable:\n\nWhat are the possible values and how common is each?\n\nThe distribution of a quantitative variable is similar:\n\nThe total area in the histogram is 1.0 (or 100%).",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#densities",
    "href": "slides/03-quantitative-variables.html#densities",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Densities",
    "text": "Densities\n\n\nIn this example, we have a limited set of possible values for age: 0, 1, 2, …., 100.\n\nWe call this a discrete variable.\n\n\n\n\n\n\nWhat if had a quantitative variable with infinite values?\n\nFor example: Price of a ticket on Titanic.\nWe call this a continuous variable.\n\n\n\n\n\n\n\nIn this case, it is not possible to list all possible values and how likely each one is.\n\nOne person paid $2.35\nTwo people paid $12.50\nOne person paid $34.98\n\\(\\vdots\\)\n\n\n\n\n\n\n\nInstead, we talk about ranges of values.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#densities-1",
    "href": "slides/03-quantitative-variables.html#densities-1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Densities",
    "text": "Densities\nAbout what percent of people in this dataset are below 18?\n\n\n\nCode\n(\n  ggplot(data = df_CO, mapping = aes(x = \"Edad\")) + \n  geom_histogram(mapping = aes(y = '..density..'), \n                 bins = 10) + \n  geom_vline(xintercept = 18, \n             color = \"red\", \n             size = 2, \n             linetype = \"dashed\") +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nHow would you code it?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#mean",
    "href": "slides/03-quantitative-variables.html#mean",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Mean",
    "text": "Mean\n\n\nOne summary of the center of a quantitative variable is the mean.\nWhen you hear “The average age is…” or the “The average income is…”, this probably refers to the mean.\nSuppose we have five people, ages: 4, 84, 12, 27, 7\nThe mean age is: \\[(4 + 84 + 12 + 27 + 7) / 5 = 134 / 5 = 26.8\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#notation-interlude",
    "href": "slides/03-quantitative-variables.html#notation-interlude",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Notation Interlude",
    "text": "Notation Interlude\n\n\n\nTo refer to our data without having to list all the numbers, we use \\(x_1, x_2, ..., x_n\\)\nIn the previous example, \\(x_1 = 4, x_2 = 84, x_3 = 12, x_4 = 27, x_5 = 7\\). So, \\(n = 5\\).\nTo add up all the numbers, we use the summation notation: \\[ \\sum_{i = 1}^5 x_i = 134\\]\nTherefore, the mean is: \\[\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#means-in-python",
    "href": "slides/03-quantitative-variables.html#means-in-python",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Means in Python",
    "text": "Means in Python\nLong version: find the sum and the number of observations\n\nsum_age = df_CO[\"Edad\"].sum()\nn = len(df_CO)\n\nsum_age / n\n\nnp.float64(39.04742568792872)\n\n\n\nShort version: use the built-in .mean() function!\n\ndf_CO[\"Edad\"].mean()\n\nnp.float64(39.04742568792872)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#activity-2.1",
    "href": "slides/03-quantitative-variables.html#activity-2.1",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Activity 2.1",
    "text": "Activity 2.1\nThe mean is only one option for summarizing the center of a quantitative variable. It isn’t perfect!\nLet’s investigate this.\n\nOpen the Activity 2.1 Collab notebook\nRead in the Titanic data\nPlot the density of ticket prices on titanic\nCalculate the mean price\nSee how many people paid more than mean price",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#what-happened",
    "href": "slides/03-quantitative-variables.html#what-happened",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "What happened",
    "text": "What happened\n\nOur fare data was skewed right: Most values were small, but a few values were very large.\nThese large values “pull” the mean up; just how the value 84 pulled the average age up in our previous example.\nSo, why do we like the mean?",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#squared-error",
    "href": "slides/03-quantitative-variables.html#squared-error",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Squared Error",
    "text": "Squared Error\n\n\nRecall: Ages 4, 84, 12, 27, 7.\n\n\nages = np.array([4, 84, 12, 27, 7])\n\n\nImagine that we had to “guess” the age of the next person.\n\n\n\n\n\n\n\nIf we guess 26.8, then our “squared error” for these five people is:\n\n\nsq_error = (ages - 26.8) ** 2\n\n(\n  sq_error\n  .round(decimals = 1)\n  .sum()\n  )\n\nnp.float64(4402.6)\n\n\n\n\n\n\n\n\n\n\nIf we guess 20, then our “squared error” for these five people is:\n\n\nsq_error = (ages - 20) ** 2\n(\n  sq_error\n  .round(decimals = 1)\n  .sum()\n  )\n\nnp.int64(4634)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#minimizing-squared-error",
    "href": "slides/03-quantitative-variables.html#minimizing-squared-error",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Minimizing squared error",
    "text": "Minimizing squared error\n\n\n\nCode\ncs = range(1, 60)\nsum_squared_distances = []\n\nfor c in cs:\n  (\n    sum_squared_distances\n    .append(\n      (\n        (df_CO[\"Edad\"] - c) ** 2\n      )\n      .sum()\n      )\n\nres_df = pd.DataFrame({\"center\": cs, \"sq_error\": sum_squared_distances})\n\n(\n  ggplot(res_df, aes(x = 'center', y = 'sq_error')) + \n  geom_line() +\n  labs(x = \"Mean\", \n       y = \"\", \n       title = \"Changes in Sum of Squared Error Based on Choice of Center\")\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#median",
    "href": "slides/03-quantitative-variables.html#median",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Median",
    "text": "Median\n\nAnother summary of center is the median, which is the “middle” of the sorted values.\nTo calculate the median of a quantitative variable with values \\(x_1, x_2, x_3, ..., x_n\\), we do the following steps:\n\n\n\n\nSort the values from smallest to largest: \\[x_{(1)}, x_{(2)}, x_{(3)}, ..., x_{(n)}.\\]\nThe “middle” value depends on whether we have an odd or an even number of observations.\n\nIf \\(n\\) is odd, then the middle value is \\(x_{(\\frac{n + 1}{2})}\\).\nIf \\(n\\) is even, then there are two middle values, \\(x_{(\\frac{n}{2})}\\) and \\(x_{(\\frac{n}{2} + 1)}\\).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is conventional to report the mean of the two values (but you can actually pick any value between them)!",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#median-in-python",
    "href": "slides/03-quantitative-variables.html#median-in-python",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Median in Python",
    "text": "Median in Python\nAges: 4, 84, 12, 7, 27. What is the median?\nMedian age in the Columbia data:\n\ndf_CO[\"Edad\"].median()\n\nnp.float64(37.0)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#variance",
    "href": "slides/03-quantitative-variables.html#variance",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Variance",
    "text": "Variance\n\n\n\nOne measure of spread is the variance.\nThe variance of a variable whose values are \\(x_1, x_2, x_3, ..., x_n\\) is calculated using the formula \\[\\textrm{var(X)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\]\n\n\n\n\n\n\n\n\n\n\nDoes this look familiar?\n\n\nIt’s the sum of squared error! Well, divided by \\(n-1\\), the “degrees of freedom”.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#variance-in-python",
    "href": "slides/03-quantitative-variables.html#variance-in-python",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Variance in Python",
    "text": "Variance in Python\nSimilar to calculating the mean, we could find the variance manually:\n\n(\n  ((df_CO[\"Edad\"] - df_CO[\"Edad\"].mean()) ** 2)\n  .sum() / (len(df_CO) - 1)\n  )\n\nnp.float64(348.0870469898451)\n\n\n\n…or using a built-in Python function.\n\ndf_CO[\"Edad\"].var()\n\nnp.float64(348.0870469898451)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#standard-deviation",
    "href": "slides/03-quantitative-variables.html#standard-deviation",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nNotice that the variance isn’t very intuitive. What do we mean by “The spread is 348”?\nThis is because it is the squared error!\n\n\n\n\n\nTo get it in more interpretable language, we can take the square root:\n\n\nnp.sqrt(df_CO[\"Edad\"].var())\n\nnp.float64(18.65709106452142)\n\n\n\n\n\n\nOr, we use the built-in function!\n\ndf_CO[\"Edad\"].std()\n\nnp.float64(18.65709106452142)",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/03-quantitative-variables.html#takeaway-messages",
    "href": "slides/03-quantitative-variables.html#takeaway-messages",
    "title": "Visualizing and Summarizing Quantitative Variables",
    "section": "Takeaway Messages",
    "text": "Takeaway Messages\n\n\n\nVisualize quantitative variables with histograms or densities.\nSummarize the center of a quantitative variable with mean or median.\nDescribe the shape of a quantitative variable with skew.\nSummarize the spread of a quantitative variable with the variance or the standard deviation.",
    "crumbs": [
      "Lecture Slides",
      "Week 2, Part 1 - Visualizing and Summarizing Quantitative Variables"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#data-are-stored-in-plain-text-files",
    "href": "slides/01-tabular-data-summaries.html#data-are-stored-in-plain-text-files",
    "title": "Tabular Data and Variable Summaries",
    "section": "Data are stored in plain text files",
    "text": "Data are stored in plain text files\n\nname,pclass,survived,sex,age,sibsp,parch,ticket,fare,cabin,embarked,boat,body,home.dest\n\"Allen, Miss. Elisabeth Walton\",1,1,female,29,0,0,24160,211.3375,B5,S,2,,\"St Louis, MO\"\n\"Allison, Master. Hudson Trevor\",1,1,male,0.9167,1,2,113781,151.5500,C22 C26,S,11,,\"Montreal, PQ / Chesterville, ON\"\n\"Allison, Miss. Helen Loraine\",1,0,female,2,1,2,113781,151.5500,C22 C26,S,,,\"Montreal, PQ / Chesterville, ON\"\n\"Allison, Mr. Hudson Joshua Creighton\",1,0,male,30,1,2,113781,151.5500,C22 C26,S,,135,\"Montreal, PQ / Chesterville, ON\"\n\"Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\",1,0,female,25,1,2,113781,151.5500,C22 C26,S,,,\"Montreal, PQ / Chesterville, ON\"\n\"Anderson, Mr. Harry\",1,1,male,48,0,0,19952,26.5500,E12,S,3,,\"New York, NY\"\n\"Andrews, Miss. Kornelia Theodosia\",1,1,female,63,1,0,13502,77.9583,D7,S,10,,\"Hudson, NY\"\n\"Andrews, Mr. Thomas Jr\",1,0,male,39,0,0,112050,0.0000,A36,S,,,\"Belfast, NI\"\n\"Appleton, Mrs. Edward Dale (Charlotte Lamson)\",1,1,female,53,2,0,11769,51.4792,C101,S,D,,\"Bayside, Queens, NY\"\n\"Artagaveytia, Mr. Ramon\",1,0,male,71,0,0,PC 17609,49.5042,,C,,22,\"Montevideo, Uruguay\"\n\"Astor, Col. John Jacob\",1,0,male,47,1,0,PC 17757,227.5250,C62 C64,C,,124,\"New York, NY\"\n\n\n\nThis is called a csv (comma-separated) file.\nYou might see it stored as something.csv or something.txt\n.txt files might have different delimiters (separators)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#reading-data",
    "href": "slides/01-tabular-data-summaries.html#reading-data",
    "title": "Tabular Data and Variable Summaries",
    "section": "Reading data",
    "text": "Reading data\nWe read the data into a program like Python by specifying:\n\nwhat type of file it is (e.g., .csv, .txt, .xlsx)\nwhere the csv file is located (the “path”)\nif the file has a header\n… and other information in special cases!",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#example-using-pandas-data-frame",
    "href": "slides/01-tabular-data-summaries.html#example-using-pandas-data-frame",
    "title": "Tabular Data and Variable Summaries",
    "section": "Example using pandas data frame:",
    "text": "Example using pandas data frame:\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\n\n\n\n\n\n\nread_csv() lives in pandas\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n                                              name  ...                        home.dest\n0                    Allen, Miss. Elisabeth Walton  ...                     St Louis, MO\n1                   Allison, Master. Hudson Trevor  ...  Montreal, PQ / Chesterville, ON\n2                     Allison, Miss. Helen Loraine  ...  Montreal, PQ / Chesterville, ON\n3             Allison, Mr. Hudson Joshua Creighton  ...  Montreal, PQ / Chesterville, ON\n4  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  ...  Montreal, PQ / Chesterville, ON\n\n[5 rows x 14 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in",
    "href": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in",
    "title": "Tabular Data and Variable Summaries",
    "section": "Lecture 1.1 Check in",
    "text": "Lecture 1.1 Check in\n\n\ndf = pd.read_csv(\"https://datasci112.stanford.edu/data/titanic.csv\")\n\n\n\n\n\nQuestion 1: What if this file lived on a computer instead of online?\nQuestion 2: Why didn’t we have to specify that this dataset has a header?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#looking-at-the-rows",
    "href": "slides/01-tabular-data-summaries.html#looking-at-the-rows",
    "title": "Tabular Data and Variable Summaries",
    "section": "Looking at the rows",
    "text": "Looking at the rows\n\n\n\n\ndf.loc[1, :]\n\nname          Allison, Master. Hudson Trevor\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: 1, dtype: object\n\n\n\n\n\n\n\n\ndf.iloc[1, :]\n\nname          Allison, Master. Hudson Trevor\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: 1, dtype: object\n\n\n\n\n\n\nWhat is the difference between .loc and .iloc?\nWhat type of object is returned?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#loc-iloc-and-index",
    "href": "slides/01-tabular-data-summaries.html#loc-iloc-and-index",
    "title": "Tabular Data and Variable Summaries",
    "section": "loc, iloc, and index",
    "text": "loc, iloc, and index\n\ndf2 = df.set_index('name')\n\n\n\n\n                                                 pclass  ...                        home.dest\nname                                                     ...                                 \nAllen, Miss. Elisabeth Walton                         1  ...                     St Louis, MO\nAllison, Master. Hudson Trevor                        1  ...  Montreal, PQ / Chesterville, ON\nAllison, Miss. Helen Loraine                          1  ...  Montreal, PQ / Chesterville, ON\nAllison, Mr. Hudson Joshua Creighton                  1  ...  Montreal, PQ / Chesterville, ON\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)       1  ...  Montreal, PQ / Chesterville, ON\n\n[5 rows x 13 columns]\n\n\n\n\n\n\n\n\n\n\nWhy are there 13 columns now? (There were 14 before!)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-1",
    "href": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-1",
    "title": "Tabular Data and Variable Summaries",
    "section": "loc, iloc, and index",
    "text": "loc, iloc, and index\n\n\n\ndf2.loc[1, :]\n\nKeyError: 1\n\n\n\n\n\n\ndf2.iloc[1, :]\n\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: Allison, Master. Hudson Trevor, dtype: object\n\n\n\n\n\n\nWhy is .loc returning an error?\n\n\n\n\n\nWhy is .iloc not returning an error?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-2",
    "href": "slides/01-tabular-data-summaries.html#loc-iloc-and-index-2",
    "title": "Tabular Data and Variable Summaries",
    "section": "loc, iloc, and index",
    "text": "loc, iloc, and index\n\n\n.loc – label-based location\n\n\nUses labels from rows (rownames) to select data\n\n\n\ndf2.loc[\"Allison, Master. Hudson Trevor\", :]\n\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: Allison, Master. Hudson Trevor, dtype: object\n\n\n\n\n\n\n\n\n.iloc – integer location\n\n\nUses indices (positions) from rows to select data\n\n\n\ndf2.iloc[1, :]\n\npclass                                     1\nsurvived                                   1\nsex                                     male\nage                                   0.9167\nsibsp                                      1\nparch                                      2\nticket                                113781\nfare                                  151.55\ncabin                                C22 C26\nembarked                                   S\nboat                                      11\nbody                                     NaN\nhome.dest    Montreal, PQ / Chesterville, ON\nName: Allison, Master. Hudson Trevor, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#looking-at-columns",
    "href": "slides/01-tabular-data-summaries.html#looking-at-columns",
    "title": "Tabular Data and Variable Summaries",
    "section": "Looking at columns",
    "text": "Looking at columns\n\n\n\ndf.columns\n\nIndex(['name', 'pclass', 'survived', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n      dtype='object')\n\n\n\n\n\n\n\ndf['home.dest']\n\n0                          St Louis, MO\n1       Montreal, PQ / Chesterville, ON\n2       Montreal, PQ / Chesterville, ON\n3       Montreal, PQ / Chesterville, ON\n4       Montreal, PQ / Chesterville, ON\n                     ...               \n1304                                NaN\n1305                                NaN\n1306                                NaN\n1307                                NaN\n1308                                NaN\nName: home.dest, Length: 1309, dtype: object\n\n\n\n\n\n\n\n\n\n\n\nNaN (Not a Number) represents missing or null data",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#caution-object-types",
    "href": "slides/01-tabular-data-summaries.html#caution-object-types",
    "title": "Tabular Data and Variable Summaries",
    "section": "Caution: Object types",
    "text": "Caution: Object types\n\ntype(df)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\ntype(df.iloc[1, :])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\ntype(df['name'])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\n\nA Series is a one-dimensional labeled array (a vector with labels)",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#questions-to-ask",
    "href": "slides/01-tabular-data-summaries.html#questions-to-ask",
    "title": "Tabular Data and Variable Summaries",
    "section": "Questions to ask",
    "text": "Questions to ask\n\nWhich variables (columns) are categorical?\nWhich variables are quantitative?\nWhich variables are labels (e.g. names or ID numbers)?\nWhich variables are text?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#a-quick-look-at-the-data",
    "href": "slides/01-tabular-data-summaries.html#a-quick-look-at-the-data",
    "title": "Tabular Data and Variable Summaries",
    "section": "A quick look at the data",
    "text": "A quick look at the data\n\n\ndf.describe()\n\n            pclass     survived  ...         fare        body\ncount  1309.000000  1309.000000  ...  1308.000000  121.000000\nmean      2.294882     0.381971  ...    33.295479  160.809917\nstd       0.837836     0.486055  ...    51.758668   97.696922\nmin       1.000000     0.000000  ...     0.000000    1.000000\n25%       2.000000     0.000000  ...     7.895800   72.000000\n50%       3.000000     0.000000  ...    14.454200  155.000000\n75%       3.000000     1.000000  ...    31.275000  256.000000\nmax       3.000000     1.000000  ...   512.329200  328.000000\n\n[8 rows x 7 columns]\n\n\n\n\nLecture 1.1 Check in\n\n\n\nQuestion 3: What percent of Titanic passengers survived?\nQuestion 4: What was the average (mean) fare paid for a ticket?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#changing-variable-types",
    "href": "slides/01-tabular-data-summaries.html#changing-variable-types",
    "title": "Tabular Data and Variable Summaries",
    "section": "Changing Variable Types",
    "text": "Changing Variable Types\n\nThe variable pclass was categorical, but Python assumed it was quantitative.\nIt’s our job to check and fix data!\n\n\n\ndf[\"pclass\"] = df[\"pclass\"].astype(\"category\")\n\n\n\n\n\n\n\n\n\n\nWhy choose to store pclass as a \"category\" instead of a \"string\"?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#summary-of-categorical-variable",
    "href": "slides/01-tabular-data-summaries.html#summary-of-categorical-variable",
    "title": "Tabular Data and Variable Summaries",
    "section": "Summary of categorical variable",
    "text": "Summary of categorical variable\n\ndf[\"pclass\"].value_counts()\n\npclass\n3    709\n1    323\n2    277\nName: count, dtype: int64\n\n\n\n\n\ndf[\"pclass\"].value_counts(normalize = True)\n\npclass\n3    0.541635\n1    0.246753\n2    0.211612\nName: proportion, dtype: float64",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in-2",
    "href": "slides/01-tabular-data-summaries.html#lecture-1.1-check-in-2",
    "title": "Tabular Data and Variable Summaries",
    "section": "Lecture 1.1 Check in",
    "text": "Lecture 1.1 Check in\nQuestion 5: What percent of Titanic passengers were in First Class?\nQuestion 6: Which is the correct way to change a numeric column to a categorical variable?",
    "crumbs": [
      "Lecture Slides",
      "Week 1, Part 1 - Tabular Data and Variable Summaries"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#course-objectives",
    "href": "course-materials/syllabus.html#course-objectives",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Course Objectives",
    "text": "Course Objectives\nAfter taking this course, you will be able to:\n\nAcquire and process tabular, textual, hierarchical, and geospatial data.\nUncover patterns by summarizing and visualizing data.\nApply machine learning to answer real-world prediction problems.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#prerequisites",
    "href": "course-materials/syllabus.html#prerequisites",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Prerequisites",
    "text": "Prerequisites\nI expect you to enter this class with:\n\nBasic knowledge of Python and computer programming concepts.\nFamiliarity with computers and technology (e.g., Internet browsing, word processing, opening/saving files, converting files to PDF format, sending and receiving e-mail, etc.).\nA positive attitude, a curious mind, and a respect for ethical use of data science tools.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#lecture-check-ins-5",
    "href": "course-materials/syllabus.html#lecture-check-ins-5",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Lecture Check-Ins (5%)",
    "text": "Lecture Check-Ins (5%)\nEvery lecture will be accompanied by a simple “Check-In Quiz” on Canvas. This will ask you to input a few answers covered in lecture, and one short mid-lecture practice exercise. Infinite submissions are allowed without penalty, so there is no reason anyone should not get 100% in this category.\nIf you miss lecture, you can still complete the Check-Ins on your own time, by midnight the next day.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#lab-attendance-and-activities-10",
    "href": "course-materials/syllabus.html#lab-attendance-and-activities-10",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Lab Attendance and Activities (10%)",
    "text": "Lab Attendance and Activities (10%)\nAttendance and participation in the lab portion of class is required. Do not take this class if you cannot commit to attending every lecture and every lab.\n\nSwapping Lab Sections\nIf you cannot make it to your assigned lab, but you can attend another section’s lab on the same day, please e-mail Dr. Theobold to inform them that you wish to swap lab sections. You may do this two times throughout the quarter.\n\n\nMissing Lab\nIf you cannot make it to any lab sections that day, you may complete the Colab assignment from lab on your own, and e-mail a PDF to Dr. Theobold by midnight that night (the day you missed class). Dr. Theobold will grade your Colab, and this score will replace your attendance for that day. You may do this two times throughout the quarter.\nTo allow for emergencies, we will also forgive one absence at the end of the quarter.\n\n\n\n\n\n\nNote\n\n\n\nWe are effectively allowing you to miss / reschedule up to three labs out of 10. If you need to miss more than three, then you will need to retake this course next quarter.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#lab-assignments-25",
    "href": "course-materials/syllabus.html#lab-assignments-25",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Lab Assignments (25%)",
    "text": "Lab Assignments (25%)\nEach week, you will be assigned longer homeworks, which will ask you to analyze a real-world data scenario. These assignments are due every Saturday at 11:59pm. See the Late Work section below for information on extensions or deductions for late submissions.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#exams-15-each",
    "href": "course-materials/syllabus.html#exams-15-each",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Exams (15% each)",
    "text": "Exams (15% each)\nYou will have two in-class exams, in Week 5 and Week 10. These will each cover half of the class material. Except in very extreme unforeseeable circumstances, no alternate exams will be given; please plan to be in class these days.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#project-30",
    "href": "course-materials/syllabus.html#project-30",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Project (30%)",
    "text": "Project (30%)\nAt the end of the quarter, you will formally present a poster of your findings on a real-world data question. To ensure that you start thinking about your project early, you are required to submit a 1-page abstract proposal in Week 7.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#communication",
    "href": "course-materials/syllabus.html#communication",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Communication",
    "text": "Communication\nFor questions of general interest, such as deadline clarifications or conceptual questions, please use the Class Discord Server. You should check the relevant thread of the server, as well as the syllabus, before reaching out to Dr. Theobold.\nOf course, if your question is truly private, such as a grade inquiry or a personal concern, you may email me directly.\n\n\n\n\n\n\nNote\n\n\n\nIf you email me to ask a question that should be public, I will likely ask you to post your question on Discord instead. Please don’t take this personally! It just means that you asked a good question, and I think the rest of the class could benefit from seeing the answer.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#late-work",
    "href": "course-materials/syllabus.html#late-work",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Late Work",
    "text": "Late Work\nLate assignments will automatically be docked -10% per day, up to a maximum grade penalty of 50%. That is, as long as your work is turned in by the end of Week 10, you will still get half credit for it!\n\n\n\n\n\n\nNote\n\n\n\nNote that turning in an assignment late means you will not get any feedback, and using lecture or section time to work on late assignments will not be tolerated.\n\n\n\nDeadline Extensions\nIn case of emergency, you have two deadline “extensions” to use throughout the quarter on any Lab Assignment. This will grant you a 72-hour (3-day) extension.\nThe rules for these are as follows:\n\nYou must request the extension through the Google form linked on Canvas. Any other request (e.g., by email, Discord message, verbally, etc.) does not count unless the Google form is filled out.\nThe extension must be requested before the deadline has passed (i.e., before Friday at 11:59pm). I do not grant after-deadline extensions for any reason.\n\nProperly requested extensions are automatically granted; you will not get a confirmation email or message, you will simply see your late penalties disappear at some point.\n\n\n\n\n\n\nNote\n\n\n\nThese deadline extensions are automatic! You don’t have to tell why you need the extension - maybe you have a busy week with other work, maybe you are traveling with a sports team, maybe you partied too hard for your friend’s birthday. It doesn’t matter to me!\nThe flip side of this, though, is that if you use your deadline extensions early on in the course, and then run into a bigger issues later on, you’re out of luck.\n\n\n\n\nSpecial cases\nSometimes, issues arise require more time than the auto-extension gives. In general, if something comes up in your life, I always want to find a way to help. Please let me know what your situation is, and we’ll work together to find a good solution.\nThe most important thing is that you tell me early. As a rule, I do not grant extensions after the deadline.\n\n\n\n\n\n\nNote\n\n\n\nOf course, in the case of a major crisis, that is truly exceptional and unforeseen, all these rules go out the window. I want you to feel comfortable reaching out to me when you are facing something extra difficult. We’ll figure it out.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#academic-integrity",
    "href": "course-materials/syllabus.html#academic-integrity",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nYou are expected to abide by the Cal Poly Code of Conduct at all times.\n\nPlagiarism\nYou are encouraged to work with other classmates on all but the exam portions of this class. You are also encouraged (realistically, required!) to make use of online resources to accomplish tasks.\nWhen dealing with code, follow these guidelines:\n\nNever copy-paste more than small snippets of code. That is, you might borrow a little three-line function from StackOverflow, but you should not copy over a full analysis you find on Kaggle.\nAttribute all code that is not completely your own. If you do borrow that StackOverflow snippet, provide a link to the source. If you reference a similar analysis for ideas, mention that in your description.\n\n\n\n\n\n\n\nWarning\n\n\n\nA good “rule of thumb” is: If I sat you down by yourself in a room with no internet, could you explain to me roughly what each line of code is doing? If not, you are probably borrowing more than you should from your online source.\n(In fact, this is exactly what I will do if I need to investigate possible cheating.)\n\n\n\nAI tools\nNew AI models like Chat GPT offer a whole new world of online coding resources. This is exciting! You should absolutely feel free to get help from these tools, they are excellent at answering questions.\nHowever, from an academic integrity perspective, treat these AI generative chat resources like, say, a tutor. Asking a tutor to help explain a homework concept to you or help debug your code? Totally fine! Giving the tutor a homework question and having them answer the whole thing? Nope. Talking to a tutor at all, about anything, during the course of the exam? Unacceptable.\n\n\n\nIntegrity Violations\nIf you accidentally forget a small citation, or go a little overboard in how much you “borrow” from StackOverflow, you’ll get a warning and a grade deduction on that assignment.\nAny instance of willful and deliberate cheating will result in a failing grade on the assignment and I will file a academic integrity report with the Office of Student Rights and responsibilities.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about being on the giving end as well as the taking end. For example: If you send your finished assignment to a friend, and that friend copies it, you have both received a failing grade on the assignment and a report filed to OSRR.\n\n\n\n\nIntellectual property\nThe materials for this course are legally the professor’s intellectual property.\nMost class materials are publicly shared, and you are welcome to direct others to this resource at any time. You are also welcome to publicly share any or all of your work on the class project.\nNon-public class materials—most importantly, assignment solutions and any exam materials—may never be shared.\n\n\n\n\n\n\nWarning\n\n\n\nThis is not just an issue of academic honestly, it is quite literally a legal copyright scenario. Please do not distribute solutions or exam questions from this class anywhere, for any reason. Doing so is a violation of the Code of Conduct, and it may constitute a violation of U.S. copyright law.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-materials/syllabus.html#class-conduct",
    "href": "course-materials/syllabus.html#class-conduct",
    "title": "DATA 301: Introduction to Data Science",
    "section": "Class Conduct",
    "text": "Class Conduct\nIn this classroom, I expect you to be polite, respectful, inclusive, and open-minded.\nSome examples of how to be a good classmate include:\n\nDoing your best to avoid language that is ableist, racist, sexist, transphobic, or classist; or that perpetuates harmful stereotypes.\nAddressing your classmates (and your professor!) by their preferred name and pronouns.\nDoing your best to be aware of your own biases, privileges, and areas of ignorance.\nListening to others’ opinions, and making an effort to understand their perspective.\nTaking the time to help your classmates grasp concepts or solve problems, even when you are ready to move on.\n\n\nAttendance\nIt is my general expectation that you will attend lecture, and remain present until you have finished the day’s in-class work. However, I do not take formal attendance in class; as long as you engage with the material and complete the small check-ins, you can decide which lectures are useful to you.\nPlease do not email me letting me know when you are missing class - I don’t need to know if you are attending, it is your responsibility to catch up on the materials and check-ins you miss.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science (in Python)",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the quarter. Note that this schedule will be updated as the quarter progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nRequired Reading\nLecture Slides\nLab Activity\nWeekly Assignment\nExams & Project\n\n\n\n\n0\nSunday, January 5\n\nWelcome to DATA 301!\n\n\n\n\n\n1 Summarizing Tabular Data\nTuesday, January 7\n\nTabular Data & Variable Summaries\nGenAI Activity (Link to Collab)\nActivity 1.1 (Link to Collab)\n\n\n\n\n\nThursday, January 9\n\nVisualizing and Comparing Categorical Variables\nActivity 1.2 (Link to Collab)\nLab 1A\nLab 1B\n\n\n\n\nFriday, January 10\n\n\n\nLab 1 Due by Midnight\n\n\n\n2 Summarizing & Visualizing Quantitative Data\nTuesday, January 14\n(Dr. T’s b-day)\n\nVisualizing and Summarizing Quantitative Variables\nLecture 2.1 Activity\nActivity 2.1 (Link to Collab)\n\n\n\n\n\nThursday, January 16\n\nMultivariate Summaries\nLecture 2.2 Activity\nActivity 2.2\nLab 2A\nLab 2B\n\n\n\n\nFriday, January 17\n\n\n\nLab 2 Due by Midnight\n\n\n\n3\nMeasuring Similarity with Distances\nTuesday, January 21\nNo Class (Classes Follow Monday Schedule)\n\n\n\n\n\n\n\nThursday, January 23\n\nDistances Between Observations\n\nLecture Activity 3.1\nActivity 3.1\nLab 3\n\n\n\n\nFriday, January 24\n\n\n\nLab 3 Due by Midnight\n\n\n\n4\nDummy Variables & TF-IDF\nTuesday, January 28\n\nDummy Variables and Column Transformers\nActivity 4.1\n\n\n\n\n\nThursday, January 30\n\nBag-of-Words and TF-IDF\nActivity 4.2\nLab 4A\nLab 4B\n\n\n\n\nFriday, January 31\n\n\n\nLab 4 Due by Midnight\n\n\n\n5\nK-Nearest Neighbors & Midterm Exam\nTuesday, February 4\n\nK-Nearest-Neighbors\nActivity 5.1\n\n\n\n\n\nThursday, February 6\n\nSpicing up Your Visualizations\nActivity 5.2\n\nMidterm Exam (in-class)\n\n\n\nFriday, February 7\n\n\n\nLab 5 Due by Midnight\n\n\n\n6\nClassification & Model Selection\nTuesday, February 11\n\nCross-Validation and Grid Search\nActivity 6.1\n\n\n\n\n\nThursday, February 13\n\nClassification\nActivity 6.2\n\n\n\n\n\nFriday, February 14\n\n\n\nLab 6 Due by Midnight\n\n\n\n7\nLogistic Regression & Unsupervised Learning\nTuesday, February 18\n\nLogistic Regression\nActivity 7.1\n\n\n\n\n\nThursday, February 20\n\nUnsupervised Learning with K-Means\nActivity 7.2\n\n\n\n\n\nFriday, February 21\n\n\n\nLab 7 Due by Midnight\nProject Proposal Due\n\n\n8\nJoining Data & Hierarchical Data\nTuesday, February 25\n\nCombining Datasets\nActivity 8.1\n\n\n\n\n\nThursday, February 27\n\nHierarchical Data\nActivity 8.2\n\n\n\n\n\nFriday, February 28\n\n\n\nLab 8 Due by Midnight\n\n\n\n9\nWebscraping\nTuesday, March 4\n\nWebscraping\nActivity 9.1\n\n\n\n\n\nThursday, March 6\n\n\nActivity 9.2\n\n\n\n\n\nFriday, March 7\n\n\n\nLab 9 Due by Midnight\n\n\n\n10\nFinal Posters & Final Exam\nTuesday, March 11\n\n\n\n\nPosters Due to be Printed\n\n\n\nThursday, March 13\n\n\n\n\nFinal Exam (in-class)\n\n\nFinals Week\nSaturday, March 15 10:10am - 1pm\n\n\n\n\nPoster Presentations",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#about-me",
    "href": "slides/00-welcome.html#about-me",
    "title": "Welcome, Intro, and Setup",
    "section": "About Me",
    "text": "About Me\n\n\n\n\n\n\nGrew up in Grand Junction, CO\nBA in Stats: 2014 from Colorado Mesa University\nPhD in Stats: 2020 from Montana State University\n2020 - now: Professor of Stats at Cal Poly\nMy research: Statistics and data science education, R programming\nThings I like: Spending time outside (e.g., running, hiking, biking), travelling, going to concerts and musicals",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#grade-brakedown",
    "href": "slides/00-welcome.html#grade-brakedown",
    "title": "Welcome, Intro, and Setup",
    "section": "Grade Brakedown",
    "text": "Grade Brakedown\n\n5%: Check-Ins – Questions interspersed throughout lecture\n10%: Lab Activities – Lab attendance is required\n25%: Weekly Assignments – due Saturdays at midnight\n15% each: Exams in Week 5 and Week 10.\n30%: Final project",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#important-dates",
    "href": "slides/00-welcome.html#important-dates",
    "title": "Welcome, Intro, and Setup",
    "section": "Important Dates",
    "text": "Important Dates\n\nThurs, February 6: Exam 1 (in-class)\nFri, February 21: 1-page Project Proposal Due\nThurs, March 13: Exam 2 (in-class)\nSaturday, March 16 (before Finals Week: Final Project Poster Presentations",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#homework-late-policy",
    "href": "slides/00-welcome.html#homework-late-policy",
    "title": "Welcome, Intro, and Setup",
    "section": "Homework Late Policy",
    "text": "Homework Late Policy\n\nTwo deadline extensions: Fill out the form on Canvas before the deadline, get an automatic 3-day (72-hour) deadline extension.\nIf you’ve used your deadline extensions, late work has a 10% deduction per day for up to 5 days.\n\nThis policy also applies to deadline extension requests that are not submitted before the assignment’s deadline.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#missing-lab",
    "href": "slides/00-welcome.html#missing-lab",
    "title": "Welcome, Intro, and Setup",
    "section": "Missing Lab",
    "text": "Missing Lab\n\nBest option: Go to another section.\nSecond-best option: Turn in your work by email to me.\nWorst option: Miss it entirely, get a 0.\nYou may miss lab up to two times during the quarter. If you need to miss / reschedule more than two of the labs this quarter, you will need to retake the class.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#discord-and-email",
    "href": "slides/00-welcome.html#discord-and-email",
    "title": "Welcome, Intro, and Setup",
    "section": "Discord and Email",
    "text": "Discord and Email\n\nClass questions go on DISCORD\nUse my email only for personal concerns that you want to talk about privately to me.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#colab-notebooks",
    "href": "slides/00-welcome.html#colab-notebooks",
    "title": "Welcome, Intro, and Setup",
    "section": "Colab Notebooks",
    "text": "Colab Notebooks\n\nIn Data Science, everyone uses Notebooks, not scripts, for coding.\nWe will be using Jupyter Notebooks (invented at Cal Poly!), hosted for free by Google Colab.\nIf you want to work offline, you can install Anaconda on your laptop.\nI recommend an IDE like PyCharm (or Positron) as well.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#academic-integrity",
    "href": "slides/00-welcome.html#academic-integrity",
    "title": "Welcome, Intro, and Setup",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nIf you copy text from a website into your essay, it’s cheating.\nIf you ask your friend to write your essay, it’s cheating.\nIf you pay someone else to write your essay, it’s cheating.\nIf you ask GenAI to do your work and then copy the answers, it’s cheating.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#conversation-not-copying",
    "href": "slides/00-welcome.html#conversation-not-copying",
    "title": "Welcome, Intro, and Setup",
    "section": "Conversation, not copying",
    "text": "Conversation, not copying\nIt is okay (in fact, encouraged) to …\n\nAsk GenAI for tips on how to get started on a coding problem\nAsk GenAI to help find bugs in your code\nAsk GenAI to help explain concepts or functions\nPretend ChatGPT is your human tutor or TA!",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#activity-good-and-bad-use-of-ai",
    "href": "slides/00-welcome.html#activity-good-and-bad-use-of-ai",
    "title": "Welcome, Intro, and Setup",
    "section": "Activity: Good and bad use of AI",
    "text": "Activity: Good and bad use of AI\n\nFind your group member\n\nIn the “People” tab on Canvas, find your “Day 1 Group”\n\nOnce you have found your group member, open the Activity 1.1 notebook (linked on Canvas).\n\n\n15-minutes:\n\nFirst, Person A follows the instructions in Part 1 of the notebook.\nThen, Person B follows the instructions in Part 2 of the notebook.\nFinally, discuss and answer the questions at the bottom.\n\n\n\n\n\n\n\n\n\nIf it is not your turn to type, you watch!\n\n\nDo not give input unless your partner asks for help.",
    "crumbs": [
      "Lecture Slides",
      "Week 0 - Welcome & Course Set-up"
    ]
  },
  {
    "objectID": "slides/08-knn.html#steps-for-data-analysis",
    "href": "slides/08-knn.html#steps-for-data-analysis",
    "title": "K-Nearest-Neighbors",
    "section": "Steps for data analysis",
    "text": "Steps for data analysis\n\nRead and then clean the data\n\nAre there missing values? Will we drop those rows, or replace the missing values with something?\nAre there quantitative variables that Python thinks are categorical?\nAre there categorical variables that Python thinks are quantitative?\nAre there any anomalies in the data that concern you?"
  },
  {
    "objectID": "slides/08-knn.html#steps-for-data-analysis-contd",
    "href": "slides/08-knn.html#steps-for-data-analysis-contd",
    "title": "K-Nearest-Neighbors",
    "section": "Steps for data analysis (cont’d)",
    "text": "Steps for data analysis (cont’d)\n\nExplore the data by visualizing and summarizing.\n\nDifferent approaches for different combos of quantitative and categorical variables.\nThink about conditional calculations (split-apply-combine)"
  },
  {
    "objectID": "slides/08-knn.html#steps-for-data-analysis-contd-1",
    "href": "slides/08-knn.html#steps-for-data-analysis-contd-1",
    "title": "K-Nearest-Neighbors",
    "section": "Steps for data analysis (cont’d)",
    "text": "Steps for data analysis (cont’d)\n\nIdentify a research question of interest.\nPerform preprocessing steps\n\nShould we scale the quantitative variables?\nShould we one-hot-encode the categorical variables?\nShould we log-transform any variables?\n\nMeasure similarity between observations by calculating distances.\n\nWhich features should be included?\nWhich distance metric should we use?"
  },
  {
    "objectID": "slides/08-knn.html#data-wine-qualities",
    "href": "slides/08-knn.html#data-wine-qualities",
    "title": "K-Nearest-Neighbors",
    "section": "Data: Wine Qualities",
    "text": "Data: Wine Qualities\n\ndf = pd.read_csv(\"https://dlsun.github.io/pods/data/bordeaux.csv\")\ndf\n\n    year  price  summer  har   sep  win  age\n0   1952   37.0    17.1  160  14.3  600   40\n1   1953   63.0    16.7   80  17.3  690   39\n2   1955   45.0    17.1  130  16.8  502   37\n3   1957   22.0    16.1  110  16.2  420   35\n4   1958   18.0    16.4  187  19.1  582   34\n5   1959   66.0    17.5  187  18.7  485   33\n6   1960   14.0    16.4  290  15.8  763   32\n7   1961  100.0    17.3   38  20.4  830   31\n8   1962   33.0    16.3   52  17.2  697   30\n9   1963   17.0    15.7  155  16.2  608   29\n10  1964   31.0    17.3   96  18.8  402   28\n11  1965   11.0    15.4  267  14.8  602   27\n12  1966   47.0    16.5   86  18.4  819   26\n13  1967   19.0    16.2  118  16.5  714   25\n14  1968   11.0    16.2  292  16.4  610   24\n15  1969   12.0    16.5  244  16.6  575   23\n16  1970   40.0    16.7   89  18.0  622   22\n17  1971   27.0    16.8  112  16.9  551   21\n18  1972   10.0    15.0  158  14.6  536   20\n19  1973   16.0    17.1  123  17.9  376   19\n20  1974   11.0    16.3  184  16.2  574   18\n21  1975   30.0    16.9  171  17.2  572   17\n22  1976   25.0    17.6  247  16.1  418   16\n23  1977   11.0    15.6   87  16.8  821   15\n24  1978   27.0    15.8   51  17.4  763   14\n25  1979   21.0    16.2  122  17.3  717   13\n26  1980   14.0    16.0   74  18.4  578   12\n27  1981    NaN    17.0  111  18.0  535   11\n28  1982    NaN    17.4  162  18.5  712   10\n29  1983    NaN    17.4  119  17.9  845    9\n30  1984    NaN    16.5  119  16.0  591    8\n31  1985    NaN    16.8   38  18.9  744    7\n32  1986    NaN    16.3  171  17.5  563    6\n33  1987    NaN    17.0  115  18.9  452    5\n34  1988    NaN    17.1   59  16.8  808    4\n35  1989    NaN    18.6   82  18.4  443    3\n36  1990    NaN    18.7   80  19.3  468    2\n37  1991    NaN    17.7  183  20.4  570    1"
  },
  {
    "objectID": "slides/08-knn.html#data-wine-qualities-1",
    "href": "slides/08-knn.html#data-wine-qualities-1",
    "title": "K-Nearest-Neighbors",
    "section": "Data: Wine qualities",
    "text": "Data: Wine qualities\n\nGoal: Predict what will be the quality (price) of wines in a future year.\n\n\nIdea: Wines with similar features probably have similar quality.\n\n\n\nInputs: Summer temperature, harvest rainfall, September temperature, winter rainfall, age of wine\n\n“Inputs” = “Features” = “Predictors” = “Independent variables”\n\nOutput: Price in 1992\n\n“Output” = “Target” = “Dependent variable”"
  },
  {
    "objectID": "slides/08-knn.html#similar-wines",
    "href": "slides/08-knn.html#similar-wines",
    "title": "K-Nearest-Neighbors",
    "section": "Similar Wines",
    "text": "Similar Wines\nWhich wines have similar summer temps and winter rainfall to the 1989 vintage?\n\n\nCode\nfrom plotnine import *\n\n(\n  ggplot(df, mapping = aes(x = \"summer\", y = \"win\")) + \n  geom_point(color = \"white\") + \n  geom_text(mapping = aes(label = \"year\")) + \n  theme_bw()\n)"
  },
  {
    "objectID": "slides/08-knn.html#predicting-1989",
    "href": "slides/08-knn.html#predicting-1989",
    "title": "K-Nearest-Neighbors",
    "section": "Predicting 1989",
    "text": "Predicting 1989\n1989\n\n\ndf[df['year'] == 1989]\n\n    year  price  summer  har   sep  win  age\n35  1989    NaN    18.6   82  18.4  443    3\n\n\n\n1990\n\n\ndf[df['year'] == 1990]\n\n    year  price  summer  har   sep  win  age\n36  1990    NaN    18.7   80  19.3  468    2\n\n\n\n1976\n\n\ndf[df['year'] == 1976]\n\n    year  price  summer  har   sep  win  age\n22  1976   25.0    17.6  247  16.1  418   16"
  },
  {
    "objectID": "slides/08-knn.html#training-and-test-data",
    "href": "slides/08-knn.html#training-and-test-data",
    "title": "K-Nearest-Neighbors",
    "section": "Training and test data",
    "text": "Training and test data\n\n\nThe data for which we know the target is called the training data.\n\n\nknown_prices = df['year'] &lt; 1981\n\ndf_train = df[known_prices].copy()\n\n\n\n\n\n\nThe data for which we don’t know the target (and want to predict it) is called the test data.\n\n\nto_predict = df['year'] == 1989\n\ndf_test = df[to_predict].copy()"
  },
  {
    "objectID": "slides/08-knn.html#specify-steps",
    "href": "slides/08-knn.html#specify-steps",
    "title": "K-Nearest-Neighbors",
    "section": "Specify steps",
    "text": "Specify steps\nFirst we make a column transformer…\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\npreproc = make_column_transformer(\n  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),\n  remainder = \"drop\"\n)"
  },
  {
    "objectID": "slides/08-knn.html#fit-the-preprocesser",
    "href": "slides/08-knn.html#fit-the-preprocesser",
    "title": "K-Nearest-Neighbors",
    "section": "Fit the Preprocesser",
    "text": "Fit the Preprocesser\nThen we fit it on the training data.\n\n\n\npreproc.fit(df_train)\n\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\n\n\n\npreproc.named_transformers_['standardscaler'].mean_\n\narray([ 16.47037037, 144.81481481,  17.04814815, 608.40740741,\n        25.18518519])\n\npreproc.named_transformers_['standardscaler'].var_\n\narray([4.09492455e-01, 5.14089163e+03, 1.89286694e+00, 1.60333525e+04,\n       6.54842250e+01])"
  },
  {
    "objectID": "slides/08-knn.html#prep-the-data",
    "href": "slides/08-knn.html#prep-the-data",
    "title": "K-Nearest-Neighbors",
    "section": "Prep the data",
    "text": "Prep the data\nThen we transform the training data AND the test data:\n\ntrain_new = preproc.transform(df_train)\ntest_new = preproc.transform(df_test)\n\ntest_new\n\narray([[ 3.32798322, -0.87607817,  0.98258257, -1.30629957, -2.74154079]])"
  },
  {
    "objectID": "slides/08-knn.html#fitting-vs-transforming",
    "href": "slides/08-knn.html#fitting-vs-transforming",
    "title": "K-Nearest-Neighbors",
    "section": "Fitting vs Transforming",
    "text": "Fitting vs Transforming\nWhat if we had fit on the test data?\n\npreproc.transform(df_test)\n\narray([[0., 0., 0., 0., 0.]])"
  },
  {
    "objectID": "slides/08-knn.html#fitting-vs-transforming-1",
    "href": "slides/08-knn.html#fitting-vs-transforming-1",
    "title": "K-Nearest-Neighbors",
    "section": "Fitting vs Transforming",
    "text": "Fitting vs Transforming\nWhat if we had fit on the test data?\n\npreproc.transform(df_test)\n\narray([[0., 0., 0., 0., 0.]])"
  },
  {
    "objectID": "slides/08-knn.html#all-together",
    "href": "slides/08-knn.html#all-together",
    "title": "K-Nearest-Neighbors",
    "section": "All together:",
    "text": "All together:\n\n\npreproc = make_column_transformer(\n  (StandardScaler(), \n  ['summer', 'har', 'sep', 'win','age']),\n  remainder = \"drop\"\n)\n\n\n\npreproc.fit(df_train)\n\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\ntrain_new = preproc.transform(df_train)\ntest_new = preproc.transform(df_test)"
  },
  {
    "objectID": "slides/08-knn.html#find-the-closest-k",
    "href": "slides/08-knn.html#find-the-closest-k",
    "title": "K-Nearest-Neighbors",
    "section": "Find the Closest k",
    "text": "Find the Closest k\n\nfrom sklearn.metrics import pairwise_distances\n\ndists = pairwise_distances(test_new, train_new)\ndists\n\narray([[6.16457005, 5.74908409, 5.01651675, 5.8002254 , 5.48664619,\n        4.35898872, 6.56017889, 5.28491106, 5.38614546, 6.01267804,\n        3.72811494, 6.99167472, 5.26008007, 5.3100901 , 5.76468136,\n        4.97806644, 4.05228644, 3.86667994, 6.7345252 , 3.18480532,\n        4.69099623, 3.65924   , 3.62660714, 5.86910657, 5.30050409,\n        4.60719435, 4.34675994]])"
  },
  {
    "objectID": "slides/08-knn.html#find-the-closest-k-1",
    "href": "slides/08-knn.html#find-the-closest-k-1",
    "title": "K-Nearest-Neighbors",
    "section": "Find the Closest k",
    "text": "Find the Closest k\n\n\nbest = (\n  dists[0]\n  .argsort()\n  )\nbest[0:5]\n\narray([19, 22, 21, 10, 17])\n\n\n\n\n\n\n\ndf_train.loc[best]\n\n    year  price  summer  har   sep  win  age\n19  1973   16.0    17.1  123  17.9  376   19\n22  1976   25.0    17.6  247  16.1  418   16\n21  1975   30.0    16.9  171  17.2  572   17\n10  1964   31.0    17.3   96  18.8  402   28\n17  1971   27.0    16.8  112  16.9  551   21\n16  1970   40.0    16.7   89  18.0  622   22\n26  1980   14.0    16.0   74  18.4  578   12\n5   1959   66.0    17.5  187  18.7  485   33\n25  1979   21.0    16.2  122  17.3  717   13\n20  1974   11.0    16.3  184  16.2  574   18\n15  1969   12.0    16.5  244  16.6  575   23\n2   1955   45.0    17.1  130  16.8  502   37\n12  1966   47.0    16.5   86  18.4  819   26\n7   1961  100.0    17.3   38  20.4  830   31\n24  1978   27.0    15.8   51  17.4  763   14\n13  1967   19.0    16.2  118  16.5  714   25\n8   1962   33.0    16.3   52  17.2  697   30\n4   1958   18.0    16.4  187  19.1  582   34\n1   1953   63.0    16.7   80  17.3  690   39\n14  1968   11.0    16.2  292  16.4  610   24\n3   1957   22.0    16.1  110  16.2  420   35\n23  1977   11.0    15.6   87  16.8  821   15\n9   1963   17.0    15.7  155  16.2  608   29\n0   1952   37.0    17.1  160  14.3  600   40\n6   1960   14.0    16.4  290  15.8  763   32\n18  1972   10.0    15.0  158  14.6  536   20\n11  1965   11.0    15.4  267  14.8  602   27"
  },
  {
    "objectID": "slides/08-knn.html#find-the-closest-k-2",
    "href": "slides/08-knn.html#find-the-closest-k-2",
    "title": "K-Nearest-Neighbors",
    "section": "Find the closest k",
    "text": "Find the closest k\n\nranked_train = df_train.loc[dists[0].argsort()]\nranked_train\n\n    year  price  summer  har   sep  win  age\n19  1973   16.0    17.1  123  17.9  376   19\n22  1976   25.0    17.6  247  16.1  418   16\n21  1975   30.0    16.9  171  17.2  572   17\n10  1964   31.0    17.3   96  18.8  402   28\n17  1971   27.0    16.8  112  16.9  551   21\n16  1970   40.0    16.7   89  18.0  622   22\n26  1980   14.0    16.0   74  18.4  578   12\n5   1959   66.0    17.5  187  18.7  485   33\n25  1979   21.0    16.2  122  17.3  717   13\n20  1974   11.0    16.3  184  16.2  574   18\n15  1969   12.0    16.5  244  16.6  575   23\n2   1955   45.0    17.1  130  16.8  502   37\n12  1966   47.0    16.5   86  18.4  819   26\n7   1961  100.0    17.3   38  20.4  830   31\n24  1978   27.0    15.8   51  17.4  763   14\n13  1967   19.0    16.2  118  16.5  714   25\n8   1962   33.0    16.3   52  17.2  697   30\n4   1958   18.0    16.4  187  19.1  582   34\n1   1953   63.0    16.7   80  17.3  690   39\n14  1968   11.0    16.2  292  16.4  610   24\n3   1957   22.0    16.1  110  16.2  420   35\n23  1977   11.0    15.6   87  16.8  821   15\n9   1963   17.0    15.7  155  16.2  608   29\n0   1952   37.0    17.1  160  14.3  600   40\n6   1960   14.0    16.4  290  15.8  763   32\n18  1972   10.0    15.0  158  14.6  536   20\n11  1965   11.0    15.4  267  14.8  602   27"
  },
  {
    "objectID": "slides/08-knn.html#predict-from-the-closest-k",
    "href": "slides/08-knn.html#predict-from-the-closest-k",
    "title": "K-Nearest-Neighbors",
    "section": "Predict from the closest k",
    "text": "Predict from the closest k\nIf \\(k = 1\\) …\n\ndf_train.loc[best[0]]\n\nyear      1973.0\nprice       16.0\nsummer      17.1\nhar        123.0\nsep         17.9\nwin        376.0\nage         19.0\nName: 19, dtype: float64\n\n\n\n\n…we would predict a price of…\n\ndf_train.loc[best[0], \"price\"]\n\nnp.float64(16.0)"
  },
  {
    "objectID": "slides/08-knn.html#predict-from-the-closest-k-1",
    "href": "slides/08-knn.html#predict-from-the-closest-k-1",
    "title": "K-Nearest-Neighbors",
    "section": "Predict from the closest k",
    "text": "Predict from the closest k\nIf \\(k = 5\\) …\n\ndf_train.loc[best[0:5]]\n\n    year  price  summer  har   sep  win  age\n19  1973   16.0    17.1  123  17.9  376   19\n22  1976   25.0    17.6  247  16.1  418   16\n21  1975   30.0    16.9  171  17.2  572   17\n10  1964   31.0    17.3   96  18.8  402   28\n17  1971   27.0    16.8  112  16.9  551   21\n\n\n\n\n…we would predict a price of…\n\ndf_train.loc[best[0:5], 'price'].mean()\n\nnp.float64(25.8)"
  },
  {
    "objectID": "slides/08-knn.html#predict-from-the-closest-k-2",
    "href": "slides/08-knn.html#predict-from-the-closest-k-2",
    "title": "K-Nearest-Neighbors",
    "section": "Predict from the closest k",
    "text": "Predict from the closest k\nIf \\(k = 100\\) …\n\ndf_train.loc[best[0:100]]\n\n\n\nwe would predict a price of…\n\ndf_train.loc[best[0:100], 'price'].mean()\n\nnp.float64(28.814814814814813)"
  },
  {
    "objectID": "slides/08-knn.html#activity-1",
    "href": "slides/08-knn.html#activity-1",
    "title": "K-Nearest-Neighbors",
    "section": "Activity 1",
    "text": "Activity 1\nFind the predicted 1992 price for all the unknown wines, with\n\n\\(k = 1\\)\n\\(k = 5\\)\n\\(k = 10\\)\n\nHow close was each prediction to the right answer?\n(Optional hint: Write a function to help you!)"
  },
  {
    "objectID": "slides/08-knn.html#activity-2-together",
    "href": "slides/08-knn.html#activity-2-together",
    "title": "K-Nearest-Neighbors",
    "section": "Activity 2 (together)",
    "text": "Activity 2 (together)\nFind the predicted 1992 price for all the training data, with\n\n\\(k = 1\\)\n\\(k = 5\\)\n\\(k = 10\\)\n\nHow close was each prediction to the right answer?\n(Optional hint: Write a function to help you!)"
  },
  {
    "objectID": "slides/08-knn.html#knn",
    "href": "slides/08-knn.html#knn",
    "title": "K-Nearest-Neighbors",
    "section": "KNN",
    "text": "KNN\n\nWe have existing observations\n\\[(X_1, y_1), ... (X_n, y_n)\\] Where \\(X_i\\) is a set of features, and \\(y_i\\) is a target value.\nGiven a new observation \\(X_{new}\\), how do we predict \\(y_{new}\\)?\n\nFind the \\(k\\) values in \\((X_1, ..., X_n)\\) that are closest to \\(X_{new}\\)\nTake the average of the corresponding \\(y_i\\)’s to our five closest \\(X_i\\)’s.\nPredict \\(\\widehat{y}_{new}\\) = average of these \\(y_i\\)’s"
  },
  {
    "objectID": "slides/08-knn.html#knn-1",
    "href": "slides/08-knn.html#knn-1",
    "title": "K-Nearest-Neighbors",
    "section": "KNN",
    "text": "KNN\nTo perform K-Nearest-Neighbors, we choose the K closest observations to our target, and we average their response values.\nThe Big Questions:\n\nWhat is our definition of closest?\nWhat number should we use for K?\nHow do we evaluate the success of this approach?"
  },
  {
    "objectID": "slides/08-knn.html#knn-in-sklearn",
    "href": "slides/08-knn.html#knn-in-sklearn",
    "title": "K-Nearest-Neighbors",
    "section": "KNN in sklearn",
    "text": "KNN in sklearn\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n  preproc,\n  KNeighborsRegressor(n_neighbors=5)\n  )\n          \npipeline\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor()"
  },
  {
    "objectID": "slides/08-knn.html#knn-in-sklearn-1",
    "href": "slides/08-knn.html#knn-in-sklearn-1",
    "title": "K-Nearest-Neighbors",
    "section": "KNN in sklearn",
    "text": "KNN in sklearn\n\npipeline.fit(y = df_train['price'], X = df_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() \n\npipeline.predict(X=df_test)\n\narray([25.8])"
  },
  {
    "objectID": "slides/08-knn.html#activity-2",
    "href": "slides/08-knn.html#activity-2",
    "title": "K-Nearest-Neighbors",
    "section": "Activity 2",
    "text": "Activity 2\nFind the predicted 1992 price for the wines with known prices, with:\n\n\\(k = 1\\)\n\\(k = 5\\)\n\\(k = 10\\)\n\nHow close was each prediction to the right answer?"
  },
  {
    "objectID": "slides/06-preprocessing.html#distances",
    "href": "slides/06-preprocessing.html#distances",
    "title": "Dummy Variables and Column Transformers",
    "section": "Distances",
    "text": "Distances\n\nWe measure similarity between observations by calculating distances.\n\n\n\nEuclidean distance: sum of squared differences, then square root\nManhattan distance: sum of absolute differences\n\n\n\n\n\n\n\n\n\nscikit-learn\n\n\nUse the pairwise_distances() function to get back a 2D numpy array of distances.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#scaling",
    "href": "slides/06-preprocessing.html#scaling",
    "title": "Dummy Variables and Column Transformers",
    "section": "Scaling",
    "text": "Scaling\n\nIt is important that all our features be on the same scale for distances to be meaningful.\n\n\n\nStandardize: Subtract the mean (of the column) and divide by the standard deviation (of the column).\nMinMax: Subtract the minimum value, divide by the range.\n\n\n\n\n\n\n\n\n\nscikit-learn\n\n\nFollow the specify - fit - transform code structure. In the specify step, you should use the StandardScaler() or MinMaxScaler() functions.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#recall-ames-housing-data",
    "href": "slides/06-preprocessing.html#recall-ames-housing-data",
    "title": "Dummy Variables and Column Transformers",
    "section": "Recall: AMES Housing data",
    "text": "Recall: AMES Housing data\n\n\ndf = pd.read_table(\"https://datasci112.stanford.edu/data/housing.tsv\")\nfeatures = [\"Gr Liv Area\", \"Bedroom AbvGr\", \"Full Bath\", \"Half Bath\", \"Bldg Type\", \"Neighborhood\"]\ndf[features].head()\n\n   Gr Liv Area  Bedroom AbvGr  Full Bath  Half Bath Bldg Type Neighborhood\n0         1656              3          1          0      1Fam        NAmes\n1          896              2          1          0      1Fam        NAmes\n2         1329              3          1          1      1Fam        NAmes\n3         2110              3          2          1      1Fam        NAmes\n4         1629              3          2          1      1Fam      Gilbert",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#what-about-categorical-variables",
    "href": "slides/06-preprocessing.html#what-about-categorical-variables",
    "title": "Dummy Variables and Column Transformers",
    "section": "What about categorical variables?",
    "text": "What about categorical variables?\nSuppose we want to include the variable Bldg Type in our distance calculation…\n\ndf[\"Bldg Type\"].value_counts()\n\nBldg Type\n1Fam      2425\nTwnhsE     233\nDuplex     109\nTwnhs      101\n2fmCon      62\nName: count, dtype: int64\n\n\n\n\nThen we need a way to calculate \\((\\texttt{1Fam} - \\texttt{Twnhs} )^ 2\\).",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#converting-to-binary",
    "href": "slides/06-preprocessing.html#converting-to-binary",
    "title": "Dummy Variables and Column Transformers",
    "section": "Converting to Binary",
    "text": "Converting to Binary\nLet’s instead think about a variable that summarizes whether an observation is a single family home or not.\n\ndf[\"is_single_fam\"] = df[\"Bldg Type\"] == \"1Fam\"\ndf[\"is_single_fam\"].value_counts()\n\nis_single_fam\nTrue     2425\nFalse     505\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nWhat does a value of True represent? False?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummy-variables",
    "href": "slides/06-preprocessing.html#dummy-variables",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nWhen we transform a variable into binary (True / False), we call this variable a dummy variable or we say the variable has been one-hot-encoded.\n\n\nRemember that that computers interpret logical values (True / False) the same as 1 / 0:\n\ndf[\"is_single_fam\"] = df[\"is_single_fam\"].astype(\"int\")\ndf[\"is_single_fam\"].value_counts()\n\nis_single_fam\n1    2425\n0     505\nName: count, dtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#now-we-can-do-math",
    "href": "slides/06-preprocessing.html#now-we-can-do-math",
    "title": "Dummy Variables and Column Transformers",
    "section": "Now we can do math!",
    "text": "Now we can do math!\nSpecify\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import pairwise_distances\n\nscaler = StandardScaler()\n\n\n\nFit\n\n\n\ndf_orig = df[['Gr Liv Area', 'Bedroom AbvGr', 'is_single_fam']]\nscaler.fit(df_orig)\n\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\n\n\n\n\nTransform\n\n\ndf_scaled = scaler.transform(df_orig)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#calculating-distances",
    "href": "slides/06-preprocessing.html#calculating-distances",
    "title": "Dummy Variables and Column Transformers",
    "section": "Calculating Distances",
    "text": "Calculating Distances\n\n\ndists = pairwise_distances(df_scaled[[1707]], df_scaled)\nbest = (\n  dists\n  .argsort()\n  .flatten()\n  [1:10]\n  )\ndf_orig.iloc[best]\n\n      Gr Liv Area  Bedroom AbvGr  is_single_fam\n160          2978              5              1\n909          3082              5              1\n1288         2792              5              1\n2350         2784              5              1\n253          3222              5              1\n585          2640              5              1\n2027         2526              5              1\n2330         3390              5              1\n2501         2520              5              1",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-variables-1",
    "href": "slides/06-preprocessing.html#dummifying-variables-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying Variables",
    "text": "Dummifying Variables\n\n\n\nWhat if we don’t just want to study is_single_fam, but rather, all categories of the Bldg Type variable?\nIn principle, we just make dummy variables for each category: is_single_fam, is_twnhse, etc.\nEach category becomes one column, with 0’s and 1’s to show if the observation in that row matches that category.\nThat sounds pretty tedious, especially if you have a lot of categories…\nLuckily, we have shortcuts in both pandas and sklearn!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-pandas",
    "href": "slides/06-preprocessing.html#dummifying-in-pandas",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in Pandas",
    "text": "Dummifying in Pandas\n\n\npd.get_dummies(df[[\"Bldg Type\"]])\n\n      Bldg Type_1Fam  Bldg Type_2fmCon  ...  Bldg Type_Twnhs  Bldg Type_TwnhsE\n0               True             False  ...            False             False\n1               True             False  ...            False             False\n2               True             False  ...            False             False\n3               True             False  ...            False             False\n4               True             False  ...            False             False\n...              ...               ...  ...              ...               ...\n2925            True             False  ...            False             False\n2926            True             False  ...            False             False\n2927            True             False  ...            False             False\n2928            True             False  ...            False             False\n2929            True             False  ...            False             False\n\n[2930 rows x 5 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-pandas-1",
    "href": "slides/06-preprocessing.html#dummifying-in-pandas-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in Pandas",
    "text": "Dummifying in Pandas\n\n\n\n      Bldg Type_1Fam  Bldg Type_2fmCon  ...  Bldg Type_Twnhs  Bldg Type_TwnhsE\n0               True             False  ...            False             False\n1               True             False  ...            False             False\n2               True             False  ...            False             False\n3               True             False  ...            False             False\n4               True             False  ...            False             False\n...              ...               ...  ...              ...               ...\n2925            True             False  ...            False             False\n2926            True             False  ...            False             False\n2927            True             False  ...            False             False\n2928            True             False  ...            False             False\n2929            True             False  ...            False             False\n\n[2930 rows x 5 columns]\n\n\n\n\nSome things to notice here…\n\n\n\n\nWhat is the naming convention for the new columns?\nDoes this change the original dataframe df? If not, what would you need to do to add this information back in?\nWhat happens if you put the whole dataframe into the get_dummies function? What problems might arise from this?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-sklearn",
    "href": "slides/06-preprocessing.html#dummifying-in-sklearn",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in sklearn",
    "text": "Dummifying in sklearn\nSpecify\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\n\n\n\nFit\n\n\n\nencoder.fit(df[[\"Bldg Type\"]])\n\n\nOneHotEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder?Documentation for OneHotEncoderiFittedOneHotEncoder() \n\n\n\n\n\nTransform\n\n\ndf_bldg = encoder.transform(df[[\"Bldg Type\"]])\ndf_bldg\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 2930 stored elements and shape (2930, 5)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#dummifying-in-sklearn-1",
    "href": "slides/06-preprocessing.html#dummifying-in-sklearn-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Dummifying in sklearn",
    "text": "Dummifying in sklearn\n\n\n\n\ndf_bldg.todense()\n\n\n\n\n\n\n\n\nmatrix([[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]], shape=(2930, 5))\n\n\n\n\n\nThings to notice:\n\n\n\nWhat object type was the result?\nDoes this change the original dataframe df? If not, what would you need to do to add this information back in?\nWhat pros and cons do you see for the pandas approach vs the sklearn approach?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#preprocessing",
    "href": "slides/06-preprocessing.html#preprocessing",
    "title": "Dummy Variables and Column Transformers",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nSo far, we have now seen two preprocessing steps that might need to happen to do an analysis of distances:\n\nScaling the quantitative variables\nDummifying the categorical variables\n\n\n\n\nPreprocessing steps are things you do only to make the following analysis/visualization better.\n\nThis is not the same as data cleaning, where you make changes to fix the data (e.g., changing data types).\nThis is not the same as data wrangling, where you change the structure of the data (e.g., adding or deleting rows or columns).",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#lecture-4.1-quiz",
    "href": "slides/06-preprocessing.html#lecture-4.1-quiz",
    "title": "Dummy Variables and Column Transformers",
    "section": "Lecture 4.1 Quiz",
    "text": "Lecture 4.1 Quiz\nIdentify the following as cleaning, wrangling, or preprocessing:\n\n\nRemoving the $ symbol from a column and converting it to numeric.\nNarrowing your data down to only first class Titanic passengers, because you are not studying the others.\nConverting a Zip Code variable from numeric to categorical using .astype().\nCreating a new column called n_investment that counts the number of people who invested in a project.\nLog-transforming a column because it is very skewed.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#preprocessing-in-sklearn",
    "href": "slides/06-preprocessing.html#preprocessing-in-sklearn",
    "title": "Dummy Variables and Column Transformers",
    "section": "Preprocessing in sklearn",
    "text": "Preprocessing in sklearn\n\n\nUnlike cleaning and wrangling, the preprocessing steps are “temporary” changes to the dataframe.\n\n\n\n\n\nIt would be nice if we could trigger these changes as part of our analysis, instead of doing them “by hand”.\n\nThis is why the specify - fit - transform process is useful!\nWe will first specify all our preprocessing steps.\nThen we will fit the whole preprocess\nThen we will save the transform step for only when we need it.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#column-transformers-specify",
    "href": "slides/06-preprocessing.html#column-transformers-specify",
    "title": "Dummy Variables and Column Transformers",
    "section": "Column Transformers – Specify",
    "text": "Column Transformers – Specify\n\nfrom sklearn.compose import make_column_transformer\n\n\n\n\npreproc = make_column_transformer(\n  (OneHotEncoder(), [\"Bldg Type\", \"Neighborhood\"]),\n    remainder = \"passthrough\")",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#column-transformers-fit",
    "href": "slides/06-preprocessing.html#column-transformers-fit",
    "title": "Dummy Variables and Column Transformers",
    "section": "Column Transformers – Fit",
    "text": "Column Transformers – Fit\n\npreproc.fit(df[features])\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Bldg Type', 'Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Bldg Type', 'Neighborhood'])]) onehotencoder['Bldg Type', 'Neighborhood'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder() remainder['Gr Liv Area', 'Bedroom AbvGr', 'Full Bath', 'Half Bath'] passthroughpassthrough",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#column-transformers-transform",
    "href": "slides/06-preprocessing.html#column-transformers-transform",
    "title": "Dummy Variables and Column Transformers",
    "section": "Column Transformers – Transform",
    "text": "Column Transformers – Transform\n\npreproc.transform(df[features])\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 15717 stored elements and shape (2930, 37)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#things-to-notice",
    "href": "slides/06-preprocessing.html#things-to-notice",
    "title": "Dummy Variables and Column Transformers",
    "section": "Things to notice…",
    "text": "Things to notice…\n\nWhat submodule did we import make_column_transformer from?\nWhat are the two arguments to the make_column_transformer() function? What object structures are they?\nWhat happens if you fit and transform on the whole dataset, not just df[features]? Why might this be useful?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#lecture-activity-4.2",
    "href": "slides/06-preprocessing.html#lecture-activity-4.2",
    "title": "Dummy Variables and Column Transformers",
    "section": "Lecture Activity 4.2",
    "text": "Lecture Activity 4.2\nTry the following:\n\nWhat happens if you change remainder = \"passthrough\" to remainder = \"drop\"?\nWhat happens if you add the argument sparse_output = False to the OneHotEncoder() function?\nWhat happens if you add this line before the transform step: preproc.set_output(transform = \"pandas\") (keep the sparse_output = False when you try this)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#multiple-preprocessing-steps",
    "href": "slides/06-preprocessing.html#multiple-preprocessing-steps",
    "title": "Dummy Variables and Column Transformers",
    "section": "Multiple Preprocessing Steps",
    "text": "Multiple Preprocessing Steps\nWhy are column transformers so useful? We can do multiple preprocessing steps at once!\n\n\nfrom sklearn.preprocessing import StandardScaler\n\npreproc = make_column_transformer(\n        (StandardScaler(), [\"Gr Liv Area\"]),\n        (OneHotEncoder(sparse_output = False), [\"Bldg Type\", \"Neighborhood\"]),\n        remainder = \"passthrough\")",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#fit",
    "href": "slides/06-preprocessing.html#fit",
    "title": "Dummy Variables and Column Transformers",
    "section": "Fit!",
    "text": "Fit!\n\n(\n  preproc\n  .fit(df[features])\n  .set_output(transform = \"pandas\")\n)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 ['Gr Liv Area']),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type', 'Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 ['Gr Liv Area']),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type', 'Neighborhood'])]) standardscaler['Gr Liv Area'] StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoder['Bldg Type', 'Neighborhood'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(sparse_output=False) remainder['Bedroom AbvGr', 'Full Bath', 'Half Bath'] passthroughpassthrough",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#transform",
    "href": "slides/06-preprocessing.html#transform",
    "title": "Dummy Variables and Column Transformers",
    "section": "Transform!",
    "text": "Transform!\n\n\ndf_transformed = preproc.transform(df[features])\ndf_transformed\n\n      standardscaler__Gr Liv Area  ...  remainder__Half Bath\n0                        0.309265  ...                     0\n1                       -1.194427  ...                     0\n2                       -0.337718  ...                     1\n3                        1.207523  ...                     1\n4                        0.255844  ...                     1\n...                           ...  ...                   ...\n2925                    -0.982723  ...                     0\n2926                    -1.182556  ...                     0\n2927                    -1.048015  ...                     0\n2928                    -0.219006  ...                     0\n2929                     0.989884  ...                     1\n\n[2930 rows x 37 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#finding-all-categorical-variables",
    "href": "slides/06-preprocessing.html#finding-all-categorical-variables",
    "title": "Dummy Variables and Column Transformers",
    "section": "Finding All Categorical Variables",
    "text": "Finding All Categorical Variables\nWhat if we want to tell sklearn, “Please dummify every categorical variable.”? Use a selector instead of exact column names!\n\n\n\nfrom sklearn.compose import make_column_selector\n\npreproc = make_column_transformer(\n    (StandardScaler(),  \n     make_column_selector(dtype_include = np.number)\n     ),\n    (OneHotEncoder(sparse_output = False), \n     make_column_selector(dtype_include = object)\n     ),\n    remainder = \"passthrough\")",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#fit-1",
    "href": "slides/06-preprocessing.html#fit-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Fit!",
    "text": "Fit!\n\n(\n  preproc\n  .fit(df[features])\n  .set_output(transform = \"pandas\")\n)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x112152a50&gt;),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x111fb7610&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x112152a50&gt;),\n                                ('onehotencoder',\n                                 OneHotEncoder(sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x111fb7610&gt;)]) standardscaler&lt;sklearn.compose._column_transformer.make_column_selector object at 0x112152a50&gt; StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoder&lt;sklearn.compose._column_transformer.make_column_selector object at 0x111fb7610&gt; OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(sparse_output=False) remainder[] passthroughpassthrough",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#transform-1",
    "href": "slides/06-preprocessing.html#transform-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Transform!",
    "text": "Transform!\n\n\ndf_transformed = preproc.transform(df[features])\ndf_transformed\n\n      standardscaler__Gr Liv Area  ...  onehotencoder__Neighborhood_Veenker\n0                        0.309265  ...                                  0.0\n1                       -1.194427  ...                                  0.0\n2                       -0.337718  ...                                  0.0\n3                        1.207523  ...                                  0.0\n4                        0.255844  ...                                  0.0\n...                           ...  ...                                  ...\n2925                    -0.982723  ...                                  0.0\n2926                    -1.182556  ...                                  0.0\n2927                    -1.048015  ...                                  0.0\n2928                    -0.219006  ...                                  0.0\n2929                     0.989884  ...                                  0.0\n\n[2930 rows x 37 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#think-about-it",
    "href": "slides/06-preprocessing.html#think-about-it",
    "title": "Dummy Variables and Column Transformers",
    "section": "Think about it",
    "text": "Think about it\n\nWhat are the advantages of using a selector?\nWhat are the possible disadvantages of using a selector?\nDoes the order matter when using selectors? Try switching the steps and see what happens!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/06-preprocessing.html#takeaways-1",
    "href": "slides/06-preprocessing.html#takeaways-1",
    "title": "Dummy Variables and Column Transformers",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe dummify or one-hot-encode categorical variables to make them numbers.\nWe can do this with pd.get_dummies() or with OneHotEncoder() from sklearn.\nColumn Transformers let us apply multiple preprocessing steps at the same time.\n\nThink about which variables you want to apply the steps to\nThink about options for the steps, like sparseness\nThink about passthrough in your transformer",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 1 - Dummy Variables & Column Transformers"
    ]
  },
  {
    "objectID": "slides/05-distances.html#what-is-my-job",
    "href": "slides/05-distances.html#what-is-my-job",
    "title": "Distances Between Observations",
    "section": "What is my job?",
    "text": "What is my job?\n\n\n\n\nTeaching you stuff\n\n(Thoughtfully) choosing what to teach and how to teach it.\n\n\n\n\n\nAssessing what you’ve learned\n\nWhat do you understand about the tools I’ve taught you?\n\n\nThis is not the same as assessing if you figured out a way to accomplish a given task.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#using-the-tools-i-teach",
    "href": "slides/05-distances.html#using-the-tools-i-teach",
    "title": "Distances Between Observations",
    "section": "Using the tools I teach",
    "text": "Using the tools I teach\n\n\nis_100m = df_bolt[\"Event\"] == \"2008 Olympics 100m\"\ndf_100m = df_bolt[is_100m]\none_mean = df_100m[\"Time\"].mean()\none_std = df_100m[\"Time\"].std()\n\ndf_bolt[\"Standardized_Time\"] = 0.0\ndf_bolt.loc[is_100m, \"Standardized_Time\"] = (df_bolt.loc[is_100m, \"Time\"] - \n\n\n\n\n\n\nevent_stats = df_phelps.groupby('Event')['Time_in_seconds'].agg(['mean', 'std'])\ndf_phelps = df_phelps.merge(event_stats, on='Event')\ndf_phelps['Standardized_Time'] = (\n(df_phelps['Time_in_seconds'] - df_phelps['mean']) / df_phelps['std']\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#a-nice-clean-efficient-approach",
    "href": "slides/05-distances.html#a-nice-clean-efficient-approach",
    "title": "Distances Between Observations",
    "section": "A nice clean, efficient approach",
    "text": "A nice clean, efficient approach\n\nbolt_stats = (\n  df_bolt\n  .groupby(\"Event\")[\"Time\"]\n  .aggregate([\"mean\", \"std\"])\n  )\n\nstandardized_bolt = (\n  df_bolt\n  .set_index(['Event', 'Athlete'])['Time']\n  .subtract(bolt_stats[\"mean\"])\n  .divide(bolt_stats[\"std\"])\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lambda-functions",
    "href": "slides/05-distances.html#lambda-functions",
    "title": "Distances Between Observations",
    "section": "lambda Functions",
    "text": "lambda Functions\n\nphelps_sec = (\n  df_phelps[\"Time\"]\n  .str.split(\":\")\n  .apply(lambda x: float(x[0])*60 + float(x[1]))\n)\n\n\n\n\ndf_phelps[[\"Minutes\", \"Seconds\"]] = df_phelps[\"Time\"].str.split(\":\")\ndf_phelps[\"Time_New\"] = (\n  df_phelps[\"Minutes\"].astype(float) * 60 +\n  df_phelps[\"Seconds\"].astype(float)\n  )",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#a-loop-is-often-not-necessary",
    "href": "slides/05-distances.html#a-loop-is-often-not-necessary",
    "title": "Distances Between Observations",
    "section": "A loop is often not necessary",
    "text": "A loop is often not necessary\n\nsplit_times = df_phelps[\"Time\"].str.split(\":\")\nseconds = []\n\nfor time in split_times:\n  minute = int(time[0])\n  second = float(time[1])\n  seconds.append((minute * 60) + second)\n  df_phelps[\"Seconds\"] = seconds",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#when-to-make-a-function",
    "href": "slides/05-distances.html#when-to-make-a-function",
    "title": "Distances Between Observations",
    "section": "When to make a function?",
    "text": "When to make a function?\n\n\ndef time_to_secs(time_str):\n  mins, secs = time_str.split(':')\n  return float(mins) * 60 + float(secs)\n\ndf_phelps['time_secs'] = df_phelps['Time'].apply(time_to_secs)\n\n\n\n\n\n\ndef calculate_simpson_index(values, position):\n    \n    # Convert values to a Pandas Series, ensuring they are strings\n    values_series = pd.Series(values).astype(str)\n    \n    # Extract the specified character based on the position\n    extracted_character = values_series.str[position]\n    \n    # Calculate the frequency of each character\n    character_counts = extracted_character.value_counts(normalize=True)\n    \n    # Compute the Simpson's Index\n    simpson_index = 1 - sum(character_counts ** 2)\n    \n    return simpson_index",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#summarizing",
    "href": "slides/05-distances.html#summarizing",
    "title": "Distances Between Observations",
    "section": "Summarizing",
    "text": "Summarizing\n\nOne categorical variable: marginal distribution\nTwo categorical variables: joint and conditional distributions\nOne quantitative variable: mean, median, variance, standard deviation.\nOne quantitative, one categorical: mean, median, and std dev across groups (groupby(), split-apply-combine)\nTwo quantitative variables: z-scores, correlation",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#visualizing",
    "href": "slides/05-distances.html#visualizing",
    "title": "Distances Between Observations",
    "section": "Visualizing",
    "text": "Visualizing\n\nOne categorical variable: bar plot or column plot\nTwo categorical variables: stacked bar plot, side-by-side bar plot, or stacked percentage bar plot\nOne quantitative variable: histogram, density plot, or boxplot\nOne quantitative, one categorical: overlapping densities, side-by-side boxplots, or facetting\nTwo quantitative variables: scatterplot",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#ames-house-prices",
    "href": "slides/05-distances.html#ames-house-prices",
    "title": "Distances Between Observations",
    "section": "Ames house prices",
    "text": "Ames house prices\n\n\ndf = pd.read_table(\"https://datasci112.stanford.edu/data/housing.tsv\",\n                    sep = \"\\\\t\")\ndf.head()\n\n         PID  Gr Liv Area  Bedroom AbvGr  ...  Sale Type  Sale Condition  SalePrice\n0  526301100         1656              3  ...        WD           Normal     215000\n1  526350040          896              2  ...        WD           Normal     105000\n2  526351010         1329              3  ...        WD           Normal     172000\n3  526353030         2110              3  ...        WD           Normal     244000\n4  527105010         1629              3  ...        WD           Normal     189900\n\n[5 rows x 81 columns]\n\n\n\n\n\n\n\n\nread_table not read_csv\n\n\nThis is a tsv file (tab separated values), so we need to use a different function to read in our data! The sep argument allows you to specify the delimiter the file uses, but you can also allow the system to autodetect the delimiter.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms",
    "href": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms",
    "title": "Distances Between Observations",
    "section": "How does house size relate to number of bedrooms?",
    "text": "How does house size relate to number of bedrooms?\n\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(x = \"Gr Liv Area\", y = \"Bedroom AbvGr\"))  + \n  geom_point() +\n  labs(x = \"Total Living Area\", \n       y = \"Number of Bedrooms\")\n )",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms-1",
    "href": "slides/05-distances.html#how-does-house-size-relate-to-number-of-bedrooms-1",
    "title": "Distances Between Observations",
    "section": "How does house size relate to number of bedrooms?",
    "text": "How does house size relate to number of bedrooms?\nWhat statistic would you calculate?\n\n\ndf[[\"Gr Liv Area\", \"Bedroom AbvGr\"]].corr()\n\n               Gr Liv Area  Bedroom AbvGr\nGr Liv Area       1.000000       0.516808\nBedroom AbvGr     0.516808       1.000000",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#similarity",
    "href": "slides/05-distances.html#similarity",
    "title": "Distances Between Observations",
    "section": "Similarity",
    "text": "Similarity\nHow might we answer the question, “Are these two houses similar?”\n\n\n\n\ndf.loc[1707, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      2956\nBedroom AbvGr       5\nName: 1707, dtype: object\n\n\n\n\n\n\n\n\ndf.loc[290, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      2650\nBedroom AbvGr       6\nName: 290, dtype: object",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#distance",
    "href": "slides/05-distances.html#distance",
    "title": "Distances Between Observations",
    "section": "Distance",
    "text": "Distance\nThe distance between the two observations is:\n\\[ \\sqrt{ (2956 - 2650)^2 + (5 - 6)^2} = 306 \\]\n\n… what does this number mean? Not much!\nBut we can use it to compare sets of houses and find houses that appear to be the most similar.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#another-house-to-consider",
    "href": "slides/05-distances.html#another-house-to-consider",
    "title": "Distances Between Observations",
    "section": "Another House to Consider",
    "text": "Another House to Consider\n\n\n\ndf.loc[1707, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      2956\nBedroom AbvGr       5\nName: 1707, dtype: object\n\n\n\n\n\n\ndf.loc[291, [\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n\nGr Liv Area      1666\nBedroom AbvGr       3\nName: 291, dtype: object\n\n\n\n\n\\[ \\sqrt{ (2956 - 1666)^2 + (5 - 3)^2} = 1290 \\]\nThus, house 1707 is more similar to house 290 than to house 291.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-1",
    "href": "slides/05-distances.html#lecture-activity-part-1",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 1",
    "text": "Lecture Activity Part 1\n\nComplete Part One of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#house-160-seems-more-similar",
    "href": "slides/05-distances.html#house-160-seems-more-similar",
    "title": "Distances Between Observations",
    "section": "House 160 seems more similar…",
    "text": "House 160 seems more similar…\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = \"Gr Liv Area\", x = \"Bedroom AbvGr\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 2, shape = 17) + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Square Feet)\", \n       x = \"Number of Bedrooms\")\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#even-if-we-zoom-in",
    "href": "slides/05-distances.html#even-if-we-zoom-in",
    "title": "Distances Between Observations",
    "section": "…even if we zoom in…",
    "text": "…even if we zoom in…\n\n\n\nCode\n(\n  ggplot(df, mapping = aes(y = \"Gr Liv Area\", x = \"Bedroom AbvGr\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 5, shape = \"x\") + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Square Feet)\", \n       x = \"Number of Bedrooms\") +\n  scale_y_continuous(limits = (2500, 3500))\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#but-not-if-we-put-the-axes-on-the-same-scale",
    "href": "slides/05-distances.html#but-not-if-we-put-the-axes-on-the-same-scale",
    "title": "Distances Between Observations",
    "section": "…but not if we put the axes on the same scale!",
    "text": "…but not if we put the axes on the same scale!\n\n\n\nCode\n(\n  ggplot(df, aes(y = \"Gr Liv Area\", x = \"Bedroom AbvGr\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 5, shape = \"x\") + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Square Feet)\", \n       x = \"Number of Bedrooms\") +\n  scale_y_continuous(limits = (2900, 3000)) +\n  scale_x_continuous(limits = (0, 100))\n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#scaling",
    "href": "slides/05-distances.html#scaling",
    "title": "Distances Between Observations",
    "section": "Scaling",
    "text": "Scaling\nWe need to make sure our features are on the same scale before we can use distances to measure similarity.\n\n\n\n\n\n\nStandardizing\n\n\nsubtract the mean, divide by the standard deviation",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#scaling-1",
    "href": "slides/05-distances.html#scaling-1",
    "title": "Distances Between Observations",
    "section": "Scaling",
    "text": "Scaling\n\n\ndf['size_scaled'] = (df['Gr Liv Area'] - df['Gr Liv Area'].mean()) / df['Gr Liv Area'].std()\ndf['bdrm_scaled'] = (df['Bedroom AbvGr'] - df['Bedroom AbvGr'].mean()) / df['Bedroom AbvGr'].std()\n\n\n\n\n\nCode\n(\n  ggplot(df, aes(y = \"size_scaled\", x = \"bdrm_scaled\")) + \n  geom_point(color = \"lightgrey\") + \n  geom_point(df.loc[[1707]], color = \"red\", size = 5, shape = \"x\") + \n  geom_point(df.loc[[160]], color = \"blue\", size = 2) + \n  geom_point(df.loc[[2336]], color = \"green\", size = 2) + \n  theme_bw() +\n  labs(y = \"Total Living Area (Standardized)\", \n       x = \"Number of Bedrooms (Standardized)\") \n)",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-2",
    "href": "slides/05-distances.html#lecture-activity-part-2",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 2",
    "text": "Lecture Activity Part 2\n\nComplete Part Two of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#scikit-learn-1",
    "href": "slides/05-distances.html#scikit-learn-1",
    "title": "Distances Between Observations",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\nscikit-learn is a library for machine learning and modeling\nWe will use it a lot in this class!\nFor now, we will use it as a shortcut for scaling and for computing distances\n\n\n\nThe philosophy of sklearn is:\n\nspecify your analysis\nfit on the data to prepare the analysis\ntransform the data",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#specify",
    "href": "slides/05-distances.html#specify",
    "title": "Distances Between Observations",
    "section": "Specify",
    "text": "Specify\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriNot fittedStandardScaler() \n\n\nNo calculations have happened yet!",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#fit",
    "href": "slides/05-distances.html#fit",
    "title": "Distances Between Observations",
    "section": "Fit",
    "text": "Fit\nThe scaler object “learns” the means and standard deviations.\n\n\ndf_orig = df[['Gr Liv Area', 'Bedroom AbvGr']]\nscaler.fit(df_orig)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\nscaler.mean_\n\narray([1499.69044369,    2.85426621])\n\nscaler.scale_\n\narray([505.4226158 ,   0.82758988])\n\n\n\n\nWe still have not altered the data at all!",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#transform",
    "href": "slides/05-distances.html#transform",
    "title": "Distances Between Observations",
    "section": "Transform",
    "text": "Transform\n\ndf_scaled = scaler.transform(df_orig)\ndf_scaled\n\narray([[ 0.30926506,  0.17609421],\n       [-1.19442705, -1.03223376],\n       [-0.33771825,  0.17609421],\n       ...,\n       [-1.04801492,  0.17609421],\n       [-0.21900572, -1.03223376],\n       [ 0.9898836 ,  0.17609421]], shape=(2930, 2))",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#sklearn-numpy-and-pandas",
    "href": "slides/05-distances.html#sklearn-numpy-and-pandas",
    "title": "Distances Between Observations",
    "section": "sklearn, numpy, and pandas",
    "text": "sklearn, numpy, and pandas\n\n\nBy default, sklearn functions return numpy objects.\nThis is sometimes annoying, maybe we want to plot things after scaling.\nSolution: remake it, with the original column names.\n\n\n\n\n\npd.DataFrame(df_scaled, columns = df_orig.columns)\n\n      Gr Liv Area  Bedroom AbvGr\n0        0.309265       0.176094\n1       -1.194427      -1.032234\n2       -0.337718       0.176094\n3        1.207523       0.176094\n4        0.255844       0.176094\n...           ...            ...\n2925    -0.982723       0.176094\n2926    -1.182556      -1.032234\n2927    -1.048015       0.176094\n2928    -0.219006      -1.032234\n2929     0.989884       0.176094\n\n[2930 rows x 2 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#distances-with-sklearn",
    "href": "slides/05-distances.html#distances-with-sklearn",
    "title": "Distances Between Observations",
    "section": "Distances with sklearn",
    "text": "Distances with sklearn\n\nfrom sklearn.metrics import pairwise_distances\n\npairwise_distances(df_scaled[[1707]], df_scaled)\n\narray([[3.52929876, 5.45459713, 4.0252646 , ..., 4.61305666, 4.76999349,\n        3.06886734]], shape=(1, 2930))",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#finding-the-most-similar",
    "href": "slides/05-distances.html#finding-the-most-similar",
    "title": "Distances Between Observations",
    "section": "Finding the Most Similar",
    "text": "Finding the Most Similar\n\n\n\n\ndists = pairwise_distances(df_scaled[[1707]], \n                           df_scaled)\ndists.argsort()\n\narray([[1707,  160,  909, ...,  158, 2723, 2279]], shape=(1, 2930))\n\n\n\n\n\n\n\n\nbest = (\n  dists\n  .argsort()\n  .flatten()\n  [1:10]\n  )\n  \ndf_orig.iloc[best]\n\n      Gr Liv Area  Bedroom AbvGr\n160          2978              5\n909          3082              5\n1288         2792              5\n2350         2784              5\n253          3222              5\n2592         2640              5\n585          2640              5\n2027         2526              5\n2330         3390              5",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-3",
    "href": "slides/05-distances.html#lecture-activity-part-3",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 3",
    "text": "Lecture Activity Part 3\n\nComplete Part Three of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#other-scaling",
    "href": "slides/05-distances.html#other-scaling",
    "title": "Distances Between Observations",
    "section": "Other scaling",
    "text": "Other scaling\n\nStandardization \\[x_i \\leftarrow \\frac{x_i - \\bar{X}}{\\text{sd}(X)}\\]\nMin-Max Scaling \\[x_i \\leftarrow \\frac{x_i - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#other-distances",
    "href": "slides/05-distances.html#other-distances",
    "title": "Distances Between Observations",
    "section": "Other distances",
    "text": "Other distances\n\nEuclidean (\\(\\ell_2\\))\n\\[\\sqrt{\\sum_{j=1}^m (x_j - x'_j)^2}\\]\nManhattan (\\(\\ell_1\\))\n\\[\\sum_{j=1}^m |x_j - x'_j|\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#lecture-activity-part-4",
    "href": "slides/05-distances.html#lecture-activity-part-4",
    "title": "Distances Between Observations",
    "section": "Lecture Activity Part 4",
    "text": "Lecture Activity Part 4\n\nComplete Part Four of the activity linked in Canvas.\n\n\n\n\n−+\n10:00",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/05-distances.html#takeaways-1",
    "href": "slides/05-distances.html#takeaways-1",
    "title": "Distances Between Observations",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe measure similarity between observations by calculating distances.\nIt is important that all our features be on the same scale for distances to be meaningful.\nWe can use scikit-learn functions to fit and transform data, and to compute pairwise distances.\nThere are many options of ways to scale data; most common is standardizing\nThere are many options of ways to measure distances; most common is Euclidean distance.",
    "crumbs": [
      "Lecture Slides",
      "Week 3 - Distances Between Observations"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#distances",
    "href": "slides/07-text-data.html#distances",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Distances",
    "text": "Distances\n\nWe measure similarity between observations by calculating distances.\nEuclidean distance: sum of squared differences, then square root\nManhattan distance: sum of absolute differences\nIn scikit-learn, use the pairwise_distances() function to get back a 2D numpy array of distances.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#scaling",
    "href": "slides/07-text-data.html#scaling",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Scaling",
    "text": "Scaling\n\nIt is important that all our features be on the same scale for distances to be meaningful.\nStandardize: Subtract the mean (of the column) and divide by the standard deviation (of the column).\nMinMax: Subtract the minimum value, divide by the range.\nIn scikit-learn, use the StandardScaler() or MinMaxScaler() functions.\nFollow the specify - fit - transform code structure.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-data",
    "href": "slides/07-text-data.html#text-data",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Data",
    "text": "Text Data\nA textual data set consists of multiple texts. Each text is called a document. The collection of texts is called a corpus.\nExample Corpus:\n\n\n\"I am Sam\\n\\nI am Sam\\nSam I...\"\n\"The sun did not shine.\\nIt was...\"\n\"Fox\\nSocks\\nBox\\nKnox\\n\\nKnox...\"\n\"Every Who\\nDown in Whoville\\n...\"\n\"UP PUP Pup is up.\\nCUP PUP...\"\n\"On the fifteenth of May, in the...\"\n\"Congratulations!\\nToday is your...\"\n\"One fish, two fish, red fish...\"",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#reading-text-data",
    "href": "slides/07-text-data.html#reading-text-data",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Reading Text Data",
    "text": "Reading Text Data\nDocuments are usually stored in different files.\n\nseuss_dir = \"http://dlsun.github.io/pods/data/drseuss/\"\nseuss_files = [\n    \"green_eggs_and_ham.txt\", \n    \"cat_in_the_hat.txt\",\n    \"fox_in_socks.txt\", \n    \"how_the_grinch_stole_christmas.txt\",\n    \"hop_on_pop.txt\", \n    \"horton_hears_a_who.txt\",\n    \"oh_the_places_youll_go.txt\", \n    \"one_fish_two_fish.txt\"]\n\n…so",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#we-have-to-read-them-in-one-by-one",
    "href": "slides/07-text-data.html#we-have-to-read-them-in-one-by-one",
    "title": "Bag-of-Words and TF-IDF",
    "section": "…we have to read them in one by one",
    "text": "…we have to read them in one by one\n\nimport requests\n\ndocs = {}\nfor filename in seuss_files:\n    response = requests.get(seuss_dir + filename)\n    docs[filename] = response.text\n\n\n\n\ndocs.keys()\n\ndict_keys(['green_eggs_and_ham.txt', 'cat_in_the_hat.txt', 'fox_in_socks.txt', 'how_the_grinch_stole_christmas.txt', 'hop_on_pop.txt', 'horton_hears_a_who.txt', 'oh_the_places_youll_go.txt', 'one_fish_two_fish.txt'])",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#bag-of-words-representation",
    "href": "slides/07-text-data.html#bag-of-words-representation",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Bag-of-Words Representation",
    "text": "Bag-of-Words Representation\nIn the bag-of-words representation in this data, each column represents a word, and the values in the column are the word counts for that document.\n\nFirst, we need to split each document into individual words.\n\n\ndocs[\"hop_on_pop.txt\"].split()\n\n['UP', 'PUP', 'Pup', 'is', 'up.', 'CUP', 'PUP', 'Pup', 'in', 'cup.', 'PUP', 'CUP', 'Cup', 'on', 'pup.', 'MOUSE', 'HOUSE', 'Mouse', 'on', 'house.', 'HOUSE', 'MOUSE', 'House', 'on', 'mouse.', 'ALL', 'TALL', 'We', 'all', 'are', 'tall.', 'ALL', 'SMALL', 'We', 'all', 'are', 'small.', 'ALL', 'BALL', 'We', 'all', 'play', 'ball.', 'BALL', 'WALL', 'Up', 'on', 'a', 'wall.', 'ALL', 'FALL', 'Fall', 'off', 'the', 'wall.', 'DAY', 'PLAY', 'We', 'play', 'all', 'day.', 'NIGHT', 'FIGHT', 'We', 'fight', 'all', 'night.HE', 'ME', 'He', 'is', 'after', 'me.', 'HIM', 'JIM', 'Jim', 'is', 'after', 'him.', 'SEE', 'BEE', 'We', 'see', 'a', 'bee.', 'SEE', 'BEE', 'THREE', 'Now', 'we', 'see', 'three.', 'THREE', 'TREE', 'Three', 'fish', 'in', 'a', 'tree.', 'Fish', 'in', 'a', 'tree?', 'How', 'can', 'that', 'be?', 'RED', 'RED', 'They', 'call', 'me', 'Red.', 'RED', 'BED', 'I', 'am', 'in', 'bed.', 'RED', 'NED', 'TED', 'and', 'ED', 'in', 'BED', 'PAT', 'PAT', 'they', 'call', 'him', 'Pat.', 'PAT', 'SAT', 'Pat', 'sat', 'on', 'hat.', 'PAT', 'CAT', 'Pat', 'sat', 'on', 'cat.', 'PAT', 'BAT', 'Pat', 'sat', 'on', 'bat.', 'NO', 'PAT', 'NO', 'Don’t', 'sit', 'on', 'that.', 'SAD', 'DAD', 'BAD', 'HAD', 'Dad', 'is', 'sad.', 'Very,', 'very', 'sad.', 'He', 'had', 'a', 'bad', 'day.', 'What', 'a', 'day', 'Dad', 'had!', 'THING', 'THING', 'What', 'is', 'that', 'thing?', 'THING', 'SING', 'That', 'thing', 'can', 'sing!', 'SONG', 'LONG', 'A', 'long,', 'long', 'song.', 'Good-by,', 'Thing.', 'You', 'sing', 'too', 'long.', 'WALK', 'WALK', 'We', 'like', 'to', 'walk.', 'WALK', 'TALK', 'We', 'like', 'to', 'talk.', 'HOP', 'POP', 'We', 'like', 'to', 'hop.', 'We', 'like', 'to', 'hop', 'on', 'top', 'of', 'Pop.', 'STOP', 'You', 'must', 'not', 'hop', 'on', 'Pop.', 'Mr.', 'BROWN', 'Mrs.', 'BROWN', 'Mr.', 'Brown', 'upside', 'down.', 'Pup', 'up.', 'Brown', 'down.', 'Pup', 'is', 'down.', 'Where', 'is', 'Brown?', 'WHERE', 'IS', 'BROWN?', 'THERE', 'IS', 'BROWN!', 'Mr.', 'Brown', 'is', 'out', 'of', 'town.', 'BACK', 'BLACK', 'Brown', 'came', 'back.', 'Brown', 'came', 'back', 'with', 'Mr.', 'Black.', 'SNACK', 'SNACK', 'Eat', 'a', 'snack.', 'Eat', 'a', 'snack', 'with', 'Brown', 'and', 'Black.', 'JUMP', 'BUMP', 'He', 'jumped.', 'He', 'bumped.', 'FAST', 'PAST', 'He', 'went', 'past', 'fast.', 'WENT', 'TENT', 'SENT', 'He', 'went', 'into', 'the', 'tent.', 'I', 'sent', 'him', 'out', 'of', 'the', 'tent.', 'WET', 'GET', 'Two', 'dogs', 'get', 'wet.', 'HELP', 'YELP', 'They', 'yelp', 'for', 'help.', 'HILL', 'WILL', 'Will', 'went', 'up', 'hill.', 'WILL', 'HILL', 'STILL', 'Will', 'is', 'up', 'hill', 'still.', 'FATHER', 'MOTHER', 'SISTER', 'BROTHER', 'That', 'one', 'is', 'my', 'other', 'brother.', 'My', 'brothers', 'read', 'a', 'little', 'bit.', 'Little', 'words', 'like', 'If', 'and', 'it.', 'My', 'father', 'can', 'read', 'big', 'words,', 'too.', 'Like', 'CONSTANTINOPLE', 'and', 'TIMBUKTU', 'SAY', 'SAY', 'What', 'does', 'this', 'say?', 'seehemewe', 'patpuppop', 'hethreetreebee', 'tophopstop', 'Ask', 'me', 'tomorrow', 'but', 'not', 'today.']",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#then-count-the-words",
    "href": "slides/07-text-data.html#then-count-the-words",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Then Count the Words",
    "text": "Then Count the Words\n\n\nfrom collections import Counter\nCounter(\n  docs[\"hop_on_pop.txt\"]\n  .split()\n  )\n\nCounter({'is': 10, 'on': 10, 'We': 10, 'a': 9, 'He': 6, 'PAT': 6, 'Brown': 6, 'in': 5, 'all': 5, 'like': 5, 'Pup': 4, 'ALL': 4, 'RED': 4, 'and': 4, 'to': 4, 'Mr.': 4, 'PUP': 3, 'the': 3, 'can': 3, 'Pat': 3, 'sat': 3, 'What': 3, 'THING': 3, 'WALK': 3, 'of': 3, 'down.': 3, 'went': 3, 'up.': 2, 'CUP': 2, 'MOUSE': 2, 'HOUSE': 2, 'are': 2, 'BALL': 2, 'play': 2, 'wall.': 2, 'day.': 2, 'after': 2, 'SEE': 2, 'BEE': 2, 'see': 2, 'THREE': 2, 'that': 2, 'They': 2, 'call': 2, 'me': 2, 'BED': 2, 'I': 2, 'him': 2, 'NO': 2, 'Dad': 2, 'sad.': 2, 'That': 2, 'You': 2, 'hop': 2, 'Pop.': 2, 'not': 2, 'BROWN': 2, 'IS': 2, 'out': 2, 'came': 2, 'with': 2, 'Black.': 2, 'SNACK': 2, 'Eat': 2, 'tent.': 2, 'HILL': 2, 'WILL': 2, 'Will': 2, 'up': 2, 'My': 2, 'read': 2, 'SAY': 2, 'UP': 1, 'cup.': 1, 'Cup': 1, 'pup.': 1, 'Mouse': 1, 'house.': 1, 'House': 1, 'mouse.': 1, 'TALL': 1, 'tall.': 1, 'SMALL': 1, 'small.': 1, 'ball.': 1, 'WALL': 1, 'Up': 1, 'FALL': 1, 'Fall': 1, 'off': 1, 'DAY': 1, 'PLAY': 1, 'NIGHT': 1, 'FIGHT': 1, 'fight': 1, 'night.HE': 1, 'ME': 1, 'me.': 1, 'HIM': 1, 'JIM': 1, 'Jim': 1, 'him.': 1, 'bee.': 1, 'Now': 1, 'we': 1, 'three.': 1, 'TREE': 1, 'Three': 1, 'fish': 1, 'tree.': 1, 'Fish': 1, 'tree?': 1, 'How': 1, 'be?': 1, 'Red.': 1, 'am': 1, 'bed.': 1, 'NED': 1, 'TED': 1, 'ED': 1, 'they': 1, 'Pat.': 1, 'SAT': 1, 'hat.': 1, 'CAT': 1, 'cat.': 1, 'BAT': 1, 'bat.': 1, 'Don’t': 1, 'sit': 1, 'that.': 1, 'SAD': 1, 'DAD': 1, 'BAD': 1, 'HAD': 1, 'Very,': 1, 'very': 1, 'had': 1, 'bad': 1, 'day': 1, 'had!': 1, 'thing?': 1, 'SING': 1, 'thing': 1, 'sing!': 1, 'SONG': 1, 'LONG': 1, 'A': 1, 'long,': 1, 'long': 1, 'song.': 1, 'Good-by,': 1, 'Thing.': 1, 'sing': 1, 'too': 1, 'long.': 1, 'walk.': 1, 'TALK': 1, 'talk.': 1, 'HOP': 1, 'POP': 1, 'hop.': 1, 'top': 1, 'STOP': 1, 'must': 1, 'Mrs.': 1, 'upside': 1, 'Where': 1, 'Brown?': 1, 'WHERE': 1, 'BROWN?': 1, 'THERE': 1, 'BROWN!': 1, 'town.': 1, 'BACK': 1, 'BLACK': 1, 'back.': 1, 'back': 1, 'snack.': 1, 'snack': 1, 'JUMP': 1, 'BUMP': 1, 'jumped.': 1, 'bumped.': 1, 'FAST': 1, 'PAST': 1, 'past': 1, 'fast.': 1, 'WENT': 1, 'TENT': 1, 'SENT': 1, 'into': 1, 'sent': 1, 'WET': 1, 'GET': 1, 'Two': 1, 'dogs': 1, 'get': 1, 'wet.': 1, 'HELP': 1, 'YELP': 1, 'yelp': 1, 'for': 1, 'help.': 1, 'hill.': 1, 'STILL': 1, 'hill': 1, 'still.': 1, 'FATHER': 1, 'MOTHER': 1, 'SISTER': 1, 'BROTHER': 1, 'one': 1, 'my': 1, 'other': 1, 'brother.': 1, 'brothers': 1, 'little': 1, 'bit.': 1, 'Little': 1, 'words': 1, 'If': 1, 'it.': 1, 'father': 1, 'big': 1, 'words,': 1, 'too.': 1, 'Like': 1, 'CONSTANTINOPLE': 1, 'TIMBUKTU': 1, 'does': 1, 'this': 1, 'say?': 1, 'seehemewe': 1, 'patpuppop': 1, 'hethreetreebee': 1, 'tophopstop': 1, 'Ask': 1, 'tomorrow': 1, 'but': 1, 'today.': 1})",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#bag-of-words-representation-1",
    "href": "slides/07-text-data.html#bag-of-words-representation-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Bag-of-Words Representation",
    "text": "Bag-of-Words Representation\n… then, we put these counts into a Series.\n\n\n[\n  pd.Series(\n    Counter(doc.split())\n      ) for doc in docs.values()\n  ]\n\n[I           71\nam           3\nSam          3\nThat         2\nSam-I-am     4\n            ..\ngood         2\nsee!         1\nSo           1\nThank        2\nyou!         1\nLength: 116, dtype: int64, The         4\nsun         2\ndid         6\nnot        27\nshine.      1\n           ..\nNow,        1\nWell...     1\nYOU         1\nasked       1\nYOU?        1\nLength: 503, dtype: int64, Fox       6\nSocks     4\nBox       1\nKnox      8\nin       19\n         ..\nour       1\ndone,     1\nThank     1\nlot       1\nfun,      1\nLength: 328, dtype: int64, Every        3\nWho          9\nDown         1\nin          15\nWhoville     4\n            ..\nlight,       1\nbrought      1\nhe,          1\nHIMSELF!     1\ncarved       1\nLength: 623, dtype: int64, UP             1\nPUP            3\nPup            4\nis            10\nup.            2\n              ..\ntophopstop     1\nAsk            1\ntomorrow       1\nbut            1\ntoday.         1\nLength: 241, dtype: int64, On              5\nthe            88\nfifteenth       1\nof             33\nMay,            1\n               ..\nsummer.         1\nrain            1\nit's            1\nfall-ish,       1\nsmall-ish!\"     1\nLength: 918, dtype: int64, Congratulations!     1\nToday                2\nis                   7\nyour                19\nday.                 1\n                    ..\nday!                 1\nYour                 1\nmountain             1\nSo...get             1\nway!                 1\nLength: 449, dtype: int64, One         1\nfish,       7\ntwo         2\nred         1\nblue        2\n           ..\nTomorrow    1\nanother     1\none.        1\nEvery       1\nthere.      1\nLength: 501, dtype: int64]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#create-a-dataframe",
    "href": "slides/07-text-data.html#create-a-dataframe",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Create a DataFrame",
    "text": "Create a DataFrame\n… finally, we stack the Series into a DataFrame. This is called bag of words data.\n\n\npd.DataFrame(\n    [pd.Series(\n      Counter(doc.split())\n      ) for doc in docs.values()],\n    index = docs.keys()\n    )\n\n                                       I   am  Sam  ...  gone.  Tomorrow  one.\ngreen_eggs_and_ham.txt              71.0  3.0  3.0  ...    NaN       NaN   NaN\ncat_in_the_hat.txt                  48.0  NaN  NaN  ...    NaN       NaN   NaN\nfox_in_socks.txt                     9.0  NaN  NaN  ...    NaN       NaN   NaN\nhow_the_grinch_stole_christmas.txt   6.0  NaN  NaN  ...    NaN       NaN   NaN\nhop_on_pop.txt                       2.0  1.0  NaN  ...    NaN       NaN   NaN\nhorton_hears_a_who.txt              18.0  1.0  NaN  ...    NaN       NaN   NaN\noh_the_places_youll_go.txt           2.0  NaN  NaN  ...    NaN       NaN   NaN\none_fish_two_fish.txt               48.0  3.0  NaN  ...    1.0       1.0   1.0\n\n[8 rows x 2562 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#bag-of-words-in-scikit-learn",
    "href": "slides/07-text-data.html#bag-of-words-in-scikit-learn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Bag-of-Words in Scikit-Learn",
    "text": "Bag-of-Words in Scikit-Learn\n\nAlternatively, we can use CountVectorizer() in scikit-learn to produce a bag-of-words matrix.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\nSpecify\n\n\nvec = CountVectorizer()\n\n\n\n\nFit\n\n\n\nvec.fit(docs.values())\n\n\nCountVectorizer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer() \n\n\n\n\n\nTransform\n\n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 2308 stored elements and shape (8, 1344)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#entire-vocabulary",
    "href": "slides/07-text-data.html#entire-vocabulary",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Entire Vocabulary",
    "text": "Entire Vocabulary\n\nThe set of words across a corpus is called the vocabulary. We can view the vocabulary in a fitted CountVectorizer() as follows:\n\nvec.vocabulary_\n\n{'am': 23, 'sam': 935, 'that': 1138, 'do': 287, 'not': 767, 'like': 644, 'you': 1336, 'green': 471, 'eggs': 326, 'and': 26, 'ham': 495, 'them': 1141, 'would': 1316, 'here': 526, 'or': 786, 'there': 1143, 'anywhere': 32, 'in': 576, 'house': 558, 'with': 1303, 'mouse': 722, 'eat': 323, 'box': 132, 'fox': 419, 'could': 242, 'car': 179, 'they': 1145, 'are': 35, 'may': 688, 'will': 1292, 'see': 953, 'tree': 1204, 'let': 635, 'me': 691, 'be': 62, 'mot': 718, 'train': 1202, 'on': 778, 'say': 944, 'the': 1139, 'dark': 265, 'rain': 884, 'goat': 453, 'boat': 118, 'so': 1035, 'try': 1213, 'if': 575, 'good': 459, 'thank': 1136, 'sun': 1107, 'did': 279, 'shine': 972, 'it': 586, 'was': 1255, 'too': 1188, 'wet': 1268, 'to': 1178, 'play': 836, 'we': 1261, 'sat': 940, 'all': 16, 'cold': 231, 'day': 270, 'sally': 934, 'two': 1220, 'said': 932, 'how': 560, 'wish': 1302, 'had': 488, 'something': 1042, 'go': 452, 'out': 789, 'ball': 50, 'nothing': 768, 'at': 43, 'sit': 1001, 'one': 780, 'little': 650, 'bit': 102, 'bump': 157, 'then': 1142, 'went': 1265, 'made': 673, 'us': 1231, 'jump': 594, 'looked': 660, 'saw': 943, 'him': 536, 'step': 1077, 'mat': 684, 'cat': 185, 'hat': 507, 'he': 513, 'why': 1285, 'know': 614, 'is': 583, 'sunny': 1108, 'but': 165, 'can': 176, 'have': 512, 'lots': 663, 'of': 773, 'fun': 434, 'funny': 435, 'some': 1039, 'games': 438, 'new': 747, 'tricks': 1207, 'lot': 662, 'show': 986, 'your': 1338, 'mother': 719, 'mind': 704, 'what': 1269, 'our': 788, 'for': 413, 'fish': 388, 'no': 754, 'make': 676, 'away': 44, 'tell': 1130, 'want': 1253, 'should': 980, 'about': 4, 'when': 1271, 'now': 769, 'fear': 368, 'my': 735, 'bad': 47, 'game': 437, 'call': 172, 'up': 1226, 'put': 869, 'down': 299, 'this': 1151, 'fall': 358, 'hold': 543, 'high': 532, 'as': 39, 'stand': 1064, 'book': 122, 'hand': 496, 'cup': 260, 'look': 659, 'cake': 171, 'top': 1191, 'books': 123, 'litte': 649, 'toy': 1200, 'ship': 973, 'milk': 702, 'dish': 284, 'hop': 550, 'oh': 775, 'these': 1144, 'rake': 885, 'man': 680, 'tail': 1117, 'red': 897, 'fan': 362, 'fell': 372, 'his': 538, 'head': 514, 'came': 175, 'from': 429, 'things': 1149, 'into': 582, 'pot': 853, 'lit': 648, 'sank': 937, 'deep': 275, 'shook': 979, 'bent': 85, 'get': 444, 'another': 27, 'ran': 887, 'fast': 364, 'back': 46, 'big': 93, 'wood': 1308, 'shut': 987, 'hook': 548, 'trick': 1206, 'take': 1118, 'got': 462, 'tip': 1175, 'bow': 131, 'pick': 823, 'thing': 1148, 'bite': 103, 'shake': 964, 'hands': 497, 'their': 1140, 'those': 1153, 'gave': 442, 'pat': 807, 'tame': 1125, 'come': 233, 'give': 448, 'fly': 405, 'kites': 611, 'hit': 540, 'run': 925, 'hall': 494, 'wall': 1251, 'thump': 1161, 'string': 1096, 'kite': 610, 'gown': 463, 'her': 525, 'dots': 297, 'pink': 829, 'white': 1277, 'bed': 67, 'bumps': 159, 'jumps': 596, 'kicks': 605, 'hops': 551, 'thumps': 1162, 'kinds': 609, 'way': 1260, 'she': 969, 'home': 545, 'hear': 517, 'find': 380, 'near': 738, 'think': 1150, 'rid': 903, 'after': 7, 'net': 745, 'bet': 87, 'yet': 1327, 'plop': 841, 'last': 625, 'thoe': 1152, 'stop': 1086, 'pack': 793, 'dear': 273, 'shame': 967, 'sad': 929, 'kind': 607, 'has': 505, 'gone': 457, 'yes': 1326, 'mess': 697, 'tall': 1124, 'ca': 168, 'who': 1279, 'always': 22, 'playthings': 838, 'were': 1266, 'picked': 824, 'strings': 1097, 'any': 29, 'well': 1264, 'asked': 41, 'socks': 1037, 'knox': 616, 'chicks': 203, 'bricks': 143, 'blocks': 112, 'clocks': 224, 'sir': 999, 'mr': 727, 'first': 387, 'll': 653, 'quick': 875, 'brick': 142, 'stack': 1063, 'block': 111, 'chick': 202, 'clock': 223, 'ticks': 1165, 'tocks': 1180, 'tick': 1164, 'tock': 1179, 'six': 1003, 'sick': 988, 'please': 840, 'don': 293, 'tongue': 1187, 'isn': 585, 'slick': 1011, 'mixed': 709, 'sorry': 1048, 'an': 25, 'easy': 322, 'whose': 1283, 'sue': 1105, 'sews': 963, 'sees': 959, 'sew': 962, 'comes': 234, 'crow': 255, 'slow': 1014, 'joe': 590, 'clothes': 226, 'rose': 920, 'hose': 554, 'nose': 766, 'goes': 454, 'grows': 482, 'hate': 508, 'makes': 678, 'quite': 879, 'lame': 622, 'blue': 117, 'goo': 458, 'gooey': 460, 'gluey': 451, 'chewy': 201, 'chewing': 200, 'goose': 461, 'doing': 292, 'choose': 209, 'chew': 199, 'won': 1305, 'very': 1237, 'bim': 97, 'ben': 82, 'brings': 146, 'broom': 148, 'bends': 83, 'breaks': 140, 'band': 51, 'bands': 52, 'pig': 825, 'lead': 630, 'brooms': 149, 'bangs': 54, 'booms': 125, 'boom': 124, 'poor': 849, 'mouth': 724, 'much': 730, 'bring': 145, 'luke': 669, 'luck': 668, 'likes': 647, 'lakes': 621, 'duck': 309, 'licks': 637, 'takes': 1119, 'blab': 105, 'such': 1103, 'blibber': 110, 'blubber': 116, 'rubber': 923, 'dumb': 311, 'through': 1160, 'three': 1158, 'cheese': 198, 'trees': 1205, 'free': 420, 'fleas': 395, 'flew': 396, 'while': 1276, 'freezy': 422, 'breeze': 141, 'blew': 109, 'freeze': 421, 'sneeze': 1033, 'enough': 340, 'silly': 991, 'stuff': 1100, 'talk': 1121, 'tweetle': 1218, 'beetles': 72, 'fight': 377, 'called': 173, 'beetle': 71, 'battle': 59, 'puddle': 866, 'paddles': 798, 'paddle': 796, 'bottle': 127, 'muddle': 731, 'battles': 60, 'poodle': 846, 'eating': 324, 'noodles': 761, 'noodle': 760, 'wait': 1244, 'minute': 706, 'where': 1272, 'bottled': 128, 'paddled': 797, 'muddled': 732, 'duddled': 310, 'fuddled': 432, 'wuddled': 1319, 'done': 294, 'every': 346, 'whoville': 1284, 'liked': 645, 'christmas': 210, 'grinch': 473, 'lived': 652, 'just': 598, 'north': 765, 'hated': 509, 'whole': 1280, 'season': 952, 'ask': 40, 'knows': 615, 'reason': 896, 'wasn': 1256, 'screwed': 950, 'right': 905, 'perhaps': 817, 'shoes': 978, 'tight': 1168, 'most': 716, 'likely': 646, 'been': 69, 'heart': 519, 'sizes': 1005, 'small': 1019, 'whatever': 1270, 'stood': 1085, 'eve': 343, 'hating': 510, 'whos': 1282, 'staring': 1068, 'cave': 189, 'sour': 1053, 'grinchy': 475, 'frown': 430, 'warm': 1254, 'lighted': 643, 'windows': 1295, 'below': 81, 'town': 1198, 'knew': 612, 'beneath': 84, 'busy': 164, 'hanging': 499, 'mistletoe': 707, 'wreath': 1318, 're': 892, 'stockings': 1082, 'snarled': 1027, 'sneer': 1031, 'tomorrow': 1186, 'practically': 856, 'growled': 481, 'fingers': 384, 'nervously': 744, 'drumming': 307, 'must': 734, 'coming': 235, 'girls': 447, 'boys': 134, 'wake': 1246, 'bright': 144, 'early': 318, 'rush': 926, 'toys': 1201, 'noise': 755, 'young': 1337, 'old': 777, 'feast': 369, 'pudding': 865, 'rare': 889, 'roast': 912, 'beast': 64, 'which': 1275, 'couldn': 243, 'least': 632, 'close': 225, 'together': 1183, 'bells': 80, 'ringing': 907, 'start': 1069, 'singing': 997, 'sing': 996, 'more': 714, 'thought': 1155, 'fifty': 376, 'years': 1322, 've': 1236, 'idea': 574, 'awful': 45, 'wonderful': 1306, 'laughed': 627, 'throat': 1159, 'santy': 939, 'claus': 216, 'coat': 230, 'chuckled': 211, 'clucked': 229, 'great': 467, 'saint': 933, 'nick': 750, 'need': 742, 'reindeer': 898, 'around': 38, 'since': 994, 'scarce': 945, 'none': 757, 'found': 417, 'simply': 993, 'instead': 581, 'dog': 290, 'max': 687, 'took': 1189, 'thread': 1157, 'tied': 1167, 'horn': 552, 'loaded': 655, 'bags': 48, 'empty': 335, 'sacks': 928, 'ramshackle': 886, 'sleigh': 1010, 'hitched': 541, 'giddap': 446, 'started': 1070, 'toward': 1195, 'homes': 546, 'lay': 629, 'asnooze': 42, 'quiet': 877, 'snow': 1034, 'filled': 378, 'air': 12, 'dreaming': 301, 'sweet': 1112, 'dreams': 302, 'without': 1304, 'care': 180, 'square': 1062, 'number': 771, 'hissed': 539, 'climbed': 222, 'roof': 916, 'fist': 389, 'slid': 1012, 'chimney': 206, 'rather': 890, 'pinch': 828, 'santa': 938, 'stuck': 1099, 'only': 781, 'once': 779, 'moment': 710, 'fireplace': 386, 'flue': 403, 'hung': 570, 'row': 922, 'grinned': 477, 'slithered': 1013, 'slunk': 1017, 'smile': 1023, 'unpleasant': 1225, 'room': 917, 'present': 857, 'pop': 850, 'guns': 485, 'bicycles': 92, 'roller': 915, 'skates': 1006, 'drums': 308, 'checkerboards': 197, 'tricycles': 1208, 'popcorn': 851, 'plums': 843, 'stuffed': 1101, 'nimbly': 752, 'by': 167, 'icebox': 573, 'cleaned': 218, 'flash': 394, 'even': 344, 'hash': 506, 'food': 408, 'glee': 450, 'grabbed': 466, 'shove': 985, 'heard': 518, 'sound': 1051, 'coo': 239, 'dove': 298, 'turned': 1215, 'cindy': 213, 'lou': 664, 'than': 1135, 'caught': 187, 'tiny': 1174, 'daughter': 268, 'water': 1258, 'stared': 1067, 'taking': 1120, 'smart': 1021, 'lie': 638, 'tot': 1194, 'fake': 357, 'lied': 639, 'light': 642, 'side': 989, 'workshop': 1312, 'fix': 391, 'fib': 374, 'fooled': 410, 'child': 204, 'patted': 810, 'drink': 303, 'sent': 960, 'log': 656, 'fire': 385, 'himself': 537, 'liar': 636, 'walls': 1252, 'left': 634, 'hooks': 549, 'wire': 1301, 'speck': 1056, 'crumb': 256, 'same': 936, 'other': 787, 'houses': 559, 'leaving': 633, 'crumbs': 257, 'mouses': 723, 'quarter': 873, 'past': 806, 'dawn': 269, 'still': 1081, 'packed': 795, 'sled': 1008, 'presents': 858, 'ribbons': 902, 'wrappings': 1317, 'tags': 1116, 'tinsel': 1173, 'trimmings': 1209, 'trappings': 1203, 'thousand': 1156, 'feet': 371, 'mt': 729, 'crumpit': 258, 'rode': 914, 'load': 654, 'tiptop': 1176, 'dump': 312, 'pooh': 847, 'grinchishly': 474, 'humming': 565, 'finding': 381, 'waking': 1247, 'mouths': 725, 'hang': 498, 'open': 784, 'cry': 259, 'boo': 121, 'hoo': 547, 'paused': 811, 'ear': 317, 'rising': 908, 'over': 790, 'low': 667, 'grow': 480, 'sounded': 1052, 'merry': 696, 'popped': 852, 'eyes': 352, 'shocking': 976, 'surprise': 1111, 'hadn': 489, 'stopped': 1087, 'somehow': 1040, 'ice': 572, 'puzzling': 872, 'packages': 794, 'boxes': 133, 'puzzled': 870, 'hours': 557, 'till': 1169, 'puzzler': 871, 'sore': 1047, 'before': 74, 'maybe': 689, 'doesn': 289, 'store': 1088, 'means': 693, 'happened': 501, 'grew': 472, 'didn': 280, 'feel': 370, 'whizzed': 1278, 'morning': 715, 'brought': 152, 'carved': 183, 'pup': 868, 'off': 774, 'night': 751, 'jim': 588, 'bee': 68, 'ned': 741, 'ted': 1128, 'ed': 325, 'bat': 57, 'dad': 263, 'song': 1045, 'long': 658, 'walk': 1248, 'brown': 153, 'mrs': 728, 'upside': 1230, 'black': 106, 'snack': 1025, 'jumped': 595, 'bumped': 158, 'tent': 1132, 'dogs': 291, 'help': 523, 'yelp': 1325, 'hill': 534, 'father': 366, 'sister': 1000, 'brother': 150, 'brothers': 151, 'read': 893, 'words': 1309, 'constantinople': 238, 'timbuktu': 1170, 'does': 288, 'seehemewe': 954, 'patpuppop': 809, 'hethreetreebee': 527, 'tophopstop': 1192, 'today': 1181, 'fifteenth': 375, 'jungle': 597, 'nool': 763, 'heat': 520, 'cool': 241, 'pool': 848, 'splashing': 1058, 'enjoying': 339, 'joys': 592, 'horton': 553, 'elephant': 331, 'towards': 1196, 'again': 9, 'faint': 355, 'person': 818, 'calling': 174, 'dust': 314, 'blowing': 115, 'though': 1154, 'murmured': 733, 'never': 746, 'able': 3, 'yell': 1323, 'someone': 1041, 'sort': 1049, 'creature': 251, 'size': 1004, 'seen': 958, 'shaking': 965, 'blow': 114, 'steer': 1076, 'save': 941, 'because': 66, 'matter': 685, 'gently': 443, 'using': 1233, 'greatest': 469, 'stretched': 1095, 'trunk': 1212, 'lifted': 641, 'carried': 181, 'placed': 832, 'safe': 931, 'soft': 1038, 'clover': 227, 'humpf': 567, 'humpfed': 568, 'voice': 1242, 'twas': 1217, 'kangaroo': 599, 'pouch': 855, 'pin': 827, 'believe': 77, 'sincerely': 995, 'ears': 319, 'keen': 601, 'clearly': 221, 'four': 418, 'family': 360, 'children': 205, 'starting': 1071, 'favour': 367, 'disturb': 286, 'fool': 409, 'biggest': 95, 'blame': 107, 'kangaroos': 600, 'plunged': 844, 'terrible': 1133, 'frowned': 431, 'persons': 819, 'drowned': 306, 'protect': 861, 'bigger': 94, 'plucked': 842, 'hustled': 571, 'tops': 1193, 'news': 748, 'quickly': 876, 'spread': 1061, 'talks': 1123, 'flower': 402, 'walked': 1249, 'worrying': 1315, 'almost': 18, 'hour': 556, 'alarm': 13, 'harm': 504, 'walking': 1250, 'talking': 1122, 'barely': 56, 'speak': 1055, 'friend': 425, 'fine': 382, 'helped': 524, 'folks': 407, 'end': 336, 'saved': 942, 'ceilings': 190, 'floors': 401, 'churches': 212, 'grocery': 479, 'stores': 1089, 'mean': 692, 'gasped': 441, 'buildings': 156, 'piped': 830, 'certainly': 191, 'mayor': 690, 'friendly': 426, 'clean': 217, 'seem': 956, 'terribly': 1134, 'aren': 36, 'wonderfully': 1307, 'ville': 1239, 'thankful': 1137, 'greatful': 470, 'worry': 1314, 'spoke': 1059, 'monkeys': 711, 'neck': 739, 'wickersham': 1286, 'shouting': 984, 'rot': 921, 'elephants': 332, 'going': 455, 'nonsense': 758, 'snatched': 1028, 'bottomed': 129, 'eagle': 316, 'named': 737, 'valad': 1234, 'vlad': 1241, 'koff': 617, 'mighty': 699, 'strong': 1098, 'swift': 1113, 'wing': 1296, 'kindly': 608, 'beak': 63, 'late': 626, 'afternoon': 8, 'far': 363, 'bird': 99, 'flapped': 392, 'wings': 1297, 'flight': 398, 'chased': 195, 'groans': 478, 'stones': 1084, 'tattered': 1126, 'toenails': 1182, 'battered': 58, 'bones': 120, 'begged': 75, 'live': 651, 'folk': 406, 'beyond': 90, 'kept': 602, 'flapping': 393, 'shoulder': 981, 'quit': 878, 'yapping': 1321, 'hide': 531, '56': 1, 'next': 749, 'sure': 1109, 'place': 831, 'hid': 529, 'drop': 304, 'somewhere': 1044, 'inside': 579, 'patch': 808, 'clovers': 228, 'hundred': 569, 'miles': 701, 'wide': 1288, 'sneered': 1032, 'fail': 354, 'flip': 399, 'cried': 253, 'bust': 163, 'shall': 966, 'friends': 427, 'searched': 951, 'sought': 1050, 'noon': 764, 'dead': 272, 'alive': 15, 'piled': 826, 'nine': 753, 'five': 390, 'millionth': 703, 'really': 895, 'trouble': 1210, 'share': 968, 'birdie': 100, 'dropped': 305, 'landed': 623, 'hard': 503, 'tea': 1127, 'pots': 854, 'broken': 147, 'rocking': 913, 'chairs': 192, 'smashed': 1022, 'bicycle': 91, 'tires': 1177, 'crashed': 250, 'pleaded': 839, 'stick': 1079, 'making': 679, 'repairs': 900, 'course': 246, 'answered': 28, 'thin': 1147, 'thick': 1146, 'days': 271, 'wild': 1291, 'insisted': 580, 'chatting': 196, 'existed': 350, 'carryings': 182, 'peaceable': 812, 'bellowing': 79, 'bungle': 160, 'state': 1072, 'snapped': 1026, 'nonsensical': 759, 'dozens': 300, 'uncles': 1223, 'wickershams': 1287, 'cousins': 247, 'laws': 628, 'engaged': 338, 'roped': 919, 'caged': 170, 'hah': 490, 'boil': 119, 'hot': 555, 'steaming': 1075, 'kettle': 603, 'beezle': 73, 'nut': 772, 'oil': 776, 'full': 433, 'prove': 862, 'meeting': 695, 'everyone': 347, 'holler': 544, 'shout': 982, 'scream': 949, 'stew': 1078, 'scared': 947, 'people': 814, 'loudly': 666, 'smiled': 1024, 'clear': 219, 'bell': 78, 'surely': 1110, 'wind': 1294, 'distant': 285, 'voices': 1243, 'either': 329, 'neither': 743, 'grab': 465, 'shouted': 983, 'cage': 169, 'dope': 296, 'lasso': 624, 'stomach': 1083, 'ten': 1131, 'rope': 918, 'tie': 1166, 'knots': 613, 'lose': 661, 'dunk': 313, 'juice': 593, 'fought': 415, 'vigor': 1238, 'vim': 1240, 'gang': 439, 'many': 682, 'beat': 65, 'mauled': 686, 'haul': 511, 'managed': 681, 'die': 281, 'yourselves': 1340, 'tom': 1185, 'smack': 1018, 'whooped': 1281, 'racked': 882, 'rattled': 891, 'kettles': 604, 'brass': 137, 'pans': 802, 'garbage': 440, 'pail': 800, 'cranberry': 249, 'cans': 178, 'bazooka': 61, 'blasted': 108, 'toots': 1190, 'clarinets': 214, 'oom': 783, 'pahs': 799, 'flutes': 404, 'gusts': 486, 'loud': 665, 'racket': 883, 'rang': 888, 'sky': 1007, 'howling': 562, 'mad': 672, 'hullabaloo': 564, 'hey': 528, 'hows': 563, 'mine': 705, 'best': 86, 'working': 1311, 'anyone': 30, 'shirking': 975, 'rushed': 927, 'east': 321, 'west': 1267, 'seemed': 957, 'yipping': 1331, 'beeping': 70, 'bipping': 98, 'ruckus': 924, 'roar': 911, 'raced': 881, 'each': 315, 'building': 155, 'floor': 400, 'felt': 373, 'getting': 445, 'nowhere': 770, 'despair': 277, 'suddenly': 1104, 'burst': 161, 'door': 295, 'discovered': 283, 'shirker': 974, 'hidden': 530, 'fairfax': 356, 'apartments': 34, 'apartment': 33, '12': 0, 'jo': 589, 'standing': 1065, 'bouncing': 130, 'yo': 1332, 'yipp': 1330, 'chirp': 208, 'twerp': 1219, 'lad': 619, 'eiffelberg': 327, 'tower': 1197, 'towns': 1199, 'darkest': 267, 'time': 1171, 'blood': 113, 'aid': 11, 'country': 244, 'noises': 756, 'greater': 468, 'amounts': 24, 'counts': 245, 'thus': 1163, 'cleared': 220, 'yopp': 1335, 'extra': 351, 'finally': 379, 'proved': 863, 'world': 1313, 'smallest': 1020, 'true': 1211, 'planning': 835, 'summer': 1106, 'ish': 584, 'congratulations': 237, 'places': 833, 'brains': 135, 'yourself': 1339, 'direction': 282, 'own': 791, 'guy': 487, 'decide': 274, 'streets': 1094, 'em': 334, 'street': 1093, 'case': 184, 'straight': 1091, 'opener': 785, 'happen': 500, 'frequently': 423, 'brainy': 136, 'footsy': 412, 'along': 20, 'happening': 502, 'seeing': 955, 'sights': 990, 'join': 591, 'fliers': 397, 'soar': 1036, 'heights': 521, 'lag': 620, 'behind': 76, 'speed': 1057, 'pass': 805, 'soon': 1046, 'wherever': 1273, 'rest': 901, 'except': 349, 'sometimes': 1043, 'sadly': 930, 'bang': 53, 'ups': 1229, 'prickle': 859, 'ly': 671, 'perch': 816, 'lurch': 670, 'chances': 194, 'slump': 1015, 'un': 1221, 'slumping': 1016, 'easily': 320, 'marked': 683, 'mostly': 717, 'darked': 266, 'sprain': 1060, 'both': 126, 'elbow': 330, 'chin': 207, 'dare': 264, 'stay': 1073, 'win': 1293, 'turn': 1214, 'quarters': 874, 'sneak': 1029, 'simple': 992, 'afraid': 6, 'maker': 677, 'upper': 1228, 'confused': 236, 'race': 880, 'wiggled': 1290, 'roads': 910, 'break': 139, 'necking': 740, 'pace': 792, 'grind': 476, 'cross': 254, 'weirdish': 1263, 'space': 1054, 'headed': 515, 'useless': 1232, 'waiting': 1245, 'bus': 162, 'plane': 834, 'mail': 675, 'phone': 822, 'ring': 906, 'hair': 491, 'friday': 424, 'uncle': 1222, 'jake': 587, 'better': 88, 'pearls': 813, 'pair': 801, 'pants': 803, 'wig': 1289, 'curls': 261, 'chance': 193, 'escape': 341, 'staying': 1074, 'playing': 837, 'banner': 55, 'ride': 904, 'ready': 894, 'anything': 31, 'under': 1224, 'points': 845, 'scored': 948, 'magical': 674, 'winning': 1300, 'est': 342, 'winner': 1299, 'fame': 359, 'famous': 361, 'watching': 1257, 'tv': 1216, 'times': 1172, 'lonely': 657, 'cause': 188, 'against': 10, 'alone': 19, 'whether': 1274, 'meet': 694, 'scare': 946, 'road': 909, 'between': 89, 'hither': 542, 'yon': 1333, 'weather': 1262, 'foul': 416, 'enemies': 337, 'prowl': 864, 'hakken': 493, 'kraks': 618, 'howl': 561, 'onward': 782, 'frightening': 428, 'creek': 252, 'arms': 37, 'sneakers': 1030, 'leak': 631, 'hike': 533, 'face': 353, 'problems': 860, 'already': 21, 'strange': 1092, 'birds': 101, 'tact': 1115, 'remember': 899, 'life': 640, 'balancing': 49, 'act': 5, 'forget': 414, 'dexterous': 278, 'deft': 276, 'mix': 708, 'foot': 411, 'succeed': 1102, 'indeed': 577, '98': 2, 'percent': 815, 'guaranteed': 483, 'kid': 606, 'move': 726, 'mountains': 721, 'name': 736, 'buxbaum': 166, 'bixby': 104, 'bray': 138, 'mordecai': 713, 'ali': 14, 'van': 1235, 'allen': 17, 'shea': 970, 'mountain': 720, 'star': 1066, 'glad': 449, 'fat': 365, 'yellow': 1324, 'everywhere': 348, 'seven': 961, 'eight': 328, 'eleven': 333, 'ever': 345, 'wump': 1320, 'hump': 566, 'gump': 484, 'pull': 867, 'sticks': 1080, 'bike': 96, 'mike': 700, 'sits': 1002, 'work': 1310, 'hills': 535, 'hello': 522, 'cow': 248, 'cannot': 177, 'teeth': 1129, 'gold': 456, 'shoe': 977, 'story': 1090, 'told': 1184, 'nook': 762, 'cook': 240, 'moon': 712, 'sheep': 971, 'sleep': 1009, 'zans': 1341, 'gox': 464, 'ying': 1328, 'sings': 998, 'yink': 1329, 'wink': 1298, 'ink': 578, 'yop': 1334, 'finger': 383, 'brush': 154, 'comb': 232, 'pet': 820, 'met': 698, 'cats': 186, 'cut': 262, 'pets': 821, 'zeds': 1342, 'upon': 1227, 'heads': 516, 'haircut': 492, 'wave': 1259, 'swish': 1114, 'gack': 436, 'park': 804, 'clark': 215, 'zeep': 1343}",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#specific-words",
    "href": "slides/07-text-data.html#specific-words",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Specific Words",
    "text": "Specific Words\n\nWe can extract specific words from the vocabulary as follows:\n\nvec.vocabulary_[\"fish\"]\n\n388\n\nvec.vocabulary_[\"pop\"]\n\n850\n\nvec.vocabulary_[\"eggs\"]\n\n326",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-normalizing",
    "href": "slides/07-text-data.html#text-normalizing",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Normalizing",
    "text": "Text Normalizing\nWhat’s wrong with the way we counted words originally?\nCounter({'UP': 1, 'PUP': 3, 'Pup': 4, 'is': 10, 'up.': 2, ...})\n\n\nIt’s usually good to normalize for punctuation and capitalization.\nNormalization options are specified when you initialize the CountVectorizer().\nBy default, scikit-learn strips punctuation and converts all characters to lowercase.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#text-normalizing-in-sklearn",
    "href": "slides/07-text-data.html#text-normalizing-in-sklearn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Text Normalizing in sklearn",
    "text": "Text Normalizing in sklearn\n\nIf you don’t want scikit-learn to normalize for punctuation and capitalization, you can do the following:\n\n\n\nvec = CountVectorizer(lowercase = False, token_pattern = r\"[\\S]+\")\nvec.fit(docs.values())\n\nCountVectorizer(lowercase=False, token_pattern='[\\\\S]+')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(lowercase=False, token_pattern='[\\\\S]+') \n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 3679 stored elements and shape (8, 2562)&gt;\n\n\n\n\n\n\n\n\n\n\nCountVectorizer()\n\n\nSetting lowercase = False doesn’t automatically convert every word to lowercase. token_pattern = r\"[\\S]+\" declares a regular expression that treats every sequence of characters that is not whitespace ([\\S]) as a single token.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#the-shortcomings-of-bag-of-words",
    "href": "slides/07-text-data.html#the-shortcomings-of-bag-of-words",
    "title": "Bag-of-Words and TF-IDF",
    "section": "The Shortcomings of Bag-of-Words",
    "text": "The Shortcomings of Bag-of-Words\nBag-of-words is easy to understand and easy to implement. What are its disadvantages?\nConsider the following documents:\n\n“The dog bit her owner.”\n“Her dog bit the owner.”\n\nBoth documents have the same exact bag-of-words representation, but they mean something quite different!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#n-grams-1",
    "href": "slides/07-text-data.html#n-grams-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "N-grams",
    "text": "N-grams\n\n\nAn n-gram is a sequence of \\(n\\) words.\nN-grams allow us to capture more of the meaning.\nFor example, if we count bigrams (2-grams) instead of words, we can distinguish the two documents from before:\n\n\n“The dog bit her owner.”\n“Her dog bit the owner.”\n\n\n\n\\[\\begin{array}{l|ccccccc}\n& \\text{the, dog} & \\text{her, dog} & \\text{dog, bit} & \\text{bit, the} & \\text{bit, her} & \\text{the, owner} & \\text{her, owner} \\\\\n\\hline\n\\text{1} & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\\\\n\\text{2} & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\\\\n\\end{array}\\]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#n-grams-in-scikit-learn",
    "href": "slides/07-text-data.html#n-grams-in-scikit-learn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "N-grams in scikit-learn",
    "text": "N-grams in scikit-learn\n\nWe can easily modify our previous approach by specifying ngram_range = in CountVectorizer(). To get bigrams, we set the range to (2, 2).\n\n\nSpecify\n\n\nvec = CountVectorizer(ngram_range = (2, 2))\n\n\n\n\nFit\n\n\n\nvec.fit(docs.values())\n\n\nCountVectorizer(ngram_range=(2, 2))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(ngram_range=(2, 2)) \n\n\n\n\n\nTransform\n\n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 6459 stored elements and shape (8, 5846)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#n-grams-in-scikit-learn-1",
    "href": "slides/07-text-data.html#n-grams-in-scikit-learn-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "N-grams in scikit-learn",
    "text": "N-grams in scikit-learn\n\n… or we can also get individual words (unigrams) alongside the bigrams:\n\nSpecify\n\n\nvec = CountVectorizer(ngram_range = (1, 2))\n\n\nFit\n\n\n\nvec.fit(docs.values())\n\n\nCountVectorizer(ngram_range=(1, 2))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(ngram_range=(1, 2)) \n\n\n\nTransform\n\n\nvec.transform(docs.values())\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 8767 stored elements and shape (8, 7190)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#similar-documents",
    "href": "slides/07-text-data.html#similar-documents",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Similar Documents",
    "text": "Similar Documents\nNow, we can use this bag-of-words data to measure similarities between documents!\n\n\nfrom sklearn.metrics import pairwise_distances\n\ndat = vec.transform(docs.values())\n\ndists = pairwise_distances(dat)\ndists\n\narray([[  0.        , 204.84628383, 178.52730884, 219.79535937,\n        168.42802617, 228.52570971, 183.5973856 , 178.83511959],\n       [204.84628383,   0.        , 189.45711916, 157.15597348,\n        186.2793601 , 152.32859219, 175.28262892, 156.12815249],\n       [178.52730884, 189.45711916,   0.        , 171.66828478,\n         95.21554495, 189.95262567, 141.52031656, 130.11533345],\n       [219.79535937, 157.15597348, 171.66828478,   0.        ,\n        163.84138671, 138.97481786, 174.56230979, 162.59766296],\n       [168.42802617, 186.2793601 ,  95.21554495, 163.84138671,\n          0.        , 188.92855793, 133.1990991 , 112.89818422],\n       [228.52570971, 152.32859219, 189.95262567, 138.97481786,\n        188.92855793,   0.        , 162.83120094, 164.95453919],\n       [183.5973856 , 175.28262892, 141.52031656, 174.56230979,\n        133.1990991 , 162.83120094,   0.        , 134.98888843],\n       [178.83511959, 156.12815249, 130.11533345, 162.59766296,\n        112.89818422, 164.95453919, 134.98888843,   0.        ]])",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#similar-documents-1",
    "href": "slides/07-text-data.html#similar-documents-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Similar Documents",
    "text": "Similar Documents\n\n\n\ndists[0].argsort()\n\narray([0, 4, 2, 7, 6, 1, 3, 5])\n\n\n\n\n\n\ndocs.keys()\n\ndict_keys(['green_eggs_and_ham.txt', 'cat_in_the_hat.txt', 'fox_in_socks.txt', 'how_the_grinch_stole_christmas.txt', 'hop_on_pop.txt', 'horton_hears_a_who.txt', 'oh_the_places_youll_go.txt', 'one_fish_two_fish.txt'])\n\n\n\n\n\n\n\n\n\nTip\n\n\nThis is how data scientists do authorship identification!",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#one-fish-two-fish",
    "href": "slides/07-text-data.html#one-fish-two-fish",
    "title": "Bag-of-Words and TF-IDF",
    "section": "One Fish Two Fish",
    "text": "One Fish Two Fish\nUsing bi-grams, unigrams, and tri-grams, which Dr. Seuss document is closest to “One Fish Two Fish”?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#issues-with-the-distance-approach",
    "href": "slides/07-text-data.html#issues-with-the-distance-approach",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Issues with the Distance Approach",
    "text": "Issues with the Distance Approach\nBUT WAIT!\n\nDon’t we care more about word choice than total words used?\nWouldn’t a longer document have more words, and thus be able to “match” other documents?\nWouldn’t more common words appear in more documents, and thus cause them to “match”?\nRecall: We have many options for scaling\nRecall: We have many options for distance metrics.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#example",
    "href": "slides/07-text-data.html#example",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Example",
    "text": "Example\n\nDocument A:\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\nDocument B:\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\nDocument C:\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\nDocument D:\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#example-with-code",
    "href": "slides/07-text-data.html#example-with-code",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Example with Code",
    "text": "Example with Code\n\n\nCode\ndocuments = [\n    \"whoever has hate for his brother is in the darkness and walks in the darkness\",\n    \"hello darkness my old friend\",\n    \"returning hate for hate multiplies hate adding deeper darkness to a night already devoid of stars darkness cannot drive out darkness only light can do that\",\n    \"happiness can be found in the darkest of times if only one remembers to turn on the light\"\n]\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer(token_pattern = r\"\\w+\")\nvec.fit(documents)\n\n\nCountVectorizer(token_pattern='\\\\w+')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(token_pattern='\\\\w+') \n\n\nCode\nbow_matrix = vec.transform(documents)\nbow_matrix\n\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 56 stored elements and shape (4, 45)&gt;",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#example-output",
    "href": "slides/07-text-data.html#example-output",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Example Output",
    "text": "Example Output\n\nbow_dataframe = pd.DataFrame(\n  bow_matrix.todense(), \n  columns = vec.get_feature_names_out()\n  )\nbow_dataframe[[\"darkness\", \"light\"]]\n\n   darkness  light\n0         2      0\n1         1      0\n2         3      1\n3         0      1",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#measuring-similarity",
    "href": "slides/07-text-data.html#measuring-similarity",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Measuring Similarity",
    "text": "Measuring Similarity\n\n\n\nCode\nfrom sklearn.metrics import pairwise_distances\n\ndists = pairwise_distances(bow_matrix)\ndists[0].argsort()\n\n\narray([0, 1, 3, 2])\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#choosing-your-distance-metric",
    "href": "slides/07-text-data.html#choosing-your-distance-metric",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Choosing Your Distance Metric",
    "text": "Choosing Your Distance Metric\nIs euclidean distance really the best choice?!\n\n\nMy name is James Bond, James Bond is my name.\n\n\nMy name is James Bond.\n\n\nMy name is James.\n\n\nIf we count words the second two will be the most similar.\nThe first document is longer, so it has “double” counts.\nBut, it has the exact same words as the first document!\nSolution: cosine distance (on board)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#cosine-distance-1",
    "href": "slides/07-text-data.html#cosine-distance-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Cosine Distance",
    "text": "Cosine Distance\nAs a rule, cosine distance is a better choice for bag-of-words data!\n\n\nfrom sklearn.metrics.pairwise import cosine_distances\n\ndists = cosine_distances(bow_matrix)\ndists[0].argsort()\n\narray([0, 2, 3, 1])\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#measuring-similarity-1",
    "href": "slides/07-text-data.html#measuring-similarity-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Measuring Similarity",
    "text": "Measuring Similarity\nWhich of these seems most important for measuring similarity?\n\nDocument B, C, D all have the word “to”\nDocuments A, B, and C all have the word darkness.\nDocument A and Document C both have the word “hate”\nDocument C and Document D both have the word “light”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-scaling",
    "href": "slides/07-text-data.html#tf-idf-scaling",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF Scaling",
    "text": "TF-IDF Scaling\nWe would like to scale our word counts by the document length (TF).\n\nWe would also like to scale our word counts by the number of documents they appear in. (IDF)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#document-lengths",
    "href": "slides/07-text-data.html#document-lengths",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Document Lengths",
    "text": "Document Lengths\nIf a document is longer, it is more likely to share words with other documents.\n\nbow_totals = bow_dataframe.sum(axis = 1)\nbow_totals\n\n0    15\n1     5\n2    26\n3    18\ndtype: int64",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#term-frequencies-tf",
    "href": "slides/07-text-data.html#term-frequencies-tf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Term Frequencies (TF)",
    "text": "Term Frequencies (TF)\nLet’s use frequencies instead of counts.\n\nbow_tf = bow_dataframe.divide(bow_totals, axis = 0)\nbow_tf\n\n          a    adding   already  ...      turn     walks   whoever\n0  0.000000  0.000000  0.000000  ...  0.000000  0.066667  0.066667\n1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n2  0.038462  0.038462  0.038462  ...  0.000000  0.000000  0.000000\n3  0.000000  0.000000  0.000000  ...  0.055556  0.000000  0.000000\n\n[4 rows x 45 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#distance-of-term-frequencies-tf",
    "href": "slides/07-text-data.html#distance-of-term-frequencies-tf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Distance of Term Frequencies (TF)",
    "text": "Distance of Term Frequencies (TF)\n\n\ndists = cosine_distances(bow_tf)\ndists[0].argsort()\n\narray([0, 2, 3, 1])\n\n\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#inverse-document-frequency-idf",
    "href": "slides/07-text-data.html#inverse-document-frequency-idf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Inverse Document Frequency (IDF)",
    "text": "Inverse Document Frequency (IDF)\n\nIn principle, if two documents share rarer words they are more similar.\nWhat matters is not overall word frequency but how many of the documents have that word.\n\n\n\nbow_dataframe\n\n   a  adding  already  and  be  brother  ...  the  times  to  turn  walks  whoever\n0  0       0        0    1   0        1  ...    2      0   0     0      1        1\n1  0       0        0    0   0        0  ...    0      0   0     0      0        0\n2  1       1        1    0   0        0  ...    0      0   1     0      0        0\n3  0       0        0    0   1        0  ...    2      1   1     1      0        0\n\n[4 rows x 45 columns]",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-1",
    "href": "slides/07-text-data.html#idf---step-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 1",
    "text": "IDF - Step 1\nFirst, isolate the words that occurred in each document.\n\nhas_word = (bow_dataframe &gt; 0)\nhas_word[[\"darkness\", \"light\", \"hate\"]]\n\n   darkness  light   hate\n0      True  False   True\n1      True  False  False\n2      True   True   True\n3     False   True  False",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-2",
    "href": "slides/07-text-data.html#idf---step-2",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 2",
    "text": "IDF - Step 2\nThen, let’s calculate how often the word occurred across the four documents.\n\n\n\nbow_df = (\n  has_word\n  .sum(axis = 0) / 4\n  )\nbow_df\n\na             0.25\nadding        0.25\nalready       0.25\nand           0.25\nbe            0.25\nbrother       0.25\ncan           0.50\ncannot        0.25\ndarkest       0.25\ndarkness      0.75\ndeeper        0.25\ndevoid        0.25\ndo            0.25\ndrive         0.25\nfor           0.50\nfound         0.25\nfriend        0.25\nhappiness     0.25\nhas           0.25\nhate          0.50\nhello         0.25\nhis           0.25\nif            0.25\nin            0.50\nis            0.25\nlight         0.50\nmultiplies    0.25\nmy            0.25\nnight         0.25\nof            0.50\nold           0.25\non            0.25\none           0.25\nonly          0.50\nout           0.25\nremembers     0.25\nreturning     0.25\nstars         0.25\nthat          0.25\nthe           0.50\ntimes         0.25\nto            0.50\nturn          0.25\nwalks         0.25\nwhoever       0.25\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\naxis = 0\n\n\nWhat values are we summing?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-3",
    "href": "slides/07-text-data.html#idf---step-3",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 3",
    "text": "IDF - Step 3\nFind the inverse document frequencies:\n\n\n\nbow_log_idf = np.log(1 / bow_df)\nbow_log_idf\n\na             1.386294\nadding        1.386294\nalready       1.386294\nand           1.386294\nbe            1.386294\nbrother       1.386294\ncan           0.693147\ncannot        1.386294\ndarkest       1.386294\ndarkness      0.287682\ndeeper        1.386294\ndevoid        1.386294\ndo            1.386294\ndrive         1.386294\nfor           0.693147\nfound         1.386294\nfriend        1.386294\nhappiness     1.386294\nhas           1.386294\nhate          0.693147\nhello         1.386294\nhis           1.386294\nif            1.386294\nin            0.693147\nis            1.386294\nlight         0.693147\nmultiplies    1.386294\nmy            1.386294\nnight         1.386294\nof            0.693147\nold           1.386294\non            1.386294\none           1.386294\nonly          0.693147\nout           1.386294\nremembers     1.386294\nreturning     1.386294\nstars         1.386294\nthat          1.386294\nthe           0.693147\ntimes         1.386294\nto            0.693147\nturn          1.386294\nwalks         1.386294\nwhoever       1.386294\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\nMore than just the inverse!\n\n\nNotice we are using \\(log(\\frac{1}{p_i})\\) to get the IDFs.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#idf---step-4",
    "href": "slides/07-text-data.html#idf---step-4",
    "title": "Bag-of-Words and TF-IDF",
    "section": "IDF - Step 4",
    "text": "IDF - Step 4\nAdjust for the inverse document frequencies:\n\n\n\n\nbow_tf[[\"darkness\", \"light\", \"hate\"]]\n\n   darkness     light      hate\n0  0.133333  0.000000  0.066667\n1  0.200000  0.000000  0.000000\n2  0.115385  0.038462  0.115385\n3  0.000000  0.055556  0.000000\n\n\n\n\n\n\n\n\nbow_log_idf[[\"darkness\", \"light\", \"hate\"]]\n\ndarkness    0.287682\nlight       0.693147\nhate        0.693147\ndtype: float64\n\n\n\n\n\n\nbow_tf_idf = bow_tf.multiply(bow_log_idf, axis = 1)\n\n\n\n\n\nbow_tf_idf[[\"darkness\", \"light\", \"hate\"]]\n\n   darkness     light      hate\n0  0.038358  0.000000  0.046210\n1  0.057536  0.000000  0.000000\n2  0.033194  0.026660  0.079979\n3  0.000000  0.038508  0.000000",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-distances",
    "href": "slides/07-text-data.html#tf-idf-distances",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF Distances",
    "text": "TF-IDF Distances\n\n\ndists = cosine_distances(bow_tf_idf).round(decimals = 2)\ndists[0].argsort()\n\narray([0, 3, 2, 1])\n\n\n\n“Whoever has hate for his brother is in the darkness and walks in the darkness.”\n\n\n“Hello darkness, my old friend, I’ve come to talk with you again.”\n\n\n“Returning hate for hate multiplies hate, adding deeper darkness to a night already devoid of stars. Darkness cannot drive out darkness; only light can do that.”\n\n\n“Happiness can be found in the darkest of times, if only one remembers to turn on the light.”",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-in-sklearn",
    "href": "slides/07-text-data.html#tf-idf-in-sklearn",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF in sklearn",
    "text": "TF-IDF in sklearn\nSpecify\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# These options ensure that the numbers match our example above\nvec = TfidfVectorizer(smooth_idf = False)\n\n\nFit\n\n\n\nvec.fit(documents)\n\n\nTfidfVectorizer(smooth_idf=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.TfidfVectorizer?Documentation for TfidfVectorizeriFittedTfidfVectorizer(smooth_idf=False) \n\n\n\nTransform\n\n\ntfidf_matrix = vec.transform(documents)",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#tf-idf-in-sklearn-1",
    "href": "slides/07-text-data.html#tf-idf-in-sklearn-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "TF-IDF in sklearn",
    "text": "TF-IDF in sklearn\n\n\ndists = cosine_distances(tfidf_matrix).round(decimals = 2)\ndists[0].argsort()\n\narray([0, 2, 3, 1])\n\n\n\n \n\n\n\n\n😕",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#what-happened",
    "href": "slides/07-text-data.html#what-happened",
    "title": "Bag-of-Words and TF-IDF",
    "section": "What happened?",
    "text": "What happened?\n\nThe formula that is used to compute the tf-idf for a term t of a document d in a document set is:\ntf-idf(t, d) = tf(t, d) * idf(t),\nand the idf is computed as:\nidf(t) = log [ n / df(t) ] + 1,\nwhere n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#sklearn-idf",
    "href": "slides/07-text-data.html#sklearn-idf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "sklearn IDF",
    "text": "sklearn IDF\n\nthe idf is computed as: idf(t) = log [ n / df(t) ] + 1\n\nWe used \\(\\text{log} \\Bigg ( \\frac{1}{\\frac{\\text{df(t)}}{n}} \\Bigg )\\) which is the same as \\(\\text{log} \\Big ( \\frac{n}{df(t)} \\Big )\\)\n\n\nBut, we never added 1 before taking the log…\n\nThe effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#sklearn-tf",
    "href": "slides/07-text-data.html#sklearn-tf",
    "title": "Bag-of-Words and TF-IDF",
    "section": "sklearn TF",
    "text": "sklearn TF\nIn the documentation of TfidfVectorizer() it states that the function uses CountVectorizer() to obtain the TF matrix.\n\nCountVectorizer(): Transforms text into a sparse matrix of n-gram counts.\n\n\n\nThis means that sklearn is not dividing the counts of each term by the length of each document.",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#what-should-you-do",
    "href": "slides/07-text-data.html#what-should-you-do",
    "title": "Bag-of-Words and TF-IDF",
    "section": "What should you do?",
    "text": "What should you do?\n\nHere’s what I know:\n\nsklearn is used by the majority of data scientists\nsklearn is focusing on “training” data not being overly specific with the documents in the training set\n\nBoth of these are points in favor of sklearn.\n\n\n\nThe fact that the authors aren’t scaling the counts by the length of each document is a major bummer, but maybe not enough to outweigh the benefits?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#activity",
    "href": "slides/07-text-data.html#activity",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Activity",
    "text": "Activity\nUsing bi-grams, unigrams, and tri-grams, which Dr. Seuss document is closest to “One Fish Two Fish”?",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/07-text-data.html#takeaways-1",
    "href": "slides/07-text-data.html#takeaways-1",
    "title": "Bag-of-Words and TF-IDF",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe represent text data as a bag-of-words or bag-of-n-grams matrix.\nEach row is a document in the corpus.\nWe typically use cosine distance to measure similarity, because it captures patterns of word choice\nWe apply TF-IDF transformations to scale the bag-of-words data, so that words that appear in fewer documents are more important",
    "crumbs": [
      "Lecture Slides",
      "Week 4, Part 2 - Bag-of-Words and TF-IDF"
    ]
  },
  {
    "objectID": "slides/08-knn.html#data-wine-quality-variables",
    "href": "slides/08-knn.html#data-wine-quality-variables",
    "title": "K-Nearest-Neighbors",
    "section": "Data: Wine Quality Variables",
    "text": "Data: Wine Quality Variables\n\n\nyear: What year the wine was produced\nprice: Average market price for Bordeaux vintages according to a series of auctions.\n\nThe price is relative to the price of the 1961 vintage, regarded as the best one ever recorded.\n\nwin: Winter rainfall (in mm)\nsummer: Average temperature during the summer months (June - August)\nsep: Average temperature in the month of September (in Celsius)\nhar: Rainfall during harvest month(s) (in mm)\nage: How old the wine was in 1992 (years since 1992)"
  },
  {
    "objectID": "slides/08-knn.html#analysis-goal",
    "href": "slides/08-knn.html#analysis-goal",
    "title": "K-Nearest-Neighbors",
    "section": "Analysis Goal",
    "text": "Analysis Goal\n\nGoal: Predict what will be the quality (price) of wines in a future year.\n\n\nIdea: Wines with similar features probably have similar quality.\n\n\n\nInputs: Summer temperature, harvest rainfall, September temperature, winter rainfall, age of wine\n\n“Inputs” = “Features” = “Predictors” = “Independent variables”\n\nOutput: Price in 1992\n\n“Output” = “Target” = “Dependent variable”"
  },
  {
    "objectID": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data",
    "href": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data",
    "title": "K-Nearest-Neighbors",
    "section": "What if we had fit on the test data?",
    "text": "What if we had fit on the test data?\n\n\n\npreproc.fit(df_test)\n\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\n\n\n\npreproc.named_transformers_['standardscaler'].mean_\n\narray([ 18.6,  82. ,  18.4, 443. ,   3. ])\n\npreproc.named_transformers_['standardscaler'].var_\n\narray([0., 0., 0., 0., 0.])"
  },
  {
    "objectID": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data-1",
    "href": "slides/08-knn.html#what-if-we-had-fit-on-the-test-data-1",
    "title": "K-Nearest-Neighbors",
    "section": "What if we had fit on the test data?",
    "text": "What if we had fit on the test data?\n\npreproc.transform(df_test)\n\narray([[0., 0., 0., 0., 0.]])"
  },
  {
    "objectID": "slides/08-knn.html#predicting-unknown-prices",
    "href": "slides/08-knn.html#predicting-unknown-prices",
    "title": "K-Nearest-Neighbors",
    "section": "Predicting Unknown Prices",
    "text": "Predicting Unknown Prices\nFind the predicted 1992 price for all the unknown wines (years 1981 through 1991), with:\n\n\\(k = 1\\)\n\\(k = 5\\)\n\\(k = 10\\)\n\n\n\n\n\n\n\nA good place for a function!\n\n\nYou are performing the same process with different inputs of \\(k\\), so this seems like a reasonable place to write a function to save you time / reduce errors from copying and pasting."
  },
  {
    "objectID": "slides/08-knn.html#knn-big-questions",
    "href": "slides/08-knn.html#knn-big-questions",
    "title": "K-Nearest-Neighbors",
    "section": "KNN Big Questions",
    "text": "KNN Big Questions\n\nWhat is our definition of closest?\nWhat number should we use for \\(k\\)?\nHow do we evaluate the success of this approach?"
  },
  {
    "objectID": "slides/08-knn.html#specifying",
    "href": "slides/08-knn.html#specifying",
    "title": "K-Nearest-Neighbors",
    "section": "Specifying",
    "text": "Specifying\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n  preproc,\n  KNeighborsRegressor(n_neighbors = 5)\n  )\n          \npipeline\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() \n\n\n\n\n\n\n\n\nRemember the preproc we made earlier!\n\n\n\npreproc = make_column_transformer(\n  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),\n  remainder = \"drop\"\n)"
  },
  {
    "objectID": "slides/08-knn.html#fitting",
    "href": "slides/08-knn.html#fitting",
    "title": "K-Nearest-Neighbors",
    "section": "Fitting",
    "text": "Fitting\n\n\n\npipeline.fit(\n  y = df_train['price'], \n  X = df_train\n  )\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['summer', 'har', 'sep',\n                                                   'win', 'age'])])),\n                ('kneighborsregressor', KNeighborsRegressor())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['summer', 'har', 'sep', 'win', 'age'])]) standardscaler['summer', 'har', 'sep', 'win', 'age'] StandardScaler?Documentation for StandardScalerStandardScaler() KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor()"
  },
  {
    "objectID": "slides/08-knn.html#predicting",
    "href": "slides/08-knn.html#predicting",
    "title": "K-Nearest-Neighbors",
    "section": "Predicting",
    "text": "Predicting\n\npipeline.predict(X = df_test)\n\narray([25.8])"
  }
]